{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLC Network",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimzhang629/Rat-Head-Direction/blob/master/DLC_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK255E7YoEIt",
        "colab_type": "text"
      },
      "source": [
        "# DeepLabCut Toolbox - Colab\n",
        "https://github.com/AlexEMG/DeepLabCut\n",
        "\n",
        "This notebook illustrates how to use the cloud to:\n",
        "- create a training set\n",
        "- train a network\n",
        "- evaluate a network\n",
        "- analyze a novel video\n",
        "\n",
        "###This notebook assumes you already have a project folder with labeled data! \n",
        "\n",
        "This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n",
        "\n",
        "This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
        "\n",
        "Nath\\*, Mathis\\* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n",
        "\n",
        "\n",
        "Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
        "\n",
        "Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txoddlM8hLKm",
        "colab_type": "text"
      },
      "source": [
        "## First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ-nlTkri4HZ",
        "colab_type": "text"
      },
      "source": [
        "## Link your Google Drive (with your labeled data, or the demo data):\n",
        "\n",
        "### First, place your project folder into your google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS4Q4UkR9rgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a615ffe4-6a71-4dd9-e300-908fa4dcadd5"
      },
      "source": [
        "#Now, let's link to your GoogleDrive. Run this cell and follow the authorization instructions:\n",
        "#need to do this every time you open this notebook :(\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frnj1RVDyEqs",
        "colab_type": "text"
      },
      "source": [
        "YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n",
        "\n",
        "Typically, this will be: /content/drive/My Drive/yourProjectFolderName\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhENAlQnFENJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e1cbf28-2cd7-4666-eed8-38f251ade060"
      },
      "source": [
        "#Setup your project variables:\n",
        "# projectfolder is the dlc project that we're training\n",
        "  \n",
        "ProjectFolderName = 'SnoutAndImplantTracking-Jim-2020-05-20'\n",
        "VideoType = 'mp4' \n",
        "\n",
        "#put the videos you want to analyze\n",
        "videofile_path = ['/content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/AnalyzedVideos/'] \n",
        "videofile_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/AnalyzedVideos/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q23BzhA6CXxu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2b315a6-f8fe-439d-950c-708d230e0f56"
      },
      "source": [
        "#need to do this every time too\n",
        "\n",
        "!pip install deeplabcut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeplabcut\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/01/8b669887369739ccfcaac13f55800eefb56d5667b9fdca952a1d70017089/deeplabcut-2.1.8.2-py3-none-any.whl (400kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 4.8MB/s \n",
            "\u001b[?25hCollecting opencv-python~=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/34/185fc669fe1e0f64f00f81a90bd8c6df9c573c6bc16255a06e6e6350efec/opencv_python-3.4.9.33-cp36-cp36m-manylinux1_x86_64.whl (28.2MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2MB 115kB/s \n",
            "\u001b[?25hRequirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (46.3.0)\n",
            "Collecting ruamel.yaml~=0.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/92/59af3e38227b9cc14520bf1e59516d99ceca53e3b8448094248171e9432b/ruamel.yaml-0.16.10-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 58.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=2.7 in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.10.0)\n",
            "Requirement already satisfied: moviepy<=1.0.1 in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.12.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (3.0.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (5.5.0)\n",
            "Collecting matplotlib==3.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/69/f5e05f578585ed9935247be3788b374f90701296a70c8871bcd6d21edb00/matplotlib-3.0.3-cp36-cp36m-manylinux1_x86_64.whl (13.0MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0MB 253kB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 65.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2020.4.5.1)\n",
            "Collecting tensorpack>=0.9.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/3d/b26490e53e40bb2891ff5d2e50adf51a1fc7eff894ed5646d6dc56df2067/tensorpack-0.10.1-py2.py3-none-any.whl (291kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 57.2MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 140kB/s \n",
            "\u001b[?25hRequirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (3.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (4.41.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.10.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.8.1)\n",
            "Requirement already satisfied: pandas>=1.0. in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.0.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.34.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (7.1.2)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2020.0.133)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.5.1)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.23.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (1.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (7.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (2.4.1)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/77/4bcd63f362bcb6c8f4f06253c11f9772f64189bf08cf3f40c5ccbda9e561/ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy<=1.0.1->deeplabcut) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.3.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->deeplabcut) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->deeplabcut) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->deeplabcut) (2.4.7)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (19.0.1)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (5.4.8)\n",
            "Collecting msgpack-numpy>=0.4.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/32/323eda6da56cdbf768e41858d491c163a6989f27b1733eb3e9fca21291aa/msgpack_numpy-0.4.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (1.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (0.8.7)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->deeplabcut) (2.7.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.->deeplabcut) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->deeplabcut) (0.15.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (2.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deeplabcut) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deeplabcut) (2.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->deeplabcut) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.1.9)\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=5f5bb60c610465854db345e1550857a80b4d8a6999ee035bd2d2e5d4576002f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built pyyaml\n",
            "\u001b[31mERROR: umap-learn 0.4.3 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, opencv-python, ruamel.yaml.clib, ruamel.yaml, matplotlib, pyyaml, msgpack-numpy, tensorpack, deeplabcut\n",
            "  Found existing installation: numpy 1.18.4\n",
            "    Uninstalling numpy-1.18.4:\n",
            "      Successfully uninstalled numpy-1.18.4\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: matplotlib 3.2.1\n",
            "    Uninstalling matplotlib-3.2.1:\n",
            "      Successfully uninstalled matplotlib-3.2.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed deeplabcut-2.1.8.2 matplotlib-3.0.3 msgpack-numpy-0.4.5 numpy-1.16.4 opencv-python-3.4.9.33 pyyaml-5.3.1 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.0 tensorpack-0.10.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcLaThtTSWhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "763cefe9-0f2e-443d-8918-9081c2fc2c3c"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.2.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.29.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (46.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=a4d22e42a452b05b18d1263c49348b93e69a1eae6c3fae60ca583e6fcd8078e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25wSj6TlVclR",
        "colab_type": "text"
      },
      "source": [
        "**(Be sure to click \"RESTART RUNTIME\" is it is displayed above above before moving on !)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXufoX6INe6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GUIs don't work on the cloud, so label your data locally on your computer! This will suppress the GUI support\n",
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K9Ndy1beyfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "d9e4d6d6-e6ac-4981-91cc-78ff39490728"
      },
      "source": [
        "import deeplabcut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4orkg9QTHKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a91d689-d2d1-4583-99be-455d0e1cd541"
      },
      "source": [
        "deeplabcut.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.8.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ZlDr3wV4D1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a46ace72-756c-4c45-fcaa-b7c0d2af0f86"
      },
      "source": [
        "#This creates a path variable that links to your google drive copy\n",
        "#path to the config.yaml file\n",
        "path_config_file = '/content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/config.yaml'\n",
        "path_config_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/config.yaml'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNi9s1dboEJN",
        "colab_type": "text"
      },
      "source": [
        "## Create a training dataset:\n",
        "### You must do this step inside of Colab:\n",
        "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
        "\n",
        "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n",
        "\n",
        "Now it is the time to start training the network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eMeUwgxPoEJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "16ef3d1b-62e1-40ba-ad6e-7b4be342a0da"
      },
      "source": [
        "#don't need to do this for already trained network\n",
        "deeplabcut.create_training_dataset(path_config_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([ 34,  24,  94, 117,  17,  31,  73,  19, 131,  60,  62,   4, 101,\n",
              "           72,  16,  70,  77,  98,  93, 105,  66,   5, 115,  82, 122,  97,\n",
              "          109, 133,  40, 124,  78,   7, 119, 114,  47,   1, 113,  67,  81,\n",
              "          127,  64,  54,  88,  48, 111,  49,  18, 121,  41,  46,  43, 102,\n",
              "          139,  57, 120,  55,  21,  22,  53, 125, 135,  80,  99,  32,  36,\n",
              "           56,  74,  26,  91,  51,  23,  45,  89, 106, 123, 107,  35,  92,\n",
              "           75,  13,  28, 118,  61,  79, 136,  42,  20,  15,  90, 108,  84,\n",
              "           50,  96,  76, 126, 129,   8, 116,   2,  33,  25, 104,  12,  71,\n",
              "           83,   6,  14, 112, 137, 138,  85, 134,  39,  95,  37,  44,  87,\n",
              "           52, 103, 110,  65,  63,  30,  29,  58,  38,   0, 100,   9,  59,\n",
              "           27,  86, 128]), array([ 10, 132,  11,  69,   3, 130,  68])))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4FczXGDoEJU",
        "colab_type": "text"
      },
      "source": [
        "## Start training:\n",
        "This function trains the network for a specific shuffle of the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pOvDq_2oEJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53f15002-5254-4f59-fbe1-50f1ffa3c086"
      },
      "source": [
        "#let's also change the display and save_iters just in case Colab takes away the GPU... \n",
        "#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n",
        "#more info and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\n",
        "#don't need to do this again for pre-trained network\n",
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\n",
        "\n",
        "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n",
        "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7]],\n",
            " 'all_joints_names': ['snout',\n",
            "                      'left',\n",
            "                      'right',\n",
            "                      'mid',\n",
            "                      'back',\n",
            "                      'tail',\n",
            "                      'body',\n",
            "                      'led'],\n",
            " 'batch_size': 1,\n",
            " 'bottomheight': 400,\n",
            " 'crop': True,\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SnoutAndImplantTrackingMay20/SnoutAndImplantTracking_Jim95shuffle1.mat',\n",
            " 'dataset_type': 'default',\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'leftwidth': 400,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SnoutAndImplantTrackingMay20/Documentation_data-SnoutAndImplantTracking_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'minsize': 100,\n",
            " 'mirror': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 8,\n",
            " 'optimizer': 'sgd',\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My '\n",
            "                 'Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20',\n",
            " 'regularize': False,\n",
            " 'rightwidth': 400,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/dlc-models/iteration-0/SnoutAndImplantTrackingMay20-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'topheight': 400,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Switching batchsize to 1, as default/tensorpack/deterministic loaders do not support batches >1. Use imgaug loader.\n",
            "Starting with standard pose-dataset loader.\n",
            "Initializing ResNet\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py:62: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py:160: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/nnet/losses.py:38: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "Loading ImageNet-pretrained resnet_50\n",
            "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/dlc-models/iteration-0/SnoutAndImplantTrackingMay20-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'default', 'deterministic': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7]], 'all_joints_names': ['snout', 'left', 'right', 'mid', 'back', 'tail', 'body', 'led'], 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SnoutAndImplantTrackingMay20/SnoutAndImplantTracking_Jim95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SnoutAndImplantTrackingMay20/Documentation_data-SnoutAndImplantTracking_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 8, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'output_stride': 16, 'deconvolutionstride': 2}\n",
            "Starting training....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration: 10 loss: 0.4310 lr: 0.005\n",
            "iteration: 20 loss: 0.0674 lr: 0.005\n",
            "iteration: 30 loss: 0.0459 lr: 0.005\n",
            "iteration: 40 loss: 0.0350 lr: 0.005\n",
            "iteration: 50 loss: 0.0358 lr: 0.005\n",
            "iteration: 60 loss: 0.0361 lr: 0.005\n",
            "iteration: 70 loss: 0.0285 lr: 0.005\n",
            "iteration: 80 loss: 0.0277 lr: 0.005\n",
            "iteration: 90 loss: 0.0309 lr: 0.005\n",
            "iteration: 100 loss: 0.0268 lr: 0.005\n",
            "iteration: 110 loss: 0.0304 lr: 0.005\n",
            "iteration: 120 loss: 0.0252 lr: 0.005\n",
            "iteration: 130 loss: 0.0257 lr: 0.005\n",
            "iteration: 140 loss: 0.0267 lr: 0.005\n",
            "iteration: 150 loss: 0.0252 lr: 0.005\n",
            "iteration: 160 loss: 0.0234 lr: 0.005\n",
            "iteration: 170 loss: 0.0246 lr: 0.005\n",
            "iteration: 180 loss: 0.0251 lr: 0.005\n",
            "iteration: 190 loss: 0.0256 lr: 0.005\n",
            "iteration: 200 loss: 0.0220 lr: 0.005\n",
            "iteration: 210 loss: 0.0235 lr: 0.005\n",
            "iteration: 220 loss: 0.0227 lr: 0.005\n",
            "iteration: 230 loss: 0.0256 lr: 0.005\n",
            "iteration: 240 loss: 0.0271 lr: 0.005\n",
            "iteration: 250 loss: 0.0192 lr: 0.005\n",
            "iteration: 260 loss: 0.0247 lr: 0.005\n",
            "iteration: 270 loss: 0.0237 lr: 0.005\n",
            "iteration: 280 loss: 0.0228 lr: 0.005\n",
            "iteration: 290 loss: 0.0193 lr: 0.005\n",
            "iteration: 300 loss: 0.0201 lr: 0.005\n",
            "iteration: 310 loss: 0.0240 lr: 0.005\n",
            "iteration: 320 loss: 0.0216 lr: 0.005\n",
            "iteration: 330 loss: 0.0215 lr: 0.005\n",
            "iteration: 340 loss: 0.0218 lr: 0.005\n",
            "iteration: 350 loss: 0.0237 lr: 0.005\n",
            "iteration: 360 loss: 0.0197 lr: 0.005\n",
            "iteration: 370 loss: 0.0241 lr: 0.005\n",
            "iteration: 380 loss: 0.0213 lr: 0.005\n",
            "iteration: 390 loss: 0.0234 lr: 0.005\n",
            "iteration: 400 loss: 0.0215 lr: 0.005\n",
            "iteration: 410 loss: 0.0259 lr: 0.005\n",
            "iteration: 420 loss: 0.0205 lr: 0.005\n",
            "iteration: 430 loss: 0.0218 lr: 0.005\n",
            "iteration: 440 loss: 0.0224 lr: 0.005\n",
            "iteration: 450 loss: 0.0230 lr: 0.005\n",
            "iteration: 460 loss: 0.0231 lr: 0.005\n",
            "iteration: 470 loss: 0.0189 lr: 0.005\n",
            "iteration: 480 loss: 0.0207 lr: 0.005\n",
            "iteration: 490 loss: 0.0250 lr: 0.005\n",
            "iteration: 500 loss: 0.0198 lr: 0.005\n",
            "iteration: 510 loss: 0.0221 lr: 0.005\n",
            "iteration: 520 loss: 0.0238 lr: 0.005\n",
            "iteration: 530 loss: 0.0215 lr: 0.005\n",
            "iteration: 540 loss: 0.0194 lr: 0.005\n",
            "iteration: 550 loss: 0.0181 lr: 0.005\n",
            "iteration: 560 loss: 0.0179 lr: 0.005\n",
            "iteration: 570 loss: 0.0197 lr: 0.005\n",
            "iteration: 580 loss: 0.0192 lr: 0.005\n",
            "iteration: 590 loss: 0.0199 lr: 0.005\n",
            "iteration: 600 loss: 0.0212 lr: 0.005\n",
            "iteration: 610 loss: 0.0182 lr: 0.005\n",
            "iteration: 620 loss: 0.0184 lr: 0.005\n",
            "iteration: 630 loss: 0.0184 lr: 0.005\n",
            "iteration: 640 loss: 0.0192 lr: 0.005\n",
            "iteration: 650 loss: 0.0205 lr: 0.005\n",
            "iteration: 660 loss: 0.0210 lr: 0.005\n",
            "iteration: 670 loss: 0.0168 lr: 0.005\n",
            "iteration: 680 loss: 0.0195 lr: 0.005\n",
            "iteration: 690 loss: 0.0233 lr: 0.005\n",
            "iteration: 700 loss: 0.0154 lr: 0.005\n",
            "iteration: 710 loss: 0.0191 lr: 0.005\n",
            "iteration: 720 loss: 0.0197 lr: 0.005\n",
            "iteration: 730 loss: 0.0202 lr: 0.005\n",
            "iteration: 740 loss: 0.0165 lr: 0.005\n",
            "iteration: 750 loss: 0.0173 lr: 0.005\n",
            "iteration: 760 loss: 0.0169 lr: 0.005\n",
            "iteration: 770 loss: 0.0188 lr: 0.005\n",
            "iteration: 780 loss: 0.0204 lr: 0.005\n",
            "iteration: 790 loss: 0.0204 lr: 0.005\n",
            "iteration: 800 loss: 0.0198 lr: 0.005\n",
            "iteration: 810 loss: 0.0151 lr: 0.005\n",
            "iteration: 820 loss: 0.0186 lr: 0.005\n",
            "iteration: 830 loss: 0.0166 lr: 0.005\n",
            "iteration: 840 loss: 0.0190 lr: 0.005\n",
            "iteration: 850 loss: 0.0167 lr: 0.005\n",
            "iteration: 860 loss: 0.0155 lr: 0.005\n",
            "iteration: 870 loss: 0.0210 lr: 0.005\n",
            "iteration: 880 loss: 0.0157 lr: 0.005\n",
            "iteration: 890 loss: 0.0176 lr: 0.005\n",
            "iteration: 900 loss: 0.0180 lr: 0.005\n",
            "iteration: 910 loss: 0.0191 lr: 0.005\n",
            "iteration: 920 loss: 0.0184 lr: 0.005\n",
            "iteration: 930 loss: 0.0184 lr: 0.005\n",
            "iteration: 940 loss: 0.0148 lr: 0.005\n",
            "iteration: 950 loss: 0.0190 lr: 0.005\n",
            "iteration: 960 loss: 0.0136 lr: 0.005\n",
            "iteration: 970 loss: 0.0163 lr: 0.005\n",
            "iteration: 980 loss: 0.0165 lr: 0.005\n",
            "iteration: 990 loss: 0.0157 lr: 0.005\n",
            "iteration: 1000 loss: 0.0177 lr: 0.005\n",
            "iteration: 1010 loss: 0.0167 lr: 0.005\n",
            "iteration: 1020 loss: 0.0160 lr: 0.005\n",
            "iteration: 1030 loss: 0.0165 lr: 0.005\n",
            "iteration: 1040 loss: 0.0177 lr: 0.005\n",
            "iteration: 1050 loss: 0.0172 lr: 0.005\n",
            "iteration: 1060 loss: 0.0188 lr: 0.005\n",
            "iteration: 1070 loss: 0.0182 lr: 0.005\n",
            "iteration: 1080 loss: 0.0158 lr: 0.005\n",
            "iteration: 1090 loss: 0.0161 lr: 0.005\n",
            "iteration: 1100 loss: 0.0173 lr: 0.005\n",
            "iteration: 1110 loss: 0.0166 lr: 0.005\n",
            "iteration: 1120 loss: 0.0161 lr: 0.005\n",
            "iteration: 1130 loss: 0.0179 lr: 0.005\n",
            "iteration: 1140 loss: 0.0171 lr: 0.005\n",
            "iteration: 1150 loss: 0.0142 lr: 0.005\n",
            "iteration: 1160 loss: 0.0112 lr: 0.005\n",
            "iteration: 1170 loss: 0.0147 lr: 0.005\n",
            "iteration: 1180 loss: 0.0195 lr: 0.005\n",
            "iteration: 1190 loss: 0.0141 lr: 0.005\n",
            "iteration: 1200 loss: 0.0139 lr: 0.005\n",
            "iteration: 1210 loss: 0.0153 lr: 0.005\n",
            "iteration: 1220 loss: 0.0138 lr: 0.005\n",
            "iteration: 1230 loss: 0.0147 lr: 0.005\n",
            "iteration: 1240 loss: 0.0132 lr: 0.005\n",
            "iteration: 1250 loss: 0.0150 lr: 0.005\n",
            "iteration: 1260 loss: 0.0171 lr: 0.005\n",
            "iteration: 1270 loss: 0.0136 lr: 0.005\n",
            "iteration: 1280 loss: 0.0123 lr: 0.005\n",
            "iteration: 1290 loss: 0.0147 lr: 0.005\n",
            "iteration: 1300 loss: 0.0149 lr: 0.005\n",
            "iteration: 1310 loss: 0.0168 lr: 0.005\n",
            "iteration: 1320 loss: 0.0157 lr: 0.005\n",
            "iteration: 1330 loss: 0.0155 lr: 0.005\n",
            "iteration: 1340 loss: 0.0143 lr: 0.005\n",
            "iteration: 1350 loss: 0.0168 lr: 0.005\n",
            "iteration: 1360 loss: 0.0130 lr: 0.005\n",
            "iteration: 1370 loss: 0.0163 lr: 0.005\n",
            "iteration: 1380 loss: 0.0181 lr: 0.005\n",
            "iteration: 1390 loss: 0.0155 lr: 0.005\n",
            "iteration: 1400 loss: 0.0122 lr: 0.005\n",
            "iteration: 1410 loss: 0.0143 lr: 0.005\n",
            "iteration: 1420 loss: 0.0135 lr: 0.005\n",
            "iteration: 1430 loss: 0.0139 lr: 0.005\n",
            "iteration: 1440 loss: 0.0139 lr: 0.005\n",
            "iteration: 1450 loss: 0.0165 lr: 0.005\n",
            "iteration: 1460 loss: 0.0120 lr: 0.005\n",
            "iteration: 1470 loss: 0.0125 lr: 0.005\n",
            "iteration: 1480 loss: 0.0146 lr: 0.005\n",
            "iteration: 1490 loss: 0.0148 lr: 0.005\n",
            "iteration: 1500 loss: 0.0143 lr: 0.005\n",
            "iteration: 1510 loss: 0.0141 lr: 0.005\n",
            "iteration: 1520 loss: 0.0166 lr: 0.005\n",
            "iteration: 1530 loss: 0.0125 lr: 0.005\n",
            "iteration: 1540 loss: 0.0111 lr: 0.005\n",
            "iteration: 1550 loss: 0.0192 lr: 0.005\n",
            "iteration: 1560 loss: 0.0149 lr: 0.005\n",
            "iteration: 1570 loss: 0.0125 lr: 0.005\n",
            "iteration: 1580 loss: 0.0136 lr: 0.005\n",
            "iteration: 1590 loss: 0.0160 lr: 0.005\n",
            "iteration: 1600 loss: 0.0188 lr: 0.005\n",
            "iteration: 1610 loss: 0.0137 lr: 0.005\n",
            "iteration: 1620 loss: 0.0151 lr: 0.005\n",
            "iteration: 1630 loss: 0.0137 lr: 0.005\n",
            "iteration: 1640 loss: 0.0155 lr: 0.005\n",
            "iteration: 1650 loss: 0.0135 lr: 0.005\n",
            "iteration: 1660 loss: 0.0125 lr: 0.005\n",
            "iteration: 1670 loss: 0.0116 lr: 0.005\n",
            "iteration: 1680 loss: 0.0135 lr: 0.005\n",
            "iteration: 1690 loss: 0.0166 lr: 0.005\n",
            "iteration: 1700 loss: 0.0144 lr: 0.005\n",
            "iteration: 1710 loss: 0.0125 lr: 0.005\n",
            "iteration: 1720 loss: 0.0135 lr: 0.005\n",
            "iteration: 1730 loss: 0.0117 lr: 0.005\n",
            "iteration: 1740 loss: 0.0149 lr: 0.005\n",
            "iteration: 1750 loss: 0.0135 lr: 0.005\n",
            "iteration: 1760 loss: 0.0172 lr: 0.005\n",
            "iteration: 1770 loss: 0.0123 lr: 0.005\n",
            "iteration: 1780 loss: 0.0122 lr: 0.005\n",
            "iteration: 1790 loss: 0.0125 lr: 0.005\n",
            "iteration: 1800 loss: 0.0148 lr: 0.005\n",
            "iteration: 1810 loss: 0.0137 lr: 0.005\n",
            "iteration: 1820 loss: 0.0130 lr: 0.005\n",
            "iteration: 1830 loss: 0.0127 lr: 0.005\n",
            "iteration: 1840 loss: 0.0098 lr: 0.005\n",
            "iteration: 1850 loss: 0.0134 lr: 0.005\n",
            "iteration: 1860 loss: 0.0143 lr: 0.005\n",
            "iteration: 1870 loss: 0.0118 lr: 0.005\n",
            "iteration: 1880 loss: 0.0145 lr: 0.005\n",
            "iteration: 1890 loss: 0.0150 lr: 0.005\n",
            "iteration: 1900 loss: 0.0140 lr: 0.005\n",
            "iteration: 1910 loss: 0.0106 lr: 0.005\n",
            "iteration: 1920 loss: 0.0111 lr: 0.005\n",
            "iteration: 1930 loss: 0.0116 lr: 0.005\n",
            "iteration: 1940 loss: 0.0123 lr: 0.005\n",
            "iteration: 1950 loss: 0.0130 lr: 0.005\n",
            "iteration: 1960 loss: 0.0127 lr: 0.005\n",
            "iteration: 1970 loss: 0.0107 lr: 0.005\n",
            "iteration: 1980 loss: 0.0137 lr: 0.005\n",
            "iteration: 1990 loss: 0.0109 lr: 0.005\n",
            "iteration: 2000 loss: 0.0102 lr: 0.005\n",
            "iteration: 2010 loss: 0.0123 lr: 0.005\n",
            "iteration: 2020 loss: 0.0125 lr: 0.005\n",
            "iteration: 2030 loss: 0.0103 lr: 0.005\n",
            "iteration: 2040 loss: 0.0152 lr: 0.005\n",
            "iteration: 2050 loss: 0.0134 lr: 0.005\n",
            "iteration: 2060 loss: 0.0128 lr: 0.005\n",
            "iteration: 2070 loss: 0.0131 lr: 0.005\n",
            "iteration: 2080 loss: 0.0124 lr: 0.005\n",
            "iteration: 2090 loss: 0.0117 lr: 0.005\n",
            "iteration: 2100 loss: 0.0125 lr: 0.005\n",
            "iteration: 2110 loss: 0.0115 lr: 0.005\n",
            "iteration: 2120 loss: 0.0091 lr: 0.005\n",
            "iteration: 2130 loss: 0.0125 lr: 0.005\n",
            "iteration: 2140 loss: 0.0143 lr: 0.005\n",
            "iteration: 2150 loss: 0.0114 lr: 0.005\n",
            "iteration: 2160 loss: 0.0138 lr: 0.005\n",
            "iteration: 2170 loss: 0.0125 lr: 0.005\n",
            "iteration: 2180 loss: 0.0110 lr: 0.005\n",
            "iteration: 2190 loss: 0.0104 lr: 0.005\n",
            "iteration: 2200 loss: 0.0109 lr: 0.005\n",
            "iteration: 2210 loss: 0.0104 lr: 0.005\n",
            "iteration: 2220 loss: 0.0114 lr: 0.005\n",
            "iteration: 2230 loss: 0.0108 lr: 0.005\n",
            "iteration: 2240 loss: 0.0137 lr: 0.005\n",
            "iteration: 2250 loss: 0.0139 lr: 0.005\n",
            "iteration: 2260 loss: 0.0110 lr: 0.005\n",
            "iteration: 2270 loss: 0.0115 lr: 0.005\n",
            "iteration: 2280 loss: 0.0102 lr: 0.005\n",
            "iteration: 2290 loss: 0.0117 lr: 0.005\n",
            "iteration: 2300 loss: 0.0117 lr: 0.005\n",
            "iteration: 2310 loss: 0.0092 lr: 0.005\n",
            "iteration: 2320 loss: 0.0110 lr: 0.005\n",
            "iteration: 2330 loss: 0.0121 lr: 0.005\n",
            "iteration: 2340 loss: 0.0095 lr: 0.005\n",
            "iteration: 2350 loss: 0.0104 lr: 0.005\n",
            "iteration: 2360 loss: 0.0122 lr: 0.005\n",
            "iteration: 2370 loss: 0.0146 lr: 0.005\n",
            "iteration: 2380 loss: 0.0138 lr: 0.005\n",
            "iteration: 2390 loss: 0.0121 lr: 0.005\n",
            "iteration: 2400 loss: 0.0113 lr: 0.005\n",
            "iteration: 2410 loss: 0.0114 lr: 0.005\n",
            "iteration: 2420 loss: 0.0123 lr: 0.005\n",
            "iteration: 2430 loss: 0.0157 lr: 0.005\n",
            "iteration: 2440 loss: 0.0100 lr: 0.005\n",
            "iteration: 2450 loss: 0.0118 lr: 0.005\n",
            "iteration: 2460 loss: 0.0165 lr: 0.005\n",
            "iteration: 2470 loss: 0.0124 lr: 0.005\n",
            "iteration: 2480 loss: 0.0114 lr: 0.005\n",
            "iteration: 2490 loss: 0.0123 lr: 0.005\n",
            "iteration: 2500 loss: 0.0130 lr: 0.005\n",
            "iteration: 2510 loss: 0.0115 lr: 0.005\n",
            "iteration: 2520 loss: 0.0096 lr: 0.005\n",
            "iteration: 2530 loss: 0.0104 lr: 0.005\n",
            "iteration: 2540 loss: 0.0104 lr: 0.005\n",
            "iteration: 2550 loss: 0.0122 lr: 0.005\n",
            "iteration: 2560 loss: 0.0108 lr: 0.005\n",
            "iteration: 2570 loss: 0.0127 lr: 0.005\n",
            "iteration: 2580 loss: 0.0099 lr: 0.005\n",
            "iteration: 2590 loss: 0.0127 lr: 0.005\n",
            "iteration: 2600 loss: 0.0106 lr: 0.005\n",
            "iteration: 2610 loss: 0.0118 lr: 0.005\n",
            "iteration: 2620 loss: 0.0104 lr: 0.005\n",
            "iteration: 2630 loss: 0.0126 lr: 0.005\n",
            "iteration: 2640 loss: 0.0115 lr: 0.005\n",
            "iteration: 2650 loss: 0.0146 lr: 0.005\n",
            "iteration: 2660 loss: 0.0123 lr: 0.005\n",
            "iteration: 2670 loss: 0.0120 lr: 0.005\n",
            "iteration: 2680 loss: 0.0099 lr: 0.005\n",
            "iteration: 2690 loss: 0.0096 lr: 0.005\n",
            "iteration: 2700 loss: 0.0095 lr: 0.005\n",
            "iteration: 2710 loss: 0.0112 lr: 0.005\n",
            "iteration: 2720 loss: 0.0097 lr: 0.005\n",
            "iteration: 2730 loss: 0.0131 lr: 0.005\n",
            "iteration: 2740 loss: 0.0110 lr: 0.005\n",
            "iteration: 2750 loss: 0.0101 lr: 0.005\n",
            "iteration: 2760 loss: 0.0118 lr: 0.005\n",
            "iteration: 2770 loss: 0.0094 lr: 0.005\n",
            "iteration: 2780 loss: 0.0129 lr: 0.005\n",
            "iteration: 2790 loss: 0.0132 lr: 0.005\n",
            "iteration: 2800 loss: 0.0109 lr: 0.005\n",
            "iteration: 2810 loss: 0.0103 lr: 0.005\n",
            "iteration: 2820 loss: 0.0117 lr: 0.005\n",
            "iteration: 2830 loss: 0.0105 lr: 0.005\n",
            "iteration: 2840 loss: 0.0109 lr: 0.005\n",
            "iteration: 2850 loss: 0.0097 lr: 0.005\n",
            "iteration: 2860 loss: 0.0098 lr: 0.005\n",
            "iteration: 2870 loss: 0.0097 lr: 0.005\n",
            "iteration: 2880 loss: 0.0112 lr: 0.005\n",
            "iteration: 2890 loss: 0.0094 lr: 0.005\n",
            "iteration: 2900 loss: 0.0104 lr: 0.005\n",
            "iteration: 2910 loss: 0.0110 lr: 0.005\n",
            "iteration: 2920 loss: 0.0102 lr: 0.005\n",
            "iteration: 2930 loss: 0.0111 lr: 0.005\n",
            "iteration: 2940 loss: 0.0124 lr: 0.005\n",
            "iteration: 2950 loss: 0.0119 lr: 0.005\n",
            "iteration: 2960 loss: 0.0105 lr: 0.005\n",
            "iteration: 2970 loss: 0.0121 lr: 0.005\n",
            "iteration: 2980 loss: 0.0087 lr: 0.005\n",
            "iteration: 2990 loss: 0.0101 lr: 0.005\n",
            "iteration: 3000 loss: 0.0101 lr: 0.005\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration: 79690 loss: 0.0019 lr: 0.02\n",
            "iteration: 79700 loss: 0.0020 lr: 0.02\n",
            "iteration: 79710 loss: 0.0017 lr: 0.02\n",
            "iteration: 79720 loss: 0.0017 lr: 0.02\n",
            "iteration: 79730 loss: 0.0016 lr: 0.02\n",
            "iteration: 79740 loss: 0.0017 lr: 0.02\n",
            "iteration: 79750 loss: 0.0018 lr: 0.02\n",
            "iteration: 79760 loss: 0.0022 lr: 0.02\n",
            "iteration: 79770 loss: 0.0019 lr: 0.02\n",
            "iteration: 79780 loss: 0.0014 lr: 0.02\n",
            "iteration: 79790 loss: 0.0023 lr: 0.02\n",
            "iteration: 79800 loss: 0.0022 lr: 0.02\n",
            "iteration: 79810 loss: 0.0016 lr: 0.02\n",
            "iteration: 79820 loss: 0.0020 lr: 0.02\n",
            "iteration: 79830 loss: 0.0022 lr: 0.02\n",
            "iteration: 79840 loss: 0.0019 lr: 0.02\n",
            "iteration: 79850 loss: 0.0016 lr: 0.02\n",
            "iteration: 79860 loss: 0.0020 lr: 0.02\n",
            "iteration: 79870 loss: 0.0017 lr: 0.02\n",
            "iteration: 79880 loss: 0.0019 lr: 0.02\n",
            "iteration: 79890 loss: 0.0016 lr: 0.02\n",
            "iteration: 79900 loss: 0.0021 lr: 0.02\n",
            "iteration: 79910 loss: 0.0019 lr: 0.02\n",
            "iteration: 79920 loss: 0.0019 lr: 0.02\n",
            "iteration: 79930 loss: 0.0021 lr: 0.02\n",
            "iteration: 79940 loss: 0.0016 lr: 0.02\n",
            "iteration: 79950 loss: 0.0026 lr: 0.02\n",
            "iteration: 79960 loss: 0.0018 lr: 0.02\n",
            "iteration: 79970 loss: 0.0025 lr: 0.02\n",
            "iteration: 79980 loss: 0.0018 lr: 0.02\n",
            "iteration: 79990 loss: 0.0018 lr: 0.02\n",
            "iteration: 80000 loss: 0.0019 lr: 0.02\n",
            "iteration: 80010 loss: 0.0018 lr: 0.02\n",
            "iteration: 80020 loss: 0.0018 lr: 0.02\n",
            "iteration: 80030 loss: 0.0020 lr: 0.02\n",
            "iteration: 80040 loss: 0.0027 lr: 0.02\n",
            "iteration: 80050 loss: 0.0022 lr: 0.02\n",
            "iteration: 80060 loss: 0.0018 lr: 0.02\n",
            "iteration: 80070 loss: 0.0019 lr: 0.02\n",
            "iteration: 80080 loss: 0.0018 lr: 0.02\n",
            "iteration: 80090 loss: 0.0021 lr: 0.02\n",
            "iteration: 80100 loss: 0.0015 lr: 0.02\n",
            "iteration: 80110 loss: 0.0016 lr: 0.02\n",
            "iteration: 80120 loss: 0.0023 lr: 0.02\n",
            "iteration: 80130 loss: 0.0017 lr: 0.02\n",
            "iteration: 80140 loss: 0.0021 lr: 0.02\n",
            "iteration: 80150 loss: 0.0022 lr: 0.02\n",
            "iteration: 80160 loss: 0.0019 lr: 0.02\n",
            "iteration: 80170 loss: 0.0017 lr: 0.02\n",
            "iteration: 80180 loss: 0.0018 lr: 0.02\n",
            "iteration: 80190 loss: 0.0016 lr: 0.02\n",
            "iteration: 80200 loss: 0.0022 lr: 0.02\n",
            "iteration: 80210 loss: 0.0020 lr: 0.02\n",
            "iteration: 80220 loss: 0.0015 lr: 0.02\n",
            "iteration: 80230 loss: 0.0017 lr: 0.02\n",
            "iteration: 80240 loss: 0.0019 lr: 0.02\n",
            "iteration: 80250 loss: 0.0015 lr: 0.02\n",
            "iteration: 80260 loss: 0.0021 lr: 0.02\n",
            "iteration: 80270 loss: 0.0018 lr: 0.02\n",
            "iteration: 80280 loss: 0.0019 lr: 0.02\n",
            "iteration: 80290 loss: 0.0021 lr: 0.02\n",
            "iteration: 80300 loss: 0.0020 lr: 0.02\n",
            "iteration: 80310 loss: 0.0021 lr: 0.02\n",
            "iteration: 80320 loss: 0.0017 lr: 0.02\n",
            "iteration: 80330 loss: 0.0019 lr: 0.02\n",
            "iteration: 80340 loss: 0.0017 lr: 0.02\n",
            "iteration: 80350 loss: 0.0021 lr: 0.02\n",
            "iteration: 80360 loss: 0.0018 lr: 0.02\n",
            "iteration: 80370 loss: 0.0018 lr: 0.02\n",
            "iteration: 80380 loss: 0.0019 lr: 0.02\n",
            "iteration: 80390 loss: 0.0017 lr: 0.02\n",
            "iteration: 80400 loss: 0.0017 lr: 0.02\n",
            "iteration: 80410 loss: 0.0025 lr: 0.02\n",
            "iteration: 80420 loss: 0.0024 lr: 0.02\n",
            "iteration: 80430 loss: 0.0016 lr: 0.02\n",
            "iteration: 80440 loss: 0.0014 lr: 0.02\n",
            "iteration: 80450 loss: 0.0016 lr: 0.02\n",
            "iteration: 80460 loss: 0.0023 lr: 0.02\n",
            "iteration: 80470 loss: 0.0016 lr: 0.02\n",
            "iteration: 80480 loss: 0.0022 lr: 0.02\n",
            "iteration: 80490 loss: 0.0022 lr: 0.02\n",
            "iteration: 80500 loss: 0.0020 lr: 0.02\n",
            "iteration: 80510 loss: 0.0018 lr: 0.02\n",
            "iteration: 80520 loss: 0.0020 lr: 0.02\n",
            "iteration: 80530 loss: 0.0014 lr: 0.02\n",
            "iteration: 80540 loss: 0.0019 lr: 0.02\n",
            "iteration: 80550 loss: 0.0016 lr: 0.02\n",
            "iteration: 80560 loss: 0.0014 lr: 0.02\n",
            "iteration: 80570 loss: 0.0018 lr: 0.02\n",
            "iteration: 80580 loss: 0.0012 lr: 0.02\n",
            "iteration: 80590 loss: 0.0015 lr: 0.02\n",
            "iteration: 80600 loss: 0.0018 lr: 0.02\n",
            "iteration: 80610 loss: 0.0018 lr: 0.02\n",
            "iteration: 80620 loss: 0.0021 lr: 0.02\n",
            "iteration: 80630 loss: 0.0019 lr: 0.02\n",
            "iteration: 80640 loss: 0.0019 lr: 0.02\n",
            "iteration: 80650 loss: 0.0018 lr: 0.02\n",
            "iteration: 80660 loss: 0.0019 lr: 0.02\n",
            "iteration: 80670 loss: 0.0018 lr: 0.02\n",
            "iteration: 80680 loss: 0.0020 lr: 0.02\n",
            "iteration: 80690 loss: 0.0019 lr: 0.02\n",
            "iteration: 80700 loss: 0.0019 lr: 0.02\n",
            "iteration: 80710 loss: 0.0023 lr: 0.02\n",
            "iteration: 80720 loss: 0.0017 lr: 0.02\n",
            "iteration: 80730 loss: 0.0015 lr: 0.02\n",
            "iteration: 80740 loss: 0.0020 lr: 0.02\n",
            "iteration: 80750 loss: 0.0018 lr: 0.02\n",
            "iteration: 80760 loss: 0.0022 lr: 0.02\n",
            "iteration: 80770 loss: 0.0020 lr: 0.02\n",
            "iteration: 80780 loss: 0.0017 lr: 0.02\n",
            "iteration: 80790 loss: 0.0016 lr: 0.02\n",
            "iteration: 80800 loss: 0.0020 lr: 0.02\n",
            "iteration: 80810 loss: 0.0013 lr: 0.02\n",
            "iteration: 80820 loss: 0.0018 lr: 0.02\n",
            "iteration: 80830 loss: 0.0017 lr: 0.02\n",
            "iteration: 80840 loss: 0.0027 lr: 0.02\n",
            "iteration: 80850 loss: 0.0019 lr: 0.02\n",
            "iteration: 80860 loss: 0.0020 lr: 0.02\n",
            "iteration: 80870 loss: 0.0015 lr: 0.02\n",
            "iteration: 80880 loss: 0.0018 lr: 0.02\n",
            "iteration: 80890 loss: 0.0021 lr: 0.02\n",
            "iteration: 80900 loss: 0.0020 lr: 0.02\n",
            "iteration: 80910 loss: 0.0023 lr: 0.02\n",
            "iteration: 80920 loss: 0.0017 lr: 0.02\n",
            "iteration: 80930 loss: 0.0016 lr: 0.02\n",
            "iteration: 80940 loss: 0.0017 lr: 0.02\n",
            "iteration: 80950 loss: 0.0019 lr: 0.02\n",
            "iteration: 80960 loss: 0.0019 lr: 0.02\n",
            "iteration: 80970 loss: 0.0020 lr: 0.02\n",
            "iteration: 80980 loss: 0.0020 lr: 0.02\n",
            "iteration: 80990 loss: 0.0022 lr: 0.02\n",
            "iteration: 81000 loss: 0.0020 lr: 0.02\n",
            "iteration: 81010 loss: 0.0024 lr: 0.02\n",
            "iteration: 81020 loss: 0.0019 lr: 0.02\n",
            "iteration: 81030 loss: 0.0018 lr: 0.02\n",
            "iteration: 81040 loss: 0.0019 lr: 0.02\n",
            "iteration: 81050 loss: 0.0019 lr: 0.02\n",
            "iteration: 81060 loss: 0.0021 lr: 0.02\n",
            "iteration: 81070 loss: 0.0015 lr: 0.02\n",
            "iteration: 81080 loss: 0.0020 lr: 0.02\n",
            "iteration: 81090 loss: 0.0016 lr: 0.02\n",
            "iteration: 81100 loss: 0.0014 lr: 0.02\n",
            "iteration: 81110 loss: 0.0020 lr: 0.02\n",
            "iteration: 81120 loss: 0.0024 lr: 0.02\n",
            "iteration: 81130 loss: 0.0021 lr: 0.02\n",
            "iteration: 81140 loss: 0.0017 lr: 0.02\n",
            "iteration: 81150 loss: 0.0019 lr: 0.02\n",
            "iteration: 81160 loss: 0.0024 lr: 0.02\n",
            "iteration: 81170 loss: 0.0019 lr: 0.02\n",
            "iteration: 81180 loss: 0.0017 lr: 0.02\n",
            "iteration: 81190 loss: 0.0019 lr: 0.02\n",
            "iteration: 81200 loss: 0.0022 lr: 0.02\n",
            "iteration: 81210 loss: 0.0016 lr: 0.02\n",
            "iteration: 81220 loss: 0.0021 lr: 0.02\n",
            "iteration: 81230 loss: 0.0021 lr: 0.02\n",
            "iteration: 81240 loss: 0.0023 lr: 0.02\n",
            "iteration: 81250 loss: 0.0019 lr: 0.02\n",
            "iteration: 81260 loss: 0.0021 lr: 0.02\n",
            "iteration: 81270 loss: 0.0018 lr: 0.02\n",
            "iteration: 81280 loss: 0.0018 lr: 0.02\n",
            "iteration: 81290 loss: 0.0018 lr: 0.02\n",
            "iteration: 81300 loss: 0.0017 lr: 0.02\n",
            "iteration: 81310 loss: 0.0016 lr: 0.02\n",
            "iteration: 81320 loss: 0.0020 lr: 0.02\n",
            "iteration: 81330 loss: 0.0021 lr: 0.02\n",
            "iteration: 81340 loss: 0.0020 lr: 0.02\n",
            "iteration: 81350 loss: 0.0019 lr: 0.02\n",
            "iteration: 81360 loss: 0.0026 lr: 0.02\n",
            "iteration: 81370 loss: 0.0019 lr: 0.02\n",
            "iteration: 81380 loss: 0.0021 lr: 0.02\n",
            "iteration: 81390 loss: 0.0015 lr: 0.02\n",
            "iteration: 81400 loss: 0.0020 lr: 0.02\n",
            "iteration: 81410 loss: 0.0021 lr: 0.02\n",
            "iteration: 81420 loss: 0.0020 lr: 0.02\n",
            "iteration: 81430 loss: 0.0018 lr: 0.02\n",
            "iteration: 81440 loss: 0.0017 lr: 0.02\n",
            "iteration: 81450 loss: 0.0022 lr: 0.02\n",
            "iteration: 81460 loss: 0.0018 lr: 0.02\n",
            "iteration: 81470 loss: 0.0020 lr: 0.02\n",
            "iteration: 81480 loss: 0.0022 lr: 0.02\n",
            "iteration: 81490 loss: 0.0021 lr: 0.02\n",
            "iteration: 81500 loss: 0.0017 lr: 0.02\n",
            "iteration: 81510 loss: 0.0018 lr: 0.02\n",
            "iteration: 81520 loss: 0.0015 lr: 0.02\n",
            "iteration: 81530 loss: 0.0015 lr: 0.02\n",
            "iteration: 81540 loss: 0.0017 lr: 0.02\n",
            "iteration: 81550 loss: 0.0016 lr: 0.02\n",
            "iteration: 81560 loss: 0.0019 lr: 0.02\n",
            "iteration: 81570 loss: 0.0022 lr: 0.02\n",
            "iteration: 81580 loss: 0.0015 lr: 0.02\n",
            "iteration: 81590 loss: 0.0017 lr: 0.02\n",
            "iteration: 81600 loss: 0.0021 lr: 0.02\n",
            "iteration: 81610 loss: 0.0019 lr: 0.02\n",
            "iteration: 81620 loss: 0.0018 lr: 0.02\n",
            "iteration: 81630 loss: 0.0016 lr: 0.02\n",
            "iteration: 81640 loss: 0.0021 lr: 0.02\n",
            "iteration: 81650 loss: 0.0020 lr: 0.02\n",
            "iteration: 81660 loss: 0.0017 lr: 0.02\n",
            "iteration: 81670 loss: 0.0017 lr: 0.02\n",
            "iteration: 81680 loss: 0.0022 lr: 0.02\n",
            "iteration: 81690 loss: 0.0018 lr: 0.02\n",
            "iteration: 81700 loss: 0.0017 lr: 0.02\n",
            "iteration: 81710 loss: 0.0021 lr: 0.02\n",
            "iteration: 81720 loss: 0.0014 lr: 0.02\n",
            "iteration: 81730 loss: 0.0017 lr: 0.02\n",
            "iteration: 81740 loss: 0.0018 lr: 0.02\n",
            "iteration: 81750 loss: 0.0014 lr: 0.02\n",
            "iteration: 81760 loss: 0.0024 lr: 0.02\n",
            "iteration: 81770 loss: 0.0017 lr: 0.02\n",
            "iteration: 81780 loss: 0.0023 lr: 0.02\n",
            "iteration: 81790 loss: 0.0015 lr: 0.02\n",
            "iteration: 81800 loss: 0.0017 lr: 0.02\n",
            "iteration: 81810 loss: 0.0019 lr: 0.02\n",
            "iteration: 81820 loss: 0.0024 lr: 0.02\n",
            "iteration: 81830 loss: 0.0019 lr: 0.02\n",
            "iteration: 81840 loss: 0.0019 lr: 0.02\n",
            "iteration: 81850 loss: 0.0014 lr: 0.02\n",
            "iteration: 81860 loss: 0.0017 lr: 0.02\n",
            "iteration: 81870 loss: 0.0019 lr: 0.02\n",
            "iteration: 81880 loss: 0.0017 lr: 0.02\n",
            "iteration: 81890 loss: 0.0020 lr: 0.02\n",
            "iteration: 81900 loss: 0.0019 lr: 0.02\n",
            "iteration: 81910 loss: 0.0018 lr: 0.02\n",
            "iteration: 81920 loss: 0.0018 lr: 0.02\n",
            "iteration: 81930 loss: 0.0019 lr: 0.02\n",
            "iteration: 81940 loss: 0.0022 lr: 0.02\n",
            "iteration: 81950 loss: 0.0018 lr: 0.02\n",
            "iteration: 81960 loss: 0.0021 lr: 0.02\n",
            "iteration: 81970 loss: 0.0026 lr: 0.02\n",
            "iteration: 81980 loss: 0.0029 lr: 0.02\n",
            "iteration: 81990 loss: 0.0018 lr: 0.02\n",
            "iteration: 82000 loss: 0.0020 lr: 0.02\n",
            "iteration: 82010 loss: 0.0014 lr: 0.02\n",
            "iteration: 82020 loss: 0.0020 lr: 0.02\n",
            "iteration: 82030 loss: 0.0020 lr: 0.02\n",
            "iteration: 82040 loss: 0.0018 lr: 0.02\n",
            "iteration: 82050 loss: 0.0019 lr: 0.02\n",
            "iteration: 82060 loss: 0.0016 lr: 0.02\n",
            "iteration: 82070 loss: 0.0023 lr: 0.02\n",
            "iteration: 82080 loss: 0.0017 lr: 0.02\n",
            "iteration: 82090 loss: 0.0018 lr: 0.02\n",
            "iteration: 82100 loss: 0.0019 lr: 0.02\n",
            "iteration: 82110 loss: 0.0020 lr: 0.02\n",
            "iteration: 82120 loss: 0.0022 lr: 0.02\n",
            "iteration: 82130 loss: 0.0015 lr: 0.02\n",
            "iteration: 82140 loss: 0.0019 lr: 0.02\n",
            "iteration: 82150 loss: 0.0014 lr: 0.02\n",
            "iteration: 82160 loss: 0.0018 lr: 0.02\n",
            "iteration: 82170 loss: 0.0020 lr: 0.02\n",
            "iteration: 82180 loss: 0.0020 lr: 0.02\n",
            "iteration: 82190 loss: 0.0022 lr: 0.02\n",
            "iteration: 82200 loss: 0.0019 lr: 0.02\n",
            "iteration: 82210 loss: 0.0020 lr: 0.02\n",
            "iteration: 82220 loss: 0.0019 lr: 0.02\n",
            "iteration: 82230 loss: 0.0022 lr: 0.02\n",
            "iteration: 82240 loss: 0.0018 lr: 0.02\n",
            "iteration: 82250 loss: 0.0020 lr: 0.02\n",
            "iteration: 82260 loss: 0.0021 lr: 0.02\n",
            "iteration: 82270 loss: 0.0019 lr: 0.02\n",
            "iteration: 82280 loss: 0.0021 lr: 0.02\n",
            "iteration: 82290 loss: 0.0018 lr: 0.02\n",
            "iteration: 82300 loss: 0.0021 lr: 0.02\n",
            "iteration: 82310 loss: 0.0019 lr: 0.02\n",
            "iteration: 82320 loss: 0.0017 lr: 0.02\n",
            "iteration: 82330 loss: 0.0017 lr: 0.02\n",
            "iteration: 82340 loss: 0.0023 lr: 0.02\n",
            "iteration: 82350 loss: 0.0016 lr: 0.02\n",
            "iteration: 82360 loss: 0.0016 lr: 0.02\n",
            "iteration: 82370 loss: 0.0023 lr: 0.02\n",
            "iteration: 82380 loss: 0.0020 lr: 0.02\n",
            "iteration: 82390 loss: 0.0016 lr: 0.02\n",
            "iteration: 82400 loss: 0.0019 lr: 0.02\n",
            "iteration: 82410 loss: 0.0015 lr: 0.02\n",
            "iteration: 82420 loss: 0.0022 lr: 0.02\n",
            "iteration: 82430 loss: 0.0012 lr: 0.02\n",
            "iteration: 82440 loss: 0.0030 lr: 0.02\n",
            "iteration: 82450 loss: 0.0019 lr: 0.02\n",
            "iteration: 82460 loss: 0.0018 lr: 0.02\n",
            "iteration: 82470 loss: 0.0021 lr: 0.02\n",
            "iteration: 82480 loss: 0.0020 lr: 0.02\n",
            "iteration: 82490 loss: 0.0014 lr: 0.02\n",
            "iteration: 82500 loss: 0.0020 lr: 0.02\n",
            "iteration: 82510 loss: 0.0022 lr: 0.02\n",
            "iteration: 82520 loss: 0.0022 lr: 0.02\n",
            "iteration: 82530 loss: 0.0017 lr: 0.02\n",
            "iteration: 82540 loss: 0.0022 lr: 0.02\n",
            "iteration: 82550 loss: 0.0017 lr: 0.02\n",
            "iteration: 82560 loss: 0.0012 lr: 0.02\n",
            "iteration: 82570 loss: 0.0015 lr: 0.02\n",
            "iteration: 82580 loss: 0.0020 lr: 0.02\n",
            "iteration: 82590 loss: 0.0017 lr: 0.02\n",
            "iteration: 82600 loss: 0.0021 lr: 0.02\n",
            "iteration: 82610 loss: 0.0017 lr: 0.02\n",
            "iteration: 82620 loss: 0.0023 lr: 0.02\n",
            "iteration: 82630 loss: 0.0017 lr: 0.02\n",
            "iteration: 82640 loss: 0.0015 lr: 0.02\n",
            "iteration: 82650 loss: 0.0021 lr: 0.02\n",
            "iteration: 82660 loss: 0.0022 lr: 0.02\n",
            "iteration: 82670 loss: 0.0016 lr: 0.02\n",
            "iteration: 82680 loss: 0.0019 lr: 0.02\n",
            "iteration: 82690 loss: 0.0025 lr: 0.02\n",
            "iteration: 82700 loss: 0.0016 lr: 0.02\n",
            "iteration: 82710 loss: 0.0019 lr: 0.02\n",
            "iteration: 82720 loss: 0.0019 lr: 0.02\n",
            "iteration: 82730 loss: 0.0017 lr: 0.02\n",
            "iteration: 82740 loss: 0.0015 lr: 0.02\n",
            "iteration: 82750 loss: 0.0017 lr: 0.02\n",
            "iteration: 82760 loss: 0.0015 lr: 0.02\n",
            "iteration: 82770 loss: 0.0020 lr: 0.02\n",
            "iteration: 82780 loss: 0.0020 lr: 0.02\n",
            "iteration: 82790 loss: 0.0019 lr: 0.02\n",
            "iteration: 82800 loss: 0.0015 lr: 0.02\n",
            "iteration: 82810 loss: 0.0022 lr: 0.02\n",
            "iteration: 82820 loss: 0.0016 lr: 0.02\n",
            "iteration: 82830 loss: 0.0017 lr: 0.02\n",
            "iteration: 82840 loss: 0.0025 lr: 0.02\n",
            "iteration: 82850 loss: 0.0018 lr: 0.02\n",
            "iteration: 82860 loss: 0.0018 lr: 0.02\n",
            "iteration: 82870 loss: 0.0018 lr: 0.02\n",
            "iteration: 82880 loss: 0.0026 lr: 0.02\n",
            "iteration: 82890 loss: 0.0015 lr: 0.02\n",
            "iteration: 82900 loss: 0.0018 lr: 0.02\n",
            "iteration: 82910 loss: 0.0017 lr: 0.02\n",
            "iteration: 82920 loss: 0.0020 lr: 0.02\n",
            "iteration: 82930 loss: 0.0018 lr: 0.02\n",
            "iteration: 82940 loss: 0.0021 lr: 0.02\n",
            "iteration: 82950 loss: 0.0018 lr: 0.02\n",
            "iteration: 82960 loss: 0.0019 lr: 0.02\n",
            "iteration: 82970 loss: 0.0015 lr: 0.02\n",
            "iteration: 82980 loss: 0.0016 lr: 0.02\n",
            "iteration: 82990 loss: 0.0023 lr: 0.02\n",
            "iteration: 83000 loss: 0.0022 lr: 0.02\n",
            "iteration: 83010 loss: 0.0016 lr: 0.02\n",
            "iteration: 83020 loss: 0.0021 lr: 0.02\n",
            "iteration: 83030 loss: 0.0012 lr: 0.02\n",
            "iteration: 83040 loss: 0.0018 lr: 0.02\n",
            "iteration: 83050 loss: 0.0021 lr: 0.02\n",
            "iteration: 83060 loss: 0.0020 lr: 0.02\n",
            "iteration: 83070 loss: 0.0018 lr: 0.02\n",
            "iteration: 83080 loss: 0.0020 lr: 0.02\n",
            "iteration: 83090 loss: 0.0018 lr: 0.02\n",
            "iteration: 83100 loss: 0.0016 lr: 0.02\n",
            "iteration: 83110 loss: 0.0018 lr: 0.02\n",
            "iteration: 83120 loss: 0.0021 lr: 0.02\n",
            "iteration: 83130 loss: 0.0019 lr: 0.02\n",
            "iteration: 83140 loss: 0.0015 lr: 0.02\n",
            "iteration: 83150 loss: 0.0015 lr: 0.02\n",
            "iteration: 83160 loss: 0.0017 lr: 0.02\n",
            "iteration: 83170 loss: 0.0020 lr: 0.02\n",
            "iteration: 83180 loss: 0.0019 lr: 0.02\n",
            "iteration: 83190 loss: 0.0018 lr: 0.02\n",
            "iteration: 83200 loss: 0.0014 lr: 0.02\n",
            "iteration: 83210 loss: 0.0014 lr: 0.02\n",
            "iteration: 83220 loss: 0.0014 lr: 0.02\n",
            "iteration: 83230 loss: 0.0019 lr: 0.02\n",
            "iteration: 83240 loss: 0.0019 lr: 0.02\n",
            "iteration: 83250 loss: 0.0020 lr: 0.02\n",
            "iteration: 83260 loss: 0.0018 lr: 0.02\n",
            "iteration: 83270 loss: 0.0016 lr: 0.02\n",
            "iteration: 83280 loss: 0.0024 lr: 0.02\n",
            "iteration: 83290 loss: 0.0016 lr: 0.02\n",
            "iteration: 83300 loss: 0.0015 lr: 0.02\n",
            "iteration: 83310 loss: 0.0021 lr: 0.02\n",
            "iteration: 83320 loss: 0.0018 lr: 0.02\n",
            "iteration: 83330 loss: 0.0019 lr: 0.02\n",
            "iteration: 83340 loss: 0.0020 lr: 0.02\n",
            "iteration: 83350 loss: 0.0022 lr: 0.02\n",
            "iteration: 83360 loss: 0.0018 lr: 0.02\n",
            "iteration: 83370 loss: 0.0017 lr: 0.02\n",
            "iteration: 83380 loss: 0.0017 lr: 0.02\n",
            "iteration: 83390 loss: 0.0021 lr: 0.02\n",
            "iteration: 83400 loss: 0.0021 lr: 0.02\n",
            "iteration: 83410 loss: 0.0016 lr: 0.02\n",
            "iteration: 83420 loss: 0.0015 lr: 0.02\n",
            "iteration: 83430 loss: 0.0016 lr: 0.02\n",
            "iteration: 83440 loss: 0.0024 lr: 0.02\n",
            "iteration: 83450 loss: 0.0019 lr: 0.02\n",
            "iteration: 83460 loss: 0.0023 lr: 0.02\n",
            "iteration: 83470 loss: 0.0019 lr: 0.02\n",
            "iteration: 83480 loss: 0.0020 lr: 0.02\n",
            "iteration: 83490 loss: 0.0022 lr: 0.02\n",
            "iteration: 83500 loss: 0.0017 lr: 0.02\n",
            "iteration: 83510 loss: 0.0020 lr: 0.02\n",
            "iteration: 83520 loss: 0.0024 lr: 0.02\n",
            "iteration: 83530 loss: 0.0016 lr: 0.02\n",
            "iteration: 83540 loss: 0.0024 lr: 0.02\n",
            "iteration: 83550 loss: 0.0019 lr: 0.02\n",
            "iteration: 83560 loss: 0.0025 lr: 0.02\n",
            "iteration: 83570 loss: 0.0018 lr: 0.02\n",
            "iteration: 83580 loss: 0.0022 lr: 0.02\n",
            "iteration: 83590 loss: 0.0021 lr: 0.02\n",
            "iteration: 83600 loss: 0.0020 lr: 0.02\n",
            "iteration: 83610 loss: 0.0022 lr: 0.02\n",
            "iteration: 83620 loss: 0.0020 lr: 0.02\n",
            "iteration: 83630 loss: 0.0017 lr: 0.02\n",
            "iteration: 83640 loss: 0.0020 lr: 0.02\n",
            "iteration: 83650 loss: 0.0020 lr: 0.02\n",
            "iteration: 83660 loss: 0.0016 lr: 0.02\n",
            "iteration: 83670 loss: 0.0040 lr: 0.02\n",
            "iteration: 83680 loss: 0.0022 lr: 0.02\n",
            "iteration: 83690 loss: 0.0018 lr: 0.02\n",
            "iteration: 83700 loss: 0.0022 lr: 0.02\n",
            "iteration: 83710 loss: 0.0020 lr: 0.02\n",
            "iteration: 83720 loss: 0.0016 lr: 0.02\n",
            "iteration: 83730 loss: 0.0016 lr: 0.02\n",
            "iteration: 83740 loss: 0.0019 lr: 0.02\n",
            "iteration: 83750 loss: 0.0020 lr: 0.02\n",
            "iteration: 83760 loss: 0.0017 lr: 0.02\n",
            "iteration: 83770 loss: 0.0021 lr: 0.02\n",
            "iteration: 83780 loss: 0.0020 lr: 0.02\n",
            "iteration: 83790 loss: 0.0016 lr: 0.02\n",
            "iteration: 83800 loss: 0.0020 lr: 0.02\n",
            "iteration: 83810 loss: 0.0021 lr: 0.02\n",
            "iteration: 83820 loss: 0.0017 lr: 0.02\n",
            "iteration: 83830 loss: 0.0017 lr: 0.02\n",
            "iteration: 83840 loss: 0.0016 lr: 0.02\n",
            "iteration: 83850 loss: 0.0018 lr: 0.02\n",
            "iteration: 83860 loss: 0.0014 lr: 0.02\n",
            "iteration: 83870 loss: 0.0018 lr: 0.02\n",
            "iteration: 83880 loss: 0.0021 lr: 0.02\n",
            "iteration: 83890 loss: 0.0016 lr: 0.02\n",
            "iteration: 83900 loss: 0.0018 lr: 0.02\n",
            "iteration: 83910 loss: 0.0018 lr: 0.02\n",
            "iteration: 83920 loss: 0.0018 lr: 0.02\n",
            "iteration: 83930 loss: 0.0021 lr: 0.02\n",
            "iteration: 83940 loss: 0.0020 lr: 0.02\n",
            "iteration: 83950 loss: 0.0015 lr: 0.02\n",
            "iteration: 83960 loss: 0.0025 lr: 0.02\n",
            "iteration: 83970 loss: 0.0018 lr: 0.02\n",
            "iteration: 83980 loss: 0.0019 lr: 0.02\n",
            "iteration: 83990 loss: 0.0018 lr: 0.02\n",
            "iteration: 84000 loss: 0.0020 lr: 0.02\n",
            "iteration: 84010 loss: 0.0017 lr: 0.02\n",
            "iteration: 84020 loss: 0.0015 lr: 0.02\n",
            "iteration: 84030 loss: 0.0016 lr: 0.02\n",
            "iteration: 84040 loss: 0.0018 lr: 0.02\n",
            "iteration: 84050 loss: 0.0018 lr: 0.02\n",
            "iteration: 84060 loss: 0.0022 lr: 0.02\n",
            "iteration: 84070 loss: 0.0025 lr: 0.02\n",
            "iteration: 84080 loss: 0.0018 lr: 0.02\n",
            "iteration: 84090 loss: 0.0019 lr: 0.02\n",
            "iteration: 84100 loss: 0.0018 lr: 0.02\n",
            "iteration: 84110 loss: 0.0019 lr: 0.02\n",
            "iteration: 84120 loss: 0.0017 lr: 0.02\n",
            "iteration: 84130 loss: 0.0020 lr: 0.02\n",
            "iteration: 84140 loss: 0.0022 lr: 0.02\n",
            "iteration: 84150 loss: 0.0018 lr: 0.02\n",
            "iteration: 84160 loss: 0.0018 lr: 0.02\n",
            "iteration: 84170 loss: 0.0016 lr: 0.02\n",
            "iteration: 84180 loss: 0.0023 lr: 0.02\n",
            "iteration: 84190 loss: 0.0017 lr: 0.02\n",
            "iteration: 84200 loss: 0.0019 lr: 0.02\n",
            "iteration: 84210 loss: 0.0016 lr: 0.02\n",
            "iteration: 84220 loss: 0.0019 lr: 0.02\n",
            "iteration: 84230 loss: 0.0016 lr: 0.02\n",
            "iteration: 84240 loss: 0.0018 lr: 0.02\n",
            "iteration: 84250 loss: 0.0019 lr: 0.02\n",
            "iteration: 84260 loss: 0.0023 lr: 0.02\n",
            "iteration: 84270 loss: 0.0019 lr: 0.02\n",
            "iteration: 84280 loss: 0.0019 lr: 0.02\n",
            "iteration: 84290 loss: 0.0019 lr: 0.02\n",
            "iteration: 84300 loss: 0.0021 lr: 0.02\n",
            "iteration: 84310 loss: 0.0024 lr: 0.02\n",
            "iteration: 84320 loss: 0.0030 lr: 0.02\n",
            "iteration: 84330 loss: 0.0017 lr: 0.02\n",
            "iteration: 84340 loss: 0.0022 lr: 0.02\n",
            "iteration: 84350 loss: 0.0020 lr: 0.02\n",
            "iteration: 84360 loss: 0.0020 lr: 0.02\n",
            "iteration: 84370 loss: 0.0026 lr: 0.02\n",
            "iteration: 84380 loss: 0.0014 lr: 0.02\n",
            "iteration: 84390 loss: 0.0023 lr: 0.02\n",
            "iteration: 84400 loss: 0.0016 lr: 0.02\n",
            "iteration: 84410 loss: 0.0018 lr: 0.02\n",
            "iteration: 84420 loss: 0.0019 lr: 0.02\n",
            "iteration: 84430 loss: 0.0013 lr: 0.02\n",
            "iteration: 84440 loss: 0.0018 lr: 0.02\n",
            "iteration: 84450 loss: 0.0017 lr: 0.02\n",
            "iteration: 84460 loss: 0.0022 lr: 0.02\n",
            "iteration: 84470 loss: 0.0016 lr: 0.02\n",
            "iteration: 84480 loss: 0.0017 lr: 0.02\n",
            "iteration: 84490 loss: 0.0020 lr: 0.02\n",
            "iteration: 84500 loss: 0.0015 lr: 0.02\n",
            "iteration: 84510 loss: 0.0016 lr: 0.02\n",
            "iteration: 84520 loss: 0.0027 lr: 0.02\n",
            "iteration: 84530 loss: 0.0017 lr: 0.02\n",
            "iteration: 84540 loss: 0.0018 lr: 0.02\n",
            "iteration: 84550 loss: 0.0018 lr: 0.02\n",
            "iteration: 84560 loss: 0.0021 lr: 0.02\n",
            "iteration: 84570 loss: 0.0019 lr: 0.02\n",
            "iteration: 84580 loss: 0.0018 lr: 0.02\n",
            "iteration: 84590 loss: 0.0014 lr: 0.02\n",
            "iteration: 84600 loss: 0.0018 lr: 0.02\n",
            "iteration: 84610 loss: 0.0017 lr: 0.02\n",
            "iteration: 84620 loss: 0.0014 lr: 0.02\n",
            "iteration: 84630 loss: 0.0016 lr: 0.02\n",
            "iteration: 84640 loss: 0.0020 lr: 0.02\n",
            "iteration: 84650 loss: 0.0015 lr: 0.02\n",
            "iteration: 84660 loss: 0.0020 lr: 0.02\n",
            "iteration: 84670 loss: 0.0026 lr: 0.02\n",
            "iteration: 84680 loss: 0.0021 lr: 0.02\n",
            "iteration: 84690 loss: 0.0018 lr: 0.02\n",
            "iteration: 84700 loss: 0.0018 lr: 0.02\n",
            "iteration: 84710 loss: 0.0021 lr: 0.02\n",
            "iteration: 84720 loss: 0.0020 lr: 0.02\n",
            "iteration: 84730 loss: 0.0015 lr: 0.02\n",
            "iteration: 84740 loss: 0.0013 lr: 0.02\n",
            "iteration: 84750 loss: 0.0020 lr: 0.02\n",
            "iteration: 84760 loss: 0.0024 lr: 0.02\n",
            "iteration: 84770 loss: 0.0019 lr: 0.02\n",
            "iteration: 84780 loss: 0.0024 lr: 0.02\n",
            "iteration: 84790 loss: 0.0017 lr: 0.02\n",
            "iteration: 84800 loss: 0.0021 lr: 0.02\n",
            "iteration: 84810 loss: 0.0014 lr: 0.02\n",
            "iteration: 84820 loss: 0.0019 lr: 0.02\n",
            "iteration: 84830 loss: 0.0021 lr: 0.02\n",
            "iteration: 84840 loss: 0.0017 lr: 0.02\n",
            "iteration: 84850 loss: 0.0021 lr: 0.02\n",
            "iteration: 84860 loss: 0.0017 lr: 0.02\n",
            "iteration: 84870 loss: 0.0026 lr: 0.02\n",
            "iteration: 84880 loss: 0.0017 lr: 0.02\n",
            "iteration: 84890 loss: 0.0019 lr: 0.02\n",
            "iteration: 84900 loss: 0.0017 lr: 0.02\n",
            "iteration: 84910 loss: 0.0023 lr: 0.02\n",
            "iteration: 84920 loss: 0.0016 lr: 0.02\n",
            "iteration: 84930 loss: 0.0014 lr: 0.02\n",
            "iteration: 84940 loss: 0.0019 lr: 0.02\n",
            "iteration: 84950 loss: 0.0020 lr: 0.02\n",
            "iteration: 84960 loss: 0.0021 lr: 0.02\n",
            "iteration: 84970 loss: 0.0022 lr: 0.02\n",
            "iteration: 84980 loss: 0.0020 lr: 0.02\n",
            "iteration: 84990 loss: 0.0017 lr: 0.02\n",
            "iteration: 85000 loss: 0.0018 lr: 0.02\n",
            "iteration: 85010 loss: 0.0024 lr: 0.02\n",
            "iteration: 85020 loss: 0.0014 lr: 0.02\n",
            "iteration: 85030 loss: 0.0022 lr: 0.02\n",
            "iteration: 85040 loss: 0.0015 lr: 0.02\n",
            "iteration: 85050 loss: 0.0015 lr: 0.02\n",
            "iteration: 85060 loss: 0.0016 lr: 0.02\n",
            "iteration: 85070 loss: 0.0020 lr: 0.02\n",
            "iteration: 85080 loss: 0.0018 lr: 0.02\n",
            "iteration: 85090 loss: 0.0017 lr: 0.02\n",
            "iteration: 85100 loss: 0.0015 lr: 0.02\n",
            "iteration: 85110 loss: 0.0022 lr: 0.02\n",
            "iteration: 85120 loss: 0.0015 lr: 0.02\n",
            "iteration: 85130 loss: 0.0020 lr: 0.02\n",
            "iteration: 85140 loss: 0.0018 lr: 0.02\n",
            "iteration: 85150 loss: 0.0018 lr: 0.02\n",
            "iteration: 85160 loss: 0.0018 lr: 0.02\n",
            "iteration: 85170 loss: 0.0018 lr: 0.02\n",
            "iteration: 85180 loss: 0.0015 lr: 0.02\n",
            "iteration: 85190 loss: 0.0017 lr: 0.02\n",
            "iteration: 85200 loss: 0.0018 lr: 0.02\n",
            "iteration: 85210 loss: 0.0019 lr: 0.02\n",
            "iteration: 85220 loss: 0.0021 lr: 0.02\n",
            "iteration: 85230 loss: 0.0018 lr: 0.02\n",
            "iteration: 85240 loss: 0.0017 lr: 0.02\n",
            "iteration: 85250 loss: 0.0018 lr: 0.02\n",
            "iteration: 85260 loss: 0.0018 lr: 0.02\n",
            "iteration: 85270 loss: 0.0020 lr: 0.02\n",
            "iteration: 85280 loss: 0.0017 lr: 0.02\n",
            "iteration: 85290 loss: 0.0022 lr: 0.02\n",
            "iteration: 85300 loss: 0.0016 lr: 0.02\n",
            "iteration: 85310 loss: 0.0016 lr: 0.02\n",
            "iteration: 85320 loss: 0.0018 lr: 0.02\n",
            "iteration: 85330 loss: 0.0021 lr: 0.02\n",
            "iteration: 85340 loss: 0.0016 lr: 0.02\n",
            "iteration: 85350 loss: 0.0028 lr: 0.02\n",
            "iteration: 85360 loss: 0.0018 lr: 0.02\n",
            "iteration: 85370 loss: 0.0020 lr: 0.02\n",
            "iteration: 85380 loss: 0.0021 lr: 0.02\n",
            "iteration: 85390 loss: 0.0019 lr: 0.02\n",
            "iteration: 85400 loss: 0.0018 lr: 0.02\n",
            "iteration: 85410 loss: 0.0022 lr: 0.02\n",
            "iteration: 85420 loss: 0.0030 lr: 0.02\n",
            "iteration: 85430 loss: 0.0018 lr: 0.02\n",
            "iteration: 85440 loss: 0.0021 lr: 0.02\n",
            "iteration: 85450 loss: 0.0019 lr: 0.02\n",
            "iteration: 85460 loss: 0.0019 lr: 0.02\n",
            "iteration: 85470 loss: 0.0015 lr: 0.02\n",
            "iteration: 85480 loss: 0.0023 lr: 0.02\n",
            "iteration: 85490 loss: 0.0026 lr: 0.02\n",
            "iteration: 85500 loss: 0.0016 lr: 0.02\n",
            "iteration: 85510 loss: 0.0020 lr: 0.02\n",
            "iteration: 85520 loss: 0.0023 lr: 0.02\n",
            "iteration: 85530 loss: 0.0021 lr: 0.02\n",
            "iteration: 85540 loss: 0.0019 lr: 0.02\n",
            "iteration: 85550 loss: 0.0019 lr: 0.02\n",
            "iteration: 85560 loss: 0.0022 lr: 0.02\n",
            "iteration: 85570 loss: 0.0019 lr: 0.02\n",
            "iteration: 85580 loss: 0.0020 lr: 0.02\n",
            "iteration: 85590 loss: 0.0018 lr: 0.02\n",
            "iteration: 85600 loss: 0.0022 lr: 0.02\n",
            "iteration: 85610 loss: 0.0019 lr: 0.02\n",
            "iteration: 85620 loss: 0.0022 lr: 0.02\n",
            "iteration: 85630 loss: 0.0020 lr: 0.02\n",
            "iteration: 85640 loss: 0.0021 lr: 0.02\n",
            "iteration: 85650 loss: 0.0019 lr: 0.02\n",
            "iteration: 85660 loss: 0.0019 lr: 0.02\n",
            "iteration: 85670 loss: 0.0014 lr: 0.02\n",
            "iteration: 85680 loss: 0.0018 lr: 0.02\n",
            "iteration: 85690 loss: 0.0018 lr: 0.02\n",
            "iteration: 85700 loss: 0.0017 lr: 0.02\n",
            "iteration: 85710 loss: 0.0017 lr: 0.02\n",
            "iteration: 85720 loss: 0.0018 lr: 0.02\n",
            "iteration: 85730 loss: 0.0022 lr: 0.02\n",
            "iteration: 85740 loss: 0.0023 lr: 0.02\n",
            "iteration: 85750 loss: 0.0021 lr: 0.02\n",
            "iteration: 85760 loss: 0.0017 lr: 0.02\n",
            "iteration: 85770 loss: 0.0022 lr: 0.02\n",
            "iteration: 85780 loss: 0.0019 lr: 0.02\n",
            "iteration: 85790 loss: 0.0016 lr: 0.02\n",
            "iteration: 85800 loss: 0.0015 lr: 0.02\n",
            "iteration: 85810 loss: 0.0017 lr: 0.02\n",
            "iteration: 85820 loss: 0.0017 lr: 0.02\n",
            "iteration: 85830 loss: 0.0023 lr: 0.02\n",
            "iteration: 85840 loss: 0.0019 lr: 0.02\n",
            "iteration: 85850 loss: 0.0019 lr: 0.02\n",
            "iteration: 85860 loss: 0.0016 lr: 0.02\n",
            "iteration: 85870 loss: 0.0014 lr: 0.02\n",
            "iteration: 85880 loss: 0.0015 lr: 0.02\n",
            "iteration: 85890 loss: 0.0022 lr: 0.02\n",
            "iteration: 85900 loss: 0.0026 lr: 0.02\n",
            "iteration: 85910 loss: 0.0017 lr: 0.02\n",
            "iteration: 85920 loss: 0.0019 lr: 0.02\n",
            "iteration: 85930 loss: 0.0022 lr: 0.02\n",
            "iteration: 85940 loss: 0.0015 lr: 0.02\n",
            "iteration: 85950 loss: 0.0017 lr: 0.02\n",
            "iteration: 85960 loss: 0.0021 lr: 0.02\n",
            "iteration: 85970 loss: 0.0017 lr: 0.02\n",
            "iteration: 85980 loss: 0.0020 lr: 0.02\n",
            "iteration: 85990 loss: 0.0023 lr: 0.02\n",
            "iteration: 86000 loss: 0.0022 lr: 0.02\n",
            "iteration: 86010 loss: 0.0020 lr: 0.02\n",
            "iteration: 86020 loss: 0.0017 lr: 0.02\n",
            "iteration: 86030 loss: 0.0024 lr: 0.02\n",
            "iteration: 86040 loss: 0.0018 lr: 0.02\n",
            "iteration: 86050 loss: 0.0018 lr: 0.02\n",
            "iteration: 86060 loss: 0.0019 lr: 0.02\n",
            "iteration: 86070 loss: 0.0020 lr: 0.02\n",
            "iteration: 86080 loss: 0.0016 lr: 0.02\n",
            "iteration: 86090 loss: 0.0016 lr: 0.02\n",
            "iteration: 86100 loss: 0.0014 lr: 0.02\n",
            "iteration: 86110 loss: 0.0020 lr: 0.02\n",
            "iteration: 86120 loss: 0.0018 lr: 0.02\n",
            "iteration: 86130 loss: 0.0019 lr: 0.02\n",
            "iteration: 86140 loss: 0.0023 lr: 0.02\n",
            "iteration: 86150 loss: 0.0023 lr: 0.02\n",
            "iteration: 86160 loss: 0.0016 lr: 0.02\n",
            "iteration: 86170 loss: 0.0020 lr: 0.02\n",
            "iteration: 86180 loss: 0.0019 lr: 0.02\n",
            "iteration: 86190 loss: 0.0019 lr: 0.02\n",
            "iteration: 86200 loss: 0.0020 lr: 0.02\n",
            "iteration: 86210 loss: 0.0018 lr: 0.02\n",
            "iteration: 86220 loss: 0.0017 lr: 0.02\n",
            "iteration: 86230 loss: 0.0019 lr: 0.02\n",
            "iteration: 86240 loss: 0.0016 lr: 0.02\n",
            "iteration: 86250 loss: 0.0015 lr: 0.02\n",
            "iteration: 86260 loss: 0.0018 lr: 0.02\n",
            "iteration: 86270 loss: 0.0016 lr: 0.02\n",
            "iteration: 86280 loss: 0.0020 lr: 0.02\n",
            "iteration: 86290 loss: 0.0014 lr: 0.02\n",
            "iteration: 86300 loss: 0.0016 lr: 0.02\n",
            "iteration: 86310 loss: 0.0022 lr: 0.02\n",
            "iteration: 86320 loss: 0.0022 lr: 0.02\n",
            "iteration: 86330 loss: 0.0015 lr: 0.02\n",
            "iteration: 86340 loss: 0.0019 lr: 0.02\n",
            "iteration: 86350 loss: 0.0016 lr: 0.02\n",
            "iteration: 86360 loss: 0.0016 lr: 0.02\n",
            "iteration: 86370 loss: 0.0017 lr: 0.02\n",
            "iteration: 86380 loss: 0.0018 lr: 0.02\n",
            "iteration: 86390 loss: 0.0017 lr: 0.02\n",
            "iteration: 86400 loss: 0.0022 lr: 0.02\n",
            "iteration: 86410 loss: 0.0017 lr: 0.02\n",
            "iteration: 86420 loss: 0.0016 lr: 0.02\n",
            "iteration: 86430 loss: 0.0016 lr: 0.02\n",
            "iteration: 86440 loss: 0.0021 lr: 0.02\n",
            "iteration: 86450 loss: 0.0019 lr: 0.02\n",
            "iteration: 86460 loss: 0.0019 lr: 0.02\n",
            "iteration: 86470 loss: 0.0023 lr: 0.02\n",
            "iteration: 86480 loss: 0.0023 lr: 0.02\n",
            "iteration: 86490 loss: 0.0021 lr: 0.02\n",
            "iteration: 86500 loss: 0.0020 lr: 0.02\n",
            "iteration: 86510 loss: 0.0025 lr: 0.02\n",
            "iteration: 86520 loss: 0.0021 lr: 0.02\n",
            "iteration: 86530 loss: 0.0015 lr: 0.02\n",
            "iteration: 86540 loss: 0.0018 lr: 0.02\n",
            "iteration: 86550 loss: 0.0016 lr: 0.02\n",
            "iteration: 86560 loss: 0.0017 lr: 0.02\n",
            "iteration: 86570 loss: 0.0030 lr: 0.02\n",
            "iteration: 86580 loss: 0.0019 lr: 0.02\n",
            "iteration: 86590 loss: 0.0017 lr: 0.02\n",
            "iteration: 86600 loss: 0.0018 lr: 0.02\n",
            "iteration: 86610 loss: 0.0019 lr: 0.02\n",
            "iteration: 86620 loss: 0.0018 lr: 0.02\n",
            "iteration: 86630 loss: 0.0017 lr: 0.02\n",
            "iteration: 86640 loss: 0.0026 lr: 0.02\n",
            "iteration: 86650 loss: 0.0017 lr: 0.02\n",
            "iteration: 86660 loss: 0.0018 lr: 0.02\n",
            "iteration: 86670 loss: 0.0017 lr: 0.02\n",
            "iteration: 86680 loss: 0.0022 lr: 0.02\n",
            "iteration: 86690 loss: 0.0022 lr: 0.02\n",
            "iteration: 86700 loss: 0.0018 lr: 0.02\n",
            "iteration: 86710 loss: 0.0019 lr: 0.02\n",
            "iteration: 86720 loss: 0.0016 lr: 0.02\n",
            "iteration: 86730 loss: 0.0016 lr: 0.02\n",
            "iteration: 86740 loss: 0.0020 lr: 0.02\n",
            "iteration: 86750 loss: 0.0012 lr: 0.02\n",
            "iteration: 86760 loss: 0.0019 lr: 0.02\n",
            "iteration: 86770 loss: 0.0016 lr: 0.02\n",
            "iteration: 86780 loss: 0.0014 lr: 0.02\n",
            "iteration: 86790 loss: 0.0020 lr: 0.02\n",
            "iteration: 86800 loss: 0.0019 lr: 0.02\n",
            "iteration: 86810 loss: 0.0023 lr: 0.02\n",
            "iteration: 86820 loss: 0.0018 lr: 0.02\n",
            "iteration: 86830 loss: 0.0018 lr: 0.02\n",
            "iteration: 86840 loss: 0.0021 lr: 0.02\n",
            "iteration: 86850 loss: 0.0022 lr: 0.02\n",
            "iteration: 86860 loss: 0.0016 lr: 0.02\n",
            "iteration: 86870 loss: 0.0022 lr: 0.02\n",
            "iteration: 86880 loss: 0.0021 lr: 0.02\n",
            "iteration: 86890 loss: 0.0023 lr: 0.02\n",
            "iteration: 86900 loss: 0.0018 lr: 0.02\n",
            "iteration: 86910 loss: 0.0022 lr: 0.02\n",
            "iteration: 86920 loss: 0.0016 lr: 0.02\n",
            "iteration: 86930 loss: 0.0019 lr: 0.02\n",
            "iteration: 86940 loss: 0.0020 lr: 0.02\n",
            "iteration: 86950 loss: 0.0016 lr: 0.02\n",
            "iteration: 86960 loss: 0.0023 lr: 0.02\n",
            "iteration: 86970 loss: 0.0014 lr: 0.02\n",
            "iteration: 86980 loss: 0.0015 lr: 0.02\n",
            "iteration: 86990 loss: 0.0016 lr: 0.02\n",
            "iteration: 87000 loss: 0.0019 lr: 0.02\n",
            "iteration: 87010 loss: 0.0021 lr: 0.02\n",
            "iteration: 87020 loss: 0.0018 lr: 0.02\n",
            "iteration: 87030 loss: 0.0017 lr: 0.02\n",
            "iteration: 87040 loss: 0.0016 lr: 0.02\n",
            "iteration: 87050 loss: 0.0013 lr: 0.02\n",
            "iteration: 87060 loss: 0.0018 lr: 0.02\n",
            "iteration: 87070 loss: 0.0020 lr: 0.02\n",
            "iteration: 87080 loss: 0.0019 lr: 0.02\n",
            "iteration: 87090 loss: 0.0018 lr: 0.02\n",
            "iteration: 87100 loss: 0.0023 lr: 0.02\n",
            "iteration: 87110 loss: 0.0019 lr: 0.02\n",
            "iteration: 87120 loss: 0.0016 lr: 0.02\n",
            "iteration: 87130 loss: 0.0019 lr: 0.02\n",
            "iteration: 87140 loss: 0.0021 lr: 0.02\n",
            "iteration: 87150 loss: 0.0014 lr: 0.02\n",
            "iteration: 87160 loss: 0.0019 lr: 0.02\n",
            "iteration: 87170 loss: 0.0018 lr: 0.02\n",
            "iteration: 87180 loss: 0.0021 lr: 0.02\n",
            "iteration: 87190 loss: 0.0023 lr: 0.02\n",
            "iteration: 87200 loss: 0.0020 lr: 0.02\n",
            "iteration: 87210 loss: 0.0023 lr: 0.02\n",
            "iteration: 87220 loss: 0.0019 lr: 0.02\n",
            "iteration: 87230 loss: 0.0018 lr: 0.02\n",
            "iteration: 87240 loss: 0.0023 lr: 0.02\n",
            "iteration: 87250 loss: 0.0018 lr: 0.02\n",
            "iteration: 87260 loss: 0.0019 lr: 0.02\n",
            "iteration: 87270 loss: 0.0020 lr: 0.02\n",
            "iteration: 87280 loss: 0.0017 lr: 0.02\n",
            "iteration: 87290 loss: 0.0024 lr: 0.02\n",
            "iteration: 87300 loss: 0.0019 lr: 0.02\n",
            "iteration: 87310 loss: 0.0019 lr: 0.02\n",
            "iteration: 87320 loss: 0.0015 lr: 0.02\n",
            "iteration: 87330 loss: 0.0015 lr: 0.02\n",
            "iteration: 87340 loss: 0.0023 lr: 0.02\n",
            "iteration: 87350 loss: 0.0020 lr: 0.02\n",
            "iteration: 87360 loss: 0.0024 lr: 0.02\n",
            "iteration: 87370 loss: 0.0016 lr: 0.02\n",
            "iteration: 87380 loss: 0.0016 lr: 0.02\n",
            "iteration: 87390 loss: 0.0022 lr: 0.02\n",
            "iteration: 87400 loss: 0.0017 lr: 0.02\n",
            "iteration: 87410 loss: 0.0024 lr: 0.02\n",
            "iteration: 87420 loss: 0.0018 lr: 0.02\n",
            "iteration: 87430 loss: 0.0021 lr: 0.02\n",
            "iteration: 87440 loss: 0.0019 lr: 0.02\n",
            "iteration: 87450 loss: 0.0017 lr: 0.02\n",
            "iteration: 87460 loss: 0.0019 lr: 0.02\n",
            "iteration: 87470 loss: 0.0015 lr: 0.02\n",
            "iteration: 87480 loss: 0.0017 lr: 0.02\n",
            "iteration: 87490 loss: 0.0022 lr: 0.02\n",
            "iteration: 87500 loss: 0.0024 lr: 0.02\n",
            "iteration: 87510 loss: 0.0016 lr: 0.02\n",
            "iteration: 87520 loss: 0.0030 lr: 0.02\n",
            "iteration: 87530 loss: 0.0021 lr: 0.02\n",
            "iteration: 87540 loss: 0.0020 lr: 0.02\n",
            "iteration: 87550 loss: 0.0016 lr: 0.02\n",
            "iteration: 87560 loss: 0.0023 lr: 0.02\n",
            "iteration: 87570 loss: 0.0024 lr: 0.02\n",
            "iteration: 87580 loss: 0.0022 lr: 0.02\n",
            "iteration: 87590 loss: 0.0021 lr: 0.02\n",
            "iteration: 87600 loss: 0.0018 lr: 0.02\n",
            "iteration: 87610 loss: 0.0015 lr: 0.02\n",
            "iteration: 87620 loss: 0.0016 lr: 0.02\n",
            "iteration: 87630 loss: 0.0016 lr: 0.02\n",
            "iteration: 87640 loss: 0.0019 lr: 0.02\n",
            "iteration: 87650 loss: 0.0019 lr: 0.02\n",
            "iteration: 87660 loss: 0.0020 lr: 0.02\n",
            "iteration: 87670 loss: 0.0015 lr: 0.02\n",
            "iteration: 87680 loss: 0.0023 lr: 0.02\n",
            "iteration: 87690 loss: 0.0016 lr: 0.02\n",
            "iteration: 87700 loss: 0.0017 lr: 0.02\n",
            "iteration: 87710 loss: 0.0024 lr: 0.02\n",
            "iteration: 87720 loss: 0.0021 lr: 0.02\n",
            "iteration: 87730 loss: 0.0020 lr: 0.02\n",
            "iteration: 87740 loss: 0.0019 lr: 0.02\n",
            "iteration: 87750 loss: 0.0019 lr: 0.02\n",
            "iteration: 87760 loss: 0.0018 lr: 0.02\n",
            "iteration: 87770 loss: 0.0022 lr: 0.02\n",
            "iteration: 87780 loss: 0.0020 lr: 0.02\n",
            "iteration: 87790 loss: 0.0016 lr: 0.02\n",
            "iteration: 87800 loss: 0.0021 lr: 0.02\n",
            "iteration: 87810 loss: 0.0018 lr: 0.02\n",
            "iteration: 87820 loss: 0.0025 lr: 0.02\n",
            "iteration: 87830 loss: 0.0020 lr: 0.02\n",
            "iteration: 87840 loss: 0.0016 lr: 0.02\n",
            "iteration: 87850 loss: 0.0017 lr: 0.02\n",
            "iteration: 87860 loss: 0.0016 lr: 0.02\n",
            "iteration: 87870 loss: 0.0018 lr: 0.02\n",
            "iteration: 87880 loss: 0.0020 lr: 0.02\n",
            "iteration: 87890 loss: 0.0020 lr: 0.02\n",
            "iteration: 87900 loss: 0.0018 lr: 0.02\n",
            "iteration: 87910 loss: 0.0013 lr: 0.02\n",
            "iteration: 87920 loss: 0.0019 lr: 0.02\n",
            "iteration: 87930 loss: 0.0018 lr: 0.02\n",
            "iteration: 87940 loss: 0.0019 lr: 0.02\n",
            "iteration: 87950 loss: 0.0018 lr: 0.02\n",
            "iteration: 87960 loss: 0.0017 lr: 0.02\n",
            "iteration: 87970 loss: 0.0016 lr: 0.02\n",
            "iteration: 87980 loss: 0.0015 lr: 0.02\n",
            "iteration: 87990 loss: 0.0025 lr: 0.02\n",
            "iteration: 88000 loss: 0.0019 lr: 0.02\n",
            "iteration: 88010 loss: 0.0018 lr: 0.02\n",
            "iteration: 88020 loss: 0.0013 lr: 0.02\n",
            "iteration: 88030 loss: 0.0019 lr: 0.02\n",
            "iteration: 88040 loss: 0.0020 lr: 0.02\n",
            "iteration: 88050 loss: 0.0017 lr: 0.02\n",
            "iteration: 88060 loss: 0.0020 lr: 0.02\n",
            "iteration: 88070 loss: 0.0016 lr: 0.02\n",
            "iteration: 88080 loss: 0.0013 lr: 0.02\n",
            "iteration: 88090 loss: 0.0014 lr: 0.02\n",
            "iteration: 88100 loss: 0.0015 lr: 0.02\n",
            "iteration: 88110 loss: 0.0014 lr: 0.02\n",
            "iteration: 88120 loss: 0.0015 lr: 0.02\n",
            "iteration: 88130 loss: 0.0018 lr: 0.02\n",
            "iteration: 88140 loss: 0.0018 lr: 0.02\n",
            "iteration: 88150 loss: 0.0015 lr: 0.02\n",
            "iteration: 88160 loss: 0.0020 lr: 0.02\n",
            "iteration: 88170 loss: 0.0020 lr: 0.02\n",
            "iteration: 88180 loss: 0.0015 lr: 0.02\n",
            "iteration: 88190 loss: 0.0015 lr: 0.02\n",
            "iteration: 88200 loss: 0.0017 lr: 0.02\n",
            "iteration: 88210 loss: 0.0016 lr: 0.02\n",
            "iteration: 88220 loss: 0.0022 lr: 0.02\n",
            "iteration: 88230 loss: 0.0021 lr: 0.02\n",
            "iteration: 88240 loss: 0.0018 lr: 0.02\n",
            "iteration: 88250 loss: 0.0019 lr: 0.02\n",
            "iteration: 88260 loss: 0.0016 lr: 0.02\n",
            "iteration: 88270 loss: 0.0015 lr: 0.02\n",
            "iteration: 88280 loss: 0.0015 lr: 0.02\n",
            "iteration: 88290 loss: 0.0012 lr: 0.02\n",
            "iteration: 88300 loss: 0.0015 lr: 0.02\n",
            "iteration: 88310 loss: 0.0020 lr: 0.02\n",
            "iteration: 88320 loss: 0.0023 lr: 0.02\n",
            "iteration: 88330 loss: 0.0020 lr: 0.02\n",
            "iteration: 88340 loss: 0.0016 lr: 0.02\n",
            "iteration: 88350 loss: 0.0020 lr: 0.02\n",
            "iteration: 88360 loss: 0.0020 lr: 0.02\n",
            "iteration: 88370 loss: 0.0015 lr: 0.02\n",
            "iteration: 88380 loss: 0.0018 lr: 0.02\n",
            "iteration: 88390 loss: 0.0016 lr: 0.02\n",
            "iteration: 88400 loss: 0.0016 lr: 0.02\n",
            "iteration: 88410 loss: 0.0022 lr: 0.02\n",
            "iteration: 88420 loss: 0.0021 lr: 0.02\n",
            "iteration: 88430 loss: 0.0019 lr: 0.02\n",
            "iteration: 88440 loss: 0.0026 lr: 0.02\n",
            "iteration: 88450 loss: 0.0025 lr: 0.02\n",
            "iteration: 88460 loss: 0.0018 lr: 0.02\n",
            "iteration: 88470 loss: 0.0025 lr: 0.02\n",
            "iteration: 88480 loss: 0.0021 lr: 0.02\n",
            "iteration: 88490 loss: 0.0028 lr: 0.02\n",
            "iteration: 88500 loss: 0.0019 lr: 0.02\n",
            "iteration: 88510 loss: 0.0015 lr: 0.02\n",
            "iteration: 88520 loss: 0.0019 lr: 0.02\n",
            "iteration: 88530 loss: 0.0017 lr: 0.02\n",
            "iteration: 88540 loss: 0.0018 lr: 0.02\n",
            "iteration: 88550 loss: 0.0017 lr: 0.02\n",
            "iteration: 88560 loss: 0.0018 lr: 0.02\n",
            "iteration: 88570 loss: 0.0022 lr: 0.02\n",
            "iteration: 88580 loss: 0.0016 lr: 0.02\n",
            "iteration: 88590 loss: 0.0020 lr: 0.02\n",
            "iteration: 88600 loss: 0.0022 lr: 0.02\n",
            "iteration: 88610 loss: 0.0019 lr: 0.02\n",
            "iteration: 88620 loss: 0.0021 lr: 0.02\n",
            "iteration: 88630 loss: 0.0015 lr: 0.02\n",
            "iteration: 88640 loss: 0.0019 lr: 0.02\n",
            "iteration: 88650 loss: 0.0019 lr: 0.02\n",
            "iteration: 88660 loss: 0.0023 lr: 0.02\n",
            "iteration: 88670 loss: 0.0015 lr: 0.02\n",
            "iteration: 88680 loss: 0.0020 lr: 0.02\n",
            "iteration: 88690 loss: 0.0017 lr: 0.02\n",
            "iteration: 88700 loss: 0.0017 lr: 0.02\n",
            "iteration: 88710 loss: 0.0021 lr: 0.02\n",
            "iteration: 88720 loss: 0.0018 lr: 0.02\n",
            "iteration: 88730 loss: 0.0021 lr: 0.02\n",
            "iteration: 88740 loss: 0.0021 lr: 0.02\n",
            "iteration: 88750 loss: 0.0021 lr: 0.02\n",
            "iteration: 88760 loss: 0.0019 lr: 0.02\n",
            "iteration: 88770 loss: 0.0017 lr: 0.02\n",
            "iteration: 88780 loss: 0.0021 lr: 0.02\n",
            "iteration: 88790 loss: 0.0021 lr: 0.02\n",
            "iteration: 88800 loss: 0.0018 lr: 0.02\n",
            "iteration: 88810 loss: 0.0028 lr: 0.02\n",
            "iteration: 88820 loss: 0.0019 lr: 0.02\n",
            "iteration: 88830 loss: 0.0016 lr: 0.02\n",
            "iteration: 88840 loss: 0.0014 lr: 0.02\n",
            "iteration: 88850 loss: 0.0027 lr: 0.02\n",
            "iteration: 88860 loss: 0.0018 lr: 0.02\n",
            "iteration: 88870 loss: 0.0019 lr: 0.02\n",
            "iteration: 88880 loss: 0.0027 lr: 0.02\n",
            "iteration: 88890 loss: 0.0019 lr: 0.02\n",
            "iteration: 88900 loss: 0.0026 lr: 0.02\n",
            "iteration: 88910 loss: 0.0022 lr: 0.02\n",
            "iteration: 88920 loss: 0.0018 lr: 0.02\n",
            "iteration: 88930 loss: 0.0020 lr: 0.02\n",
            "iteration: 88940 loss: 0.0021 lr: 0.02\n",
            "iteration: 88950 loss: 0.0022 lr: 0.02\n",
            "iteration: 88960 loss: 0.0017 lr: 0.02\n",
            "iteration: 88970 loss: 0.0016 lr: 0.02\n",
            "iteration: 88980 loss: 0.0018 lr: 0.02\n",
            "iteration: 88990 loss: 0.0020 lr: 0.02\n",
            "iteration: 89000 loss: 0.0020 lr: 0.02\n",
            "iteration: 89010 loss: 0.0021 lr: 0.02\n",
            "iteration: 89020 loss: 0.0022 lr: 0.02\n",
            "iteration: 89030 loss: 0.0022 lr: 0.02\n",
            "iteration: 89040 loss: 0.0019 lr: 0.02\n",
            "iteration: 89050 loss: 0.0021 lr: 0.02\n",
            "iteration: 89060 loss: 0.0023 lr: 0.02\n",
            "iteration: 89070 loss: 0.0022 lr: 0.02\n",
            "iteration: 89080 loss: 0.0016 lr: 0.02\n",
            "iteration: 89090 loss: 0.0017 lr: 0.02\n",
            "iteration: 89100 loss: 0.0017 lr: 0.02\n",
            "iteration: 89110 loss: 0.0016 lr: 0.02\n",
            "iteration: 89120 loss: 0.0014 lr: 0.02\n",
            "iteration: 89130 loss: 0.0025 lr: 0.02\n",
            "iteration: 89140 loss: 0.0018 lr: 0.02\n",
            "iteration: 89150 loss: 0.0021 lr: 0.02\n",
            "iteration: 89160 loss: 0.0019 lr: 0.02\n",
            "iteration: 89170 loss: 0.0017 lr: 0.02\n",
            "iteration: 89180 loss: 0.0020 lr: 0.02\n",
            "iteration: 89190 loss: 0.0019 lr: 0.02\n",
            "iteration: 89200 loss: 0.0015 lr: 0.02\n",
            "iteration: 89210 loss: 0.0019 lr: 0.02\n",
            "iteration: 89220 loss: 0.0017 lr: 0.02\n",
            "iteration: 89230 loss: 0.0014 lr: 0.02\n",
            "iteration: 89240 loss: 0.0022 lr: 0.02\n",
            "iteration: 89250 loss: 0.0019 lr: 0.02\n",
            "iteration: 89260 loss: 0.0018 lr: 0.02\n",
            "iteration: 89270 loss: 0.0017 lr: 0.02\n",
            "iteration: 89280 loss: 0.0015 lr: 0.02\n",
            "iteration: 89290 loss: 0.0019 lr: 0.02\n",
            "iteration: 89300 loss: 0.0015 lr: 0.02\n",
            "iteration: 89310 loss: 0.0018 lr: 0.02\n",
            "iteration: 89320 loss: 0.0023 lr: 0.02\n",
            "iteration: 89330 loss: 0.0015 lr: 0.02\n",
            "iteration: 89340 loss: 0.0018 lr: 0.02\n",
            "iteration: 89350 loss: 0.0018 lr: 0.02\n",
            "iteration: 89360 loss: 0.0020 lr: 0.02\n",
            "iteration: 89370 loss: 0.0017 lr: 0.02\n",
            "iteration: 89380 loss: 0.0018 lr: 0.02\n",
            "iteration: 89390 loss: 0.0018 lr: 0.02\n",
            "iteration: 89400 loss: 0.0018 lr: 0.02\n",
            "iteration: 89410 loss: 0.0015 lr: 0.02\n",
            "iteration: 89420 loss: 0.0013 lr: 0.02\n",
            "iteration: 89430 loss: 0.0021 lr: 0.02\n",
            "iteration: 89440 loss: 0.0017 lr: 0.02\n",
            "iteration: 89450 loss: 0.0020 lr: 0.02\n",
            "iteration: 89460 loss: 0.0019 lr: 0.02\n",
            "iteration: 89470 loss: 0.0017 lr: 0.02\n",
            "iteration: 89480 loss: 0.0015 lr: 0.02\n",
            "iteration: 89490 loss: 0.0018 lr: 0.02\n",
            "iteration: 89500 loss: 0.0020 lr: 0.02\n",
            "iteration: 89510 loss: 0.0016 lr: 0.02\n",
            "iteration: 89520 loss: 0.0020 lr: 0.02\n",
            "iteration: 89530 loss: 0.0016 lr: 0.02\n",
            "iteration: 89540 loss: 0.0016 lr: 0.02\n",
            "iteration: 89550 loss: 0.0020 lr: 0.02\n",
            "iteration: 89560 loss: 0.0021 lr: 0.02\n",
            "iteration: 89570 loss: 0.0015 lr: 0.02\n",
            "iteration: 89580 loss: 0.0021 lr: 0.02\n",
            "iteration: 89590 loss: 0.0015 lr: 0.02\n",
            "iteration: 89600 loss: 0.0015 lr: 0.02\n",
            "iteration: 89610 loss: 0.0018 lr: 0.02\n",
            "iteration: 89620 loss: 0.0021 lr: 0.02\n",
            "iteration: 89630 loss: 0.0021 lr: 0.02\n",
            "iteration: 89640 loss: 0.0018 lr: 0.02\n",
            "iteration: 89650 loss: 0.0023 lr: 0.02\n",
            "iteration: 89660 loss: 0.0025 lr: 0.02\n",
            "iteration: 89670 loss: 0.0015 lr: 0.02\n",
            "iteration: 89680 loss: 0.0021 lr: 0.02\n",
            "iteration: 89690 loss: 0.0018 lr: 0.02\n",
            "iteration: 89700 loss: 0.0020 lr: 0.02\n",
            "iteration: 89710 loss: 0.0018 lr: 0.02\n",
            "iteration: 89720 loss: 0.0020 lr: 0.02\n",
            "iteration: 89730 loss: 0.0016 lr: 0.02\n",
            "iteration: 89740 loss: 0.0016 lr: 0.02\n",
            "iteration: 89750 loss: 0.0020 lr: 0.02\n",
            "iteration: 89760 loss: 0.0016 lr: 0.02\n",
            "iteration: 89770 loss: 0.0019 lr: 0.02\n",
            "iteration: 89780 loss: 0.0020 lr: 0.02\n",
            "iteration: 89790 loss: 0.0015 lr: 0.02\n",
            "iteration: 89800 loss: 0.0019 lr: 0.02\n",
            "iteration: 89810 loss: 0.0016 lr: 0.02\n",
            "iteration: 89820 loss: 0.0023 lr: 0.02\n",
            "iteration: 89830 loss: 0.0015 lr: 0.02\n",
            "iteration: 89840 loss: 0.0021 lr: 0.02\n",
            "iteration: 89850 loss: 0.0017 lr: 0.02\n",
            "iteration: 89860 loss: 0.0017 lr: 0.02\n",
            "iteration: 89870 loss: 0.0017 lr: 0.02\n",
            "iteration: 89880 loss: 0.0019 lr: 0.02\n",
            "iteration: 89890 loss: 0.0017 lr: 0.02\n",
            "iteration: 89900 loss: 0.0018 lr: 0.02\n",
            "iteration: 89910 loss: 0.0016 lr: 0.02\n",
            "iteration: 89920 loss: 0.0024 lr: 0.02\n",
            "iteration: 89930 loss: 0.0018 lr: 0.02\n",
            "iteration: 89940 loss: 0.0020 lr: 0.02\n",
            "iteration: 89950 loss: 0.0014 lr: 0.02\n",
            "iteration: 89960 loss: 0.0015 lr: 0.02\n",
            "iteration: 89970 loss: 0.0025 lr: 0.02\n",
            "iteration: 89980 loss: 0.0019 lr: 0.02\n",
            "iteration: 89990 loss: 0.0020 lr: 0.02\n",
            "iteration: 90000 loss: 0.0017 lr: 0.02\n",
            "iteration: 90010 loss: 0.0017 lr: 0.02\n",
            "iteration: 90020 loss: 0.0018 lr: 0.02\n",
            "iteration: 90030 loss: 0.0019 lr: 0.02\n",
            "iteration: 90040 loss: 0.0017 lr: 0.02\n",
            "iteration: 90050 loss: 0.0019 lr: 0.02\n",
            "iteration: 90060 loss: 0.0021 lr: 0.02\n",
            "iteration: 90070 loss: 0.0018 lr: 0.02\n",
            "iteration: 90080 loss: 0.0018 lr: 0.02\n",
            "iteration: 90090 loss: 0.0017 lr: 0.02\n",
            "iteration: 90100 loss: 0.0015 lr: 0.02\n",
            "iteration: 90110 loss: 0.0019 lr: 0.02\n",
            "iteration: 90120 loss: 0.0023 lr: 0.02\n",
            "iteration: 90130 loss: 0.0017 lr: 0.02\n",
            "iteration: 90140 loss: 0.0022 lr: 0.02\n",
            "iteration: 90150 loss: 0.0020 lr: 0.02\n",
            "iteration: 90160 loss: 0.0023 lr: 0.02\n",
            "iteration: 90170 loss: 0.0018 lr: 0.02\n",
            "iteration: 90180 loss: 0.0017 lr: 0.02\n",
            "iteration: 90190 loss: 0.0020 lr: 0.02\n",
            "iteration: 90200 loss: 0.0023 lr: 0.02\n",
            "iteration: 90210 loss: 0.0020 lr: 0.02\n",
            "iteration: 90220 loss: 0.0022 lr: 0.02\n",
            "iteration: 90230 loss: 0.0020 lr: 0.02\n",
            "iteration: 90240 loss: 0.0018 lr: 0.02\n",
            "iteration: 90250 loss: 0.0017 lr: 0.02\n",
            "iteration: 90260 loss: 0.0026 lr: 0.02\n",
            "iteration: 90270 loss: 0.0021 lr: 0.02\n",
            "iteration: 90280 loss: 0.0019 lr: 0.02\n",
            "iteration: 90290 loss: 0.0018 lr: 0.02\n",
            "iteration: 90300 loss: 0.0014 lr: 0.02\n",
            "iteration: 90310 loss: 0.0016 lr: 0.02\n",
            "iteration: 90320 loss: 0.0020 lr: 0.02\n",
            "iteration: 90330 loss: 0.0019 lr: 0.02\n",
            "iteration: 90340 loss: 0.0016 lr: 0.02\n",
            "iteration: 90350 loss: 0.0017 lr: 0.02\n",
            "iteration: 90360 loss: 0.0020 lr: 0.02\n",
            "iteration: 90370 loss: 0.0015 lr: 0.02\n",
            "iteration: 90380 loss: 0.0022 lr: 0.02\n",
            "iteration: 90390 loss: 0.0016 lr: 0.02\n",
            "iteration: 90400 loss: 0.0014 lr: 0.02\n",
            "iteration: 90410 loss: 0.0015 lr: 0.02\n",
            "iteration: 90420 loss: 0.0023 lr: 0.02\n",
            "iteration: 90430 loss: 0.0020 lr: 0.02\n",
            "iteration: 90440 loss: 0.0016 lr: 0.02\n",
            "iteration: 90450 loss: 0.0017 lr: 0.02\n",
            "iteration: 90460 loss: 0.0018 lr: 0.02\n",
            "iteration: 90470 loss: 0.0014 lr: 0.02\n",
            "iteration: 90480 loss: 0.0021 lr: 0.02\n",
            "iteration: 90490 loss: 0.0018 lr: 0.02\n",
            "iteration: 90500 loss: 0.0017 lr: 0.02\n",
            "iteration: 90510 loss: 0.0017 lr: 0.02\n",
            "iteration: 90520 loss: 0.0021 lr: 0.02\n",
            "iteration: 90530 loss: 0.0019 lr: 0.02\n",
            "iteration: 90540 loss: 0.0019 lr: 0.02\n",
            "iteration: 90550 loss: 0.0015 lr: 0.02\n",
            "iteration: 90560 loss: 0.0021 lr: 0.02\n",
            "iteration: 90570 loss: 0.0017 lr: 0.02\n",
            "iteration: 90580 loss: 0.0019 lr: 0.02\n",
            "iteration: 90590 loss: 0.0016 lr: 0.02\n",
            "iteration: 90600 loss: 0.0015 lr: 0.02\n",
            "iteration: 90610 loss: 0.0016 lr: 0.02\n",
            "iteration: 90620 loss: 0.0018 lr: 0.02\n",
            "iteration: 90630 loss: 0.0015 lr: 0.02\n",
            "iteration: 90640 loss: 0.0015 lr: 0.02\n",
            "iteration: 90650 loss: 0.0021 lr: 0.02\n",
            "iteration: 90660 loss: 0.0020 lr: 0.02\n",
            "iteration: 90670 loss: 0.0019 lr: 0.02\n",
            "iteration: 90680 loss: 0.0024 lr: 0.02\n",
            "iteration: 90690 loss: 0.0020 lr: 0.02\n",
            "iteration: 90700 loss: 0.0014 lr: 0.02\n",
            "iteration: 90710 loss: 0.0020 lr: 0.02\n",
            "iteration: 90720 loss: 0.0017 lr: 0.02\n",
            "iteration: 90730 loss: 0.0021 lr: 0.02\n",
            "iteration: 90740 loss: 0.0018 lr: 0.02\n",
            "iteration: 90750 loss: 0.0019 lr: 0.02\n",
            "iteration: 90760 loss: 0.0015 lr: 0.02\n",
            "iteration: 90770 loss: 0.0019 lr: 0.02\n",
            "iteration: 90780 loss: 0.0020 lr: 0.02\n",
            "iteration: 90790 loss: 0.0032 lr: 0.02\n",
            "iteration: 90800 loss: 0.0015 lr: 0.02\n",
            "iteration: 90810 loss: 0.0021 lr: 0.02\n",
            "iteration: 90820 loss: 0.0026 lr: 0.02\n",
            "iteration: 90830 loss: 0.0014 lr: 0.02\n",
            "iteration: 90840 loss: 0.0018 lr: 0.02\n",
            "iteration: 90850 loss: 0.0014 lr: 0.02\n",
            "iteration: 90860 loss: 0.0017 lr: 0.02\n",
            "iteration: 90870 loss: 0.0021 lr: 0.02\n",
            "iteration: 90880 loss: 0.0019 lr: 0.02\n",
            "iteration: 90890 loss: 0.0015 lr: 0.02\n",
            "iteration: 90900 loss: 0.0016 lr: 0.02\n",
            "iteration: 90910 loss: 0.0020 lr: 0.02\n",
            "iteration: 90920 loss: 0.0025 lr: 0.02\n",
            "iteration: 90930 loss: 0.0017 lr: 0.02\n",
            "iteration: 90940 loss: 0.0014 lr: 0.02\n",
            "iteration: 90950 loss: 0.0017 lr: 0.02\n",
            "iteration: 90960 loss: 0.0026 lr: 0.02\n",
            "iteration: 90970 loss: 0.0021 lr: 0.02\n",
            "iteration: 90980 loss: 0.0021 lr: 0.02\n",
            "iteration: 90990 loss: 0.0018 lr: 0.02\n",
            "iteration: 91000 loss: 0.0015 lr: 0.02\n",
            "iteration: 91010 loss: 0.0016 lr: 0.02\n",
            "iteration: 91020 loss: 0.0018 lr: 0.02\n",
            "iteration: 91030 loss: 0.0019 lr: 0.02\n",
            "iteration: 91040 loss: 0.0017 lr: 0.02\n",
            "iteration: 91050 loss: 0.0023 lr: 0.02\n",
            "iteration: 91060 loss: 0.0020 lr: 0.02\n",
            "iteration: 91070 loss: 0.0021 lr: 0.02\n",
            "iteration: 91080 loss: 0.0016 lr: 0.02\n",
            "iteration: 91090 loss: 0.0021 lr: 0.02\n",
            "iteration: 91100 loss: 0.0016 lr: 0.02\n",
            "iteration: 91110 loss: 0.0017 lr: 0.02\n",
            "iteration: 91120 loss: 0.0017 lr: 0.02\n",
            "iteration: 91130 loss: 0.0016 lr: 0.02\n",
            "iteration: 91140 loss: 0.0018 lr: 0.02\n",
            "iteration: 91150 loss: 0.0016 lr: 0.02\n",
            "iteration: 91160 loss: 0.0024 lr: 0.02\n",
            "iteration: 91170 loss: 0.0020 lr: 0.02\n",
            "iteration: 91180 loss: 0.0019 lr: 0.02\n",
            "iteration: 91190 loss: 0.0014 lr: 0.02\n",
            "iteration: 91200 loss: 0.0024 lr: 0.02\n",
            "iteration: 91210 loss: 0.0018 lr: 0.02\n",
            "iteration: 91220 loss: 0.0015 lr: 0.02\n",
            "iteration: 91230 loss: 0.0018 lr: 0.02\n",
            "iteration: 91240 loss: 0.0017 lr: 0.02\n",
            "iteration: 91250 loss: 0.0019 lr: 0.02\n",
            "iteration: 91260 loss: 0.0016 lr: 0.02\n",
            "iteration: 91270 loss: 0.0018 lr: 0.02\n",
            "iteration: 91280 loss: 0.0020 lr: 0.02\n",
            "iteration: 91290 loss: 0.0016 lr: 0.02\n",
            "iteration: 91300 loss: 0.0016 lr: 0.02\n",
            "iteration: 91310 loss: 0.0021 lr: 0.02\n",
            "iteration: 91320 loss: 0.0019 lr: 0.02\n",
            "iteration: 91330 loss: 0.0018 lr: 0.02\n",
            "iteration: 91340 loss: 0.0015 lr: 0.02\n",
            "iteration: 91350 loss: 0.0017 lr: 0.02\n",
            "iteration: 91360 loss: 0.0020 lr: 0.02\n",
            "iteration: 91370 loss: 0.0019 lr: 0.02\n",
            "iteration: 91380 loss: 0.0020 lr: 0.02\n",
            "iteration: 91390 loss: 0.0014 lr: 0.02\n",
            "iteration: 91400 loss: 0.0027 lr: 0.02\n",
            "iteration: 91410 loss: 0.0020 lr: 0.02\n",
            "iteration: 91420 loss: 0.0017 lr: 0.02\n",
            "iteration: 91430 loss: 0.0019 lr: 0.02\n",
            "iteration: 91440 loss: 0.0020 lr: 0.02\n",
            "iteration: 91450 loss: 0.0017 lr: 0.02\n",
            "iteration: 91460 loss: 0.0019 lr: 0.02\n",
            "iteration: 91470 loss: 0.0016 lr: 0.02\n",
            "iteration: 91480 loss: 0.0017 lr: 0.02\n",
            "iteration: 91490 loss: 0.0018 lr: 0.02\n",
            "iteration: 91500 loss: 0.0019 lr: 0.02\n",
            "iteration: 91510 loss: 0.0020 lr: 0.02\n",
            "iteration: 91520 loss: 0.0013 lr: 0.02\n",
            "iteration: 91530 loss: 0.0018 lr: 0.02\n",
            "iteration: 91540 loss: 0.0018 lr: 0.02\n",
            "iteration: 91550 loss: 0.0016 lr: 0.02\n",
            "iteration: 91560 loss: 0.0020 lr: 0.02\n",
            "iteration: 91570 loss: 0.0016 lr: 0.02\n",
            "iteration: 91580 loss: 0.0018 lr: 0.02\n",
            "iteration: 91590 loss: 0.0016 lr: 0.02\n",
            "iteration: 91600 loss: 0.0015 lr: 0.02\n",
            "iteration: 91610 loss: 0.0015 lr: 0.02\n",
            "iteration: 91620 loss: 0.0017 lr: 0.02\n",
            "iteration: 91630 loss: 0.0014 lr: 0.02\n",
            "iteration: 91640 loss: 0.0019 lr: 0.02\n",
            "iteration: 91650 loss: 0.0015 lr: 0.02\n",
            "iteration: 91660 loss: 0.0018 lr: 0.02\n",
            "iteration: 91670 loss: 0.0017 lr: 0.02\n",
            "iteration: 91680 loss: 0.0015 lr: 0.02\n",
            "iteration: 91690 loss: 0.0018 lr: 0.02\n",
            "iteration: 91700 loss: 0.0022 lr: 0.02\n",
            "iteration: 91710 loss: 0.0025 lr: 0.02\n",
            "iteration: 91720 loss: 0.0015 lr: 0.02\n",
            "iteration: 91730 loss: 0.0016 lr: 0.02\n",
            "iteration: 91740 loss: 0.0016 lr: 0.02\n",
            "iteration: 91750 loss: 0.0026 lr: 0.02\n",
            "iteration: 91760 loss: 0.0017 lr: 0.02\n",
            "iteration: 91770 loss: 0.0022 lr: 0.02\n",
            "iteration: 91780 loss: 0.0018 lr: 0.02\n",
            "iteration: 91790 loss: 0.0020 lr: 0.02\n",
            "iteration: 91800 loss: 0.0015 lr: 0.02\n",
            "iteration: 91810 loss: 0.0022 lr: 0.02\n",
            "iteration: 91820 loss: 0.0019 lr: 0.02\n",
            "iteration: 91830 loss: 0.0017 lr: 0.02\n",
            "iteration: 91840 loss: 0.0015 lr: 0.02\n",
            "iteration: 91850 loss: 0.0021 lr: 0.02\n",
            "iteration: 91860 loss: 0.0019 lr: 0.02\n",
            "iteration: 91870 loss: 0.0020 lr: 0.02\n",
            "iteration: 91880 loss: 0.0015 lr: 0.02\n",
            "iteration: 91890 loss: 0.0018 lr: 0.02\n",
            "iteration: 91900 loss: 0.0014 lr: 0.02\n",
            "iteration: 91910 loss: 0.0020 lr: 0.02\n",
            "iteration: 91920 loss: 0.0017 lr: 0.02\n",
            "iteration: 91930 loss: 0.0019 lr: 0.02\n",
            "iteration: 91940 loss: 0.0021 lr: 0.02\n",
            "iteration: 91950 loss: 0.0021 lr: 0.02\n",
            "iteration: 91960 loss: 0.0017 lr: 0.02\n",
            "iteration: 91970 loss: 0.0021 lr: 0.02\n",
            "iteration: 91980 loss: 0.0017 lr: 0.02\n",
            "iteration: 91990 loss: 0.0019 lr: 0.02\n",
            "iteration: 92000 loss: 0.0017 lr: 0.02\n",
            "iteration: 92010 loss: 0.0019 lr: 0.02\n",
            "iteration: 92020 loss: 0.0019 lr: 0.02\n",
            "iteration: 92030 loss: 0.0021 lr: 0.02\n",
            "iteration: 92040 loss: 0.0016 lr: 0.02\n",
            "iteration: 92050 loss: 0.0025 lr: 0.02\n",
            "iteration: 92060 loss: 0.0018 lr: 0.02\n",
            "iteration: 92070 loss: 0.0016 lr: 0.02\n",
            "iteration: 92080 loss: 0.0017 lr: 0.02\n",
            "iteration: 92090 loss: 0.0020 lr: 0.02\n",
            "iteration: 92100 loss: 0.0021 lr: 0.02\n",
            "iteration: 92110 loss: 0.0018 lr: 0.02\n",
            "iteration: 92120 loss: 0.0018 lr: 0.02\n",
            "iteration: 92130 loss: 0.0016 lr: 0.02\n",
            "iteration: 92140 loss: 0.0020 lr: 0.02\n",
            "iteration: 92150 loss: 0.0018 lr: 0.02\n",
            "iteration: 92160 loss: 0.0021 lr: 0.02\n",
            "iteration: 92170 loss: 0.0019 lr: 0.02\n",
            "iteration: 92180 loss: 0.0018 lr: 0.02\n",
            "iteration: 92190 loss: 0.0019 lr: 0.02\n",
            "iteration: 92200 loss: 0.0019 lr: 0.02\n",
            "iteration: 92210 loss: 0.0026 lr: 0.02\n",
            "iteration: 92220 loss: 0.0019 lr: 0.02\n",
            "iteration: 92230 loss: 0.0018 lr: 0.02\n",
            "iteration: 92240 loss: 0.0025 lr: 0.02\n",
            "iteration: 92250 loss: 0.0016 lr: 0.02\n",
            "iteration: 92260 loss: 0.0026 lr: 0.02\n",
            "iteration: 92270 loss: 0.0015 lr: 0.02\n",
            "iteration: 92280 loss: 0.0015 lr: 0.02\n",
            "iteration: 92290 loss: 0.0015 lr: 0.02\n",
            "iteration: 92300 loss: 0.0019 lr: 0.02\n",
            "iteration: 92310 loss: 0.0018 lr: 0.02\n",
            "iteration: 92320 loss: 0.0019 lr: 0.02\n",
            "iteration: 92330 loss: 0.0020 lr: 0.02\n",
            "iteration: 92340 loss: 0.0014 lr: 0.02\n",
            "iteration: 92350 loss: 0.0017 lr: 0.02\n",
            "iteration: 92360 loss: 0.0023 lr: 0.02\n",
            "iteration: 92370 loss: 0.0019 lr: 0.02\n",
            "iteration: 92380 loss: 0.0017 lr: 0.02\n",
            "iteration: 92390 loss: 0.0019 lr: 0.02\n",
            "iteration: 92400 loss: 0.0021 lr: 0.02\n",
            "iteration: 92410 loss: 0.0016 lr: 0.02\n",
            "iteration: 92420 loss: 0.0019 lr: 0.02\n",
            "iteration: 92430 loss: 0.0018 lr: 0.02\n",
            "iteration: 92440 loss: 0.0018 lr: 0.02\n",
            "iteration: 92450 loss: 0.0017 lr: 0.02\n",
            "iteration: 92460 loss: 0.0018 lr: 0.02\n",
            "iteration: 92470 loss: 0.0016 lr: 0.02\n",
            "iteration: 92480 loss: 0.0018 lr: 0.02\n",
            "iteration: 92490 loss: 0.0018 lr: 0.02\n",
            "iteration: 92500 loss: 0.0017 lr: 0.02\n",
            "iteration: 92510 loss: 0.0017 lr: 0.02\n",
            "iteration: 92520 loss: 0.0013 lr: 0.02\n",
            "iteration: 92530 loss: 0.0021 lr: 0.02\n",
            "iteration: 92540 loss: 0.0013 lr: 0.02\n",
            "iteration: 92550 loss: 0.0018 lr: 0.02\n",
            "iteration: 92560 loss: 0.0020 lr: 0.02\n",
            "iteration: 92570 loss: 0.0020 lr: 0.02\n",
            "iteration: 92580 loss: 0.0016 lr: 0.02\n",
            "iteration: 92590 loss: 0.0018 lr: 0.02\n",
            "iteration: 92600 loss: 0.0019 lr: 0.02\n",
            "iteration: 92610 loss: 0.0021 lr: 0.02\n",
            "iteration: 92620 loss: 0.0015 lr: 0.02\n",
            "iteration: 92630 loss: 0.0014 lr: 0.02\n",
            "iteration: 92640 loss: 0.0027 lr: 0.02\n",
            "iteration: 92650 loss: 0.0018 lr: 0.02\n",
            "iteration: 92660 loss: 0.0021 lr: 0.02\n",
            "iteration: 92670 loss: 0.0017 lr: 0.02\n",
            "iteration: 92680 loss: 0.0018 lr: 0.02\n",
            "iteration: 92690 loss: 0.0024 lr: 0.02\n",
            "iteration: 92700 loss: 0.0015 lr: 0.02\n",
            "iteration: 92710 loss: 0.0021 lr: 0.02\n",
            "iteration: 92720 loss: 0.0015 lr: 0.02\n",
            "iteration: 92730 loss: 0.0017 lr: 0.02\n",
            "iteration: 92740 loss: 0.0018 lr: 0.02\n",
            "iteration: 92750 loss: 0.0016 lr: 0.02\n",
            "iteration: 92760 loss: 0.0020 lr: 0.02\n",
            "iteration: 92770 loss: 0.0017 lr: 0.02\n",
            "iteration: 92780 loss: 0.0018 lr: 0.02\n",
            "iteration: 92790 loss: 0.0019 lr: 0.02\n",
            "iteration: 92800 loss: 0.0024 lr: 0.02\n",
            "iteration: 92810 loss: 0.0020 lr: 0.02\n",
            "iteration: 92820 loss: 0.0020 lr: 0.02\n",
            "iteration: 92830 loss: 0.0020 lr: 0.02\n",
            "iteration: 92840 loss: 0.0019 lr: 0.02\n",
            "iteration: 92850 loss: 0.0018 lr: 0.02\n",
            "iteration: 92860 loss: 0.0016 lr: 0.02\n",
            "iteration: 92870 loss: 0.0016 lr: 0.02\n",
            "iteration: 92880 loss: 0.0021 lr: 0.02\n",
            "iteration: 92890 loss: 0.0018 lr: 0.02\n",
            "iteration: 92900 loss: 0.0018 lr: 0.02\n",
            "iteration: 92910 loss: 0.0023 lr: 0.02\n",
            "iteration: 92920 loss: 0.0013 lr: 0.02\n",
            "iteration: 92930 loss: 0.0021 lr: 0.02\n",
            "iteration: 92940 loss: 0.0017 lr: 0.02\n",
            "iteration: 92950 loss: 0.0019 lr: 0.02\n",
            "iteration: 92960 loss: 0.0023 lr: 0.02\n",
            "iteration: 92970 loss: 0.0022 lr: 0.02\n",
            "iteration: 92980 loss: 0.0021 lr: 0.02\n",
            "iteration: 92990 loss: 0.0016 lr: 0.02\n",
            "iteration: 93000 loss: 0.0021 lr: 0.02\n",
            "iteration: 93010 loss: 0.0017 lr: 0.02\n",
            "iteration: 93020 loss: 0.0018 lr: 0.02\n",
            "iteration: 93030 loss: 0.0018 lr: 0.02\n",
            "iteration: 93040 loss: 0.0019 lr: 0.02\n",
            "iteration: 93050 loss: 0.0017 lr: 0.02\n",
            "iteration: 93060 loss: 0.0017 lr: 0.02\n",
            "iteration: 93070 loss: 0.0017 lr: 0.02\n",
            "iteration: 93080 loss: 0.0020 lr: 0.02\n",
            "iteration: 93090 loss: 0.0022 lr: 0.02\n",
            "iteration: 93100 loss: 0.0016 lr: 0.02\n",
            "iteration: 93110 loss: 0.0022 lr: 0.02\n",
            "iteration: 93120 loss: 0.0011 lr: 0.02\n",
            "iteration: 93130 loss: 0.0016 lr: 0.02\n",
            "iteration: 93140 loss: 0.0016 lr: 0.02\n",
            "iteration: 93150 loss: 0.0018 lr: 0.02\n",
            "iteration: 93160 loss: 0.0016 lr: 0.02\n",
            "iteration: 93170 loss: 0.0015 lr: 0.02\n",
            "iteration: 93180 loss: 0.0019 lr: 0.02\n",
            "iteration: 93190 loss: 0.0021 lr: 0.02\n",
            "iteration: 93200 loss: 0.0018 lr: 0.02\n",
            "iteration: 93210 loss: 0.0018 lr: 0.02\n",
            "iteration: 93220 loss: 0.0018 lr: 0.02\n",
            "iteration: 93230 loss: 0.0017 lr: 0.02\n",
            "iteration: 93240 loss: 0.0018 lr: 0.02\n",
            "iteration: 93250 loss: 0.0015 lr: 0.02\n",
            "iteration: 93260 loss: 0.0015 lr: 0.02\n",
            "iteration: 93270 loss: 0.0012 lr: 0.02\n",
            "iteration: 93280 loss: 0.0022 lr: 0.02\n",
            "iteration: 93290 loss: 0.0016 lr: 0.02\n",
            "iteration: 93300 loss: 0.0017 lr: 0.02\n",
            "iteration: 93310 loss: 0.0022 lr: 0.02\n",
            "iteration: 93320 loss: 0.0015 lr: 0.02\n",
            "iteration: 93330 loss: 0.0021 lr: 0.02\n",
            "iteration: 93340 loss: 0.0013 lr: 0.02\n",
            "iteration: 93350 loss: 0.0018 lr: 0.02\n",
            "iteration: 93360 loss: 0.0018 lr: 0.02\n",
            "iteration: 93370 loss: 0.0020 lr: 0.02\n",
            "iteration: 93380 loss: 0.0015 lr: 0.02\n",
            "iteration: 93390 loss: 0.0016 lr: 0.02\n",
            "iteration: 93400 loss: 0.0018 lr: 0.02\n",
            "iteration: 93410 loss: 0.0017 lr: 0.02\n",
            "iteration: 93420 loss: 0.0023 lr: 0.02\n",
            "iteration: 93430 loss: 0.0018 lr: 0.02\n",
            "iteration: 93440 loss: 0.0014 lr: 0.02\n",
            "iteration: 93450 loss: 0.0015 lr: 0.02\n",
            "iteration: 93460 loss: 0.0016 lr: 0.02\n",
            "iteration: 93470 loss: 0.0015 lr: 0.02\n",
            "iteration: 93480 loss: 0.0021 lr: 0.02\n",
            "iteration: 93490 loss: 0.0019 lr: 0.02\n",
            "iteration: 93500 loss: 0.0024 lr: 0.02\n",
            "iteration: 93510 loss: 0.0016 lr: 0.02\n",
            "iteration: 93520 loss: 0.0016 lr: 0.02\n",
            "iteration: 93530 loss: 0.0014 lr: 0.02\n",
            "iteration: 93540 loss: 0.0014 lr: 0.02\n",
            "iteration: 93550 loss: 0.0021 lr: 0.02\n",
            "iteration: 93560 loss: 0.0016 lr: 0.02\n",
            "iteration: 93570 loss: 0.0017 lr: 0.02\n",
            "iteration: 93580 loss: 0.0020 lr: 0.02\n",
            "iteration: 93590 loss: 0.0022 lr: 0.02\n",
            "iteration: 93600 loss: 0.0019 lr: 0.02\n",
            "iteration: 93610 loss: 0.0016 lr: 0.02\n",
            "iteration: 93620 loss: 0.0016 lr: 0.02\n",
            "iteration: 93630 loss: 0.0015 lr: 0.02\n",
            "iteration: 93640 loss: 0.0021 lr: 0.02\n",
            "iteration: 93650 loss: 0.0022 lr: 0.02\n",
            "iteration: 93660 loss: 0.0020 lr: 0.02\n",
            "iteration: 93670 loss: 0.0016 lr: 0.02\n",
            "iteration: 93680 loss: 0.0027 lr: 0.02\n",
            "iteration: 93690 loss: 0.0019 lr: 0.02\n",
            "iteration: 93700 loss: 0.0028 lr: 0.02\n",
            "iteration: 93710 loss: 0.0018 lr: 0.02\n",
            "iteration: 93720 loss: 0.0019 lr: 0.02\n",
            "iteration: 93730 loss: 0.0019 lr: 0.02\n",
            "iteration: 93740 loss: 0.0018 lr: 0.02\n",
            "iteration: 93750 loss: 0.0019 lr: 0.02\n",
            "iteration: 93760 loss: 0.0015 lr: 0.02\n",
            "iteration: 93770 loss: 0.0021 lr: 0.02\n",
            "iteration: 93780 loss: 0.0019 lr: 0.02\n",
            "iteration: 93790 loss: 0.0020 lr: 0.02\n",
            "iteration: 93800 loss: 0.0019 lr: 0.02\n",
            "iteration: 93810 loss: 0.0015 lr: 0.02\n",
            "iteration: 93820 loss: 0.0020 lr: 0.02\n",
            "iteration: 93830 loss: 0.0021 lr: 0.02\n",
            "iteration: 93840 loss: 0.0014 lr: 0.02\n",
            "iteration: 93850 loss: 0.0021 lr: 0.02\n",
            "iteration: 93860 loss: 0.0015 lr: 0.02\n",
            "iteration: 93870 loss: 0.0016 lr: 0.02\n",
            "iteration: 93880 loss: 0.0015 lr: 0.02\n",
            "iteration: 93890 loss: 0.0018 lr: 0.02\n",
            "iteration: 93900 loss: 0.0020 lr: 0.02\n",
            "iteration: 93910 loss: 0.0015 lr: 0.02\n",
            "iteration: 93920 loss: 0.0012 lr: 0.02\n",
            "iteration: 93930 loss: 0.0022 lr: 0.02\n",
            "iteration: 93940 loss: 0.0020 lr: 0.02\n",
            "iteration: 93950 loss: 0.0018 lr: 0.02\n",
            "iteration: 93960 loss: 0.0023 lr: 0.02\n",
            "iteration: 93970 loss: 0.0017 lr: 0.02\n",
            "iteration: 93980 loss: 0.0019 lr: 0.02\n",
            "iteration: 93990 loss: 0.0013 lr: 0.02\n",
            "iteration: 94000 loss: 0.0015 lr: 0.02\n",
            "iteration: 94010 loss: 0.0027 lr: 0.02\n",
            "iteration: 94020 loss: 0.0015 lr: 0.02\n",
            "iteration: 94030 loss: 0.0017 lr: 0.02\n",
            "iteration: 94040 loss: 0.0016 lr: 0.02\n",
            "iteration: 94050 loss: 0.0019 lr: 0.02\n",
            "iteration: 94060 loss: 0.0021 lr: 0.02\n",
            "iteration: 94070 loss: 0.0015 lr: 0.02\n",
            "iteration: 94080 loss: 0.0015 lr: 0.02\n",
            "iteration: 94090 loss: 0.0013 lr: 0.02\n",
            "iteration: 94100 loss: 0.0013 lr: 0.02\n",
            "iteration: 94110 loss: 0.0019 lr: 0.02\n",
            "iteration: 94120 loss: 0.0016 lr: 0.02\n",
            "iteration: 94130 loss: 0.0021 lr: 0.02\n",
            "iteration: 94140 loss: 0.0019 lr: 0.02\n",
            "iteration: 94150 loss: 0.0014 lr: 0.02\n",
            "iteration: 94160 loss: 0.0019 lr: 0.02\n",
            "iteration: 94170 loss: 0.0024 lr: 0.02\n",
            "iteration: 94180 loss: 0.0017 lr: 0.02\n",
            "iteration: 94190 loss: 0.0017 lr: 0.02\n",
            "iteration: 94200 loss: 0.0016 lr: 0.02\n",
            "iteration: 94210 loss: 0.0019 lr: 0.02\n",
            "iteration: 94220 loss: 0.0014 lr: 0.02\n",
            "iteration: 94230 loss: 0.0018 lr: 0.02\n",
            "iteration: 94240 loss: 0.0016 lr: 0.02\n",
            "iteration: 94250 loss: 0.0020 lr: 0.02\n",
            "iteration: 94260 loss: 0.0017 lr: 0.02\n",
            "iteration: 94270 loss: 0.0017 lr: 0.02\n",
            "iteration: 94280 loss: 0.0017 lr: 0.02\n",
            "iteration: 94290 loss: 0.0019 lr: 0.02\n",
            "iteration: 94300 loss: 0.0016 lr: 0.02\n",
            "iteration: 94310 loss: 0.0016 lr: 0.02\n",
            "iteration: 94320 loss: 0.0016 lr: 0.02\n",
            "iteration: 94330 loss: 0.0016 lr: 0.02\n",
            "iteration: 94340 loss: 0.0021 lr: 0.02\n",
            "iteration: 94350 loss: 0.0019 lr: 0.02\n",
            "iteration: 94360 loss: 0.0020 lr: 0.02\n",
            "iteration: 94370 loss: 0.0014 lr: 0.02\n",
            "iteration: 94380 loss: 0.0014 lr: 0.02\n",
            "iteration: 94390 loss: 0.0019 lr: 0.02\n",
            "iteration: 94400 loss: 0.0019 lr: 0.02\n",
            "iteration: 94410 loss: 0.0014 lr: 0.02\n",
            "iteration: 94420 loss: 0.0019 lr: 0.02\n",
            "iteration: 94430 loss: 0.0021 lr: 0.02\n",
            "iteration: 94440 loss: 0.0015 lr: 0.02\n",
            "iteration: 94450 loss: 0.0018 lr: 0.02\n",
            "iteration: 94460 loss: 0.0017 lr: 0.02\n",
            "iteration: 94470 loss: 0.0018 lr: 0.02\n",
            "iteration: 94480 loss: 0.0016 lr: 0.02\n",
            "iteration: 94490 loss: 0.0016 lr: 0.02\n",
            "iteration: 94500 loss: 0.0017 lr: 0.02\n",
            "iteration: 94510 loss: 0.0021 lr: 0.02\n",
            "iteration: 94520 loss: 0.0020 lr: 0.02\n",
            "iteration: 94530 loss: 0.0017 lr: 0.02\n",
            "iteration: 94540 loss: 0.0020 lr: 0.02\n",
            "iteration: 94550 loss: 0.0020 lr: 0.02\n",
            "iteration: 94560 loss: 0.0015 lr: 0.02\n",
            "iteration: 94570 loss: 0.0018 lr: 0.02\n",
            "iteration: 94580 loss: 0.0016 lr: 0.02\n",
            "iteration: 94590 loss: 0.0019 lr: 0.02\n",
            "iteration: 94600 loss: 0.0017 lr: 0.02\n",
            "iteration: 94610 loss: 0.0020 lr: 0.02\n",
            "iteration: 94620 loss: 0.0018 lr: 0.02\n",
            "iteration: 94630 loss: 0.0016 lr: 0.02\n",
            "iteration: 94640 loss: 0.0019 lr: 0.02\n",
            "iteration: 94650 loss: 0.0023 lr: 0.02\n",
            "iteration: 94660 loss: 0.0020 lr: 0.02\n",
            "iteration: 94670 loss: 0.0017 lr: 0.02\n",
            "iteration: 94680 loss: 0.0015 lr: 0.02\n",
            "iteration: 94690 loss: 0.0020 lr: 0.02\n",
            "iteration: 94700 loss: 0.0020 lr: 0.02\n",
            "iteration: 94710 loss: 0.0015 lr: 0.02\n",
            "iteration: 94720 loss: 0.0017 lr: 0.02\n",
            "iteration: 94730 loss: 0.0022 lr: 0.02\n",
            "iteration: 94740 loss: 0.0017 lr: 0.02\n",
            "iteration: 94750 loss: 0.0019 lr: 0.02\n",
            "iteration: 94760 loss: 0.0019 lr: 0.02\n",
            "iteration: 94770 loss: 0.0018 lr: 0.02\n",
            "iteration: 94780 loss: 0.0017 lr: 0.02\n",
            "iteration: 94790 loss: 0.0017 lr: 0.02\n",
            "iteration: 94800 loss: 0.0015 lr: 0.02\n",
            "iteration: 94810 loss: 0.0020 lr: 0.02\n",
            "iteration: 94820 loss: 0.0018 lr: 0.02\n",
            "iteration: 94830 loss: 0.0018 lr: 0.02\n",
            "iteration: 94840 loss: 0.0014 lr: 0.02\n",
            "iteration: 94850 loss: 0.0017 lr: 0.02\n",
            "iteration: 94860 loss: 0.0025 lr: 0.02\n",
            "iteration: 94870 loss: 0.0017 lr: 0.02\n",
            "iteration: 94880 loss: 0.0027 lr: 0.02\n",
            "iteration: 94890 loss: 0.0019 lr: 0.02\n",
            "iteration: 94900 loss: 0.0017 lr: 0.02\n",
            "iteration: 94910 loss: 0.0021 lr: 0.02\n",
            "iteration: 94920 loss: 0.0020 lr: 0.02\n",
            "iteration: 94930 loss: 0.0017 lr: 0.02\n",
            "iteration: 94940 loss: 0.0017 lr: 0.02\n",
            "iteration: 94950 loss: 0.0019 lr: 0.02\n",
            "iteration: 94960 loss: 0.0017 lr: 0.02\n",
            "iteration: 94970 loss: 0.0015 lr: 0.02\n",
            "iteration: 94980 loss: 0.0022 lr: 0.02\n",
            "iteration: 94990 loss: 0.0014 lr: 0.02\n",
            "iteration: 95000 loss: 0.0024 lr: 0.02\n",
            "iteration: 95010 loss: 0.0024 lr: 0.02\n",
            "iteration: 95020 loss: 0.0022 lr: 0.02\n",
            "iteration: 95030 loss: 0.0020 lr: 0.02\n",
            "iteration: 95040 loss: 0.0016 lr: 0.02\n",
            "iteration: 95050 loss: 0.0015 lr: 0.02\n",
            "iteration: 95060 loss: 0.0019 lr: 0.02\n",
            "iteration: 95070 loss: 0.0023 lr: 0.02\n",
            "iteration: 95080 loss: 0.0024 lr: 0.02\n",
            "iteration: 95090 loss: 0.0020 lr: 0.02\n",
            "iteration: 95100 loss: 0.0023 lr: 0.02\n",
            "iteration: 95110 loss: 0.0018 lr: 0.02\n",
            "iteration: 95120 loss: 0.0014 lr: 0.02\n",
            "iteration: 95130 loss: 0.0016 lr: 0.02\n",
            "iteration: 95140 loss: 0.0021 lr: 0.02\n",
            "iteration: 95150 loss: 0.0018 lr: 0.02\n",
            "iteration: 95160 loss: 0.0021 lr: 0.02\n",
            "iteration: 95170 loss: 0.0013 lr: 0.02\n",
            "iteration: 95180 loss: 0.0025 lr: 0.02\n",
            "iteration: 95190 loss: 0.0016 lr: 0.02\n",
            "iteration: 95200 loss: 0.0016 lr: 0.02\n",
            "iteration: 95210 loss: 0.0016 lr: 0.02\n",
            "iteration: 95220 loss: 0.0024 lr: 0.02\n",
            "iteration: 95230 loss: 0.0020 lr: 0.02\n",
            "iteration: 95240 loss: 0.0025 lr: 0.02\n",
            "iteration: 95250 loss: 0.0014 lr: 0.02\n",
            "iteration: 95260 loss: 0.0019 lr: 0.02\n",
            "iteration: 95270 loss: 0.0021 lr: 0.02\n",
            "iteration: 95280 loss: 0.0018 lr: 0.02\n",
            "iteration: 95290 loss: 0.0017 lr: 0.02\n",
            "iteration: 95300 loss: 0.0017 lr: 0.02\n",
            "iteration: 95310 loss: 0.0020 lr: 0.02\n",
            "iteration: 95320 loss: 0.0020 lr: 0.02\n",
            "iteration: 95330 loss: 0.0019 lr: 0.02\n",
            "iteration: 95340 loss: 0.0019 lr: 0.02\n",
            "iteration: 95350 loss: 0.0016 lr: 0.02\n",
            "iteration: 95360 loss: 0.0021 lr: 0.02\n",
            "iteration: 95370 loss: 0.0016 lr: 0.02\n",
            "iteration: 95380 loss: 0.0024 lr: 0.02\n",
            "iteration: 95390 loss: 0.0021 lr: 0.02\n",
            "iteration: 95400 loss: 0.0023 lr: 0.02\n",
            "iteration: 95410 loss: 0.0014 lr: 0.02\n",
            "iteration: 95420 loss: 0.0013 lr: 0.02\n",
            "iteration: 95430 loss: 0.0016 lr: 0.02\n",
            "iteration: 95440 loss: 0.0013 lr: 0.02\n",
            "iteration: 95450 loss: 0.0021 lr: 0.02\n",
            "iteration: 95460 loss: 0.0016 lr: 0.02\n",
            "iteration: 95470 loss: 0.0018 lr: 0.02\n",
            "iteration: 95480 loss: 0.0020 lr: 0.02\n",
            "iteration: 95490 loss: 0.0015 lr: 0.02\n",
            "iteration: 95500 loss: 0.0017 lr: 0.02\n",
            "iteration: 95510 loss: 0.0024 lr: 0.02\n",
            "iteration: 95520 loss: 0.0017 lr: 0.02\n",
            "iteration: 95530 loss: 0.0018 lr: 0.02\n",
            "iteration: 95540 loss: 0.0018 lr: 0.02\n",
            "iteration: 95550 loss: 0.0015 lr: 0.02\n",
            "iteration: 95560 loss: 0.0017 lr: 0.02\n",
            "iteration: 95570 loss: 0.0017 lr: 0.02\n",
            "iteration: 95580 loss: 0.0018 lr: 0.02\n",
            "iteration: 95590 loss: 0.0016 lr: 0.02\n",
            "iteration: 95600 loss: 0.0017 lr: 0.02\n",
            "iteration: 95610 loss: 0.0014 lr: 0.02\n",
            "iteration: 95620 loss: 0.0019 lr: 0.02\n",
            "iteration: 95630 loss: 0.0018 lr: 0.02\n",
            "iteration: 95640 loss: 0.0021 lr: 0.02\n",
            "iteration: 95650 loss: 0.0018 lr: 0.02\n",
            "iteration: 95660 loss: 0.0019 lr: 0.02\n",
            "iteration: 95670 loss: 0.0015 lr: 0.02\n",
            "iteration: 95680 loss: 0.0017 lr: 0.02\n",
            "iteration: 95690 loss: 0.0016 lr: 0.02\n",
            "iteration: 95700 loss: 0.0017 lr: 0.02\n",
            "iteration: 95710 loss: 0.0019 lr: 0.02\n",
            "iteration: 95720 loss: 0.0017 lr: 0.02\n",
            "iteration: 95730 loss: 0.0016 lr: 0.02\n",
            "iteration: 95740 loss: 0.0017 lr: 0.02\n",
            "iteration: 95750 loss: 0.0016 lr: 0.02\n",
            "iteration: 95760 loss: 0.0015 lr: 0.02\n",
            "iteration: 95770 loss: 0.0021 lr: 0.02\n",
            "iteration: 95780 loss: 0.0019 lr: 0.02\n",
            "iteration: 95790 loss: 0.0019 lr: 0.02\n",
            "iteration: 95800 loss: 0.0015 lr: 0.02\n",
            "iteration: 95810 loss: 0.0018 lr: 0.02\n",
            "iteration: 95820 loss: 0.0013 lr: 0.02\n",
            "iteration: 95830 loss: 0.0017 lr: 0.02\n",
            "iteration: 95840 loss: 0.0020 lr: 0.02\n",
            "iteration: 95850 loss: 0.0019 lr: 0.02\n",
            "iteration: 95860 loss: 0.0018 lr: 0.02\n",
            "iteration: 95870 loss: 0.0021 lr: 0.02\n",
            "iteration: 95880 loss: 0.0016 lr: 0.02\n",
            "iteration: 95890 loss: 0.0025 lr: 0.02\n",
            "iteration: 95900 loss: 0.0016 lr: 0.02\n",
            "iteration: 95910 loss: 0.0025 lr: 0.02\n",
            "iteration: 95920 loss: 0.0021 lr: 0.02\n",
            "iteration: 95930 loss: 0.0020 lr: 0.02\n",
            "iteration: 95940 loss: 0.0016 lr: 0.02\n",
            "iteration: 95950 loss: 0.0019 lr: 0.02\n",
            "iteration: 95960 loss: 0.0019 lr: 0.02\n",
            "iteration: 95970 loss: 0.0015 lr: 0.02\n",
            "iteration: 95980 loss: 0.0018 lr: 0.02\n",
            "iteration: 95990 loss: 0.0015 lr: 0.02\n",
            "iteration: 96000 loss: 0.0016 lr: 0.02\n",
            "iteration: 96010 loss: 0.0015 lr: 0.02\n",
            "iteration: 96020 loss: 0.0013 lr: 0.02\n",
            "iteration: 96030 loss: 0.0019 lr: 0.02\n",
            "iteration: 96040 loss: 0.0015 lr: 0.02\n",
            "iteration: 96050 loss: 0.0018 lr: 0.02\n",
            "iteration: 96060 loss: 0.0016 lr: 0.02\n",
            "iteration: 96070 loss: 0.0017 lr: 0.02\n",
            "iteration: 96080 loss: 0.0017 lr: 0.02\n",
            "iteration: 96090 loss: 0.0020 lr: 0.02\n",
            "iteration: 96100 loss: 0.0019 lr: 0.02\n",
            "iteration: 96110 loss: 0.0018 lr: 0.02\n",
            "iteration: 96120 loss: 0.0014 lr: 0.02\n",
            "iteration: 96130 loss: 0.0019 lr: 0.02\n",
            "iteration: 96140 loss: 0.0015 lr: 0.02\n",
            "iteration: 96150 loss: 0.0022 lr: 0.02\n",
            "iteration: 96160 loss: 0.0017 lr: 0.02\n",
            "iteration: 96170 loss: 0.0016 lr: 0.02\n",
            "iteration: 96180 loss: 0.0030 lr: 0.02\n",
            "iteration: 96190 loss: 0.0018 lr: 0.02\n",
            "iteration: 96200 loss: 0.0016 lr: 0.02\n",
            "iteration: 96210 loss: 0.0016 lr: 0.02\n",
            "iteration: 96220 loss: 0.0015 lr: 0.02\n",
            "iteration: 96230 loss: 0.0015 lr: 0.02\n",
            "iteration: 96240 loss: 0.0019 lr: 0.02\n",
            "iteration: 96250 loss: 0.0018 lr: 0.02\n",
            "iteration: 96260 loss: 0.0013 lr: 0.02\n",
            "iteration: 96270 loss: 0.0019 lr: 0.02\n",
            "iteration: 96280 loss: 0.0017 lr: 0.02\n",
            "iteration: 96290 loss: 0.0019 lr: 0.02\n",
            "iteration: 96300 loss: 0.0015 lr: 0.02\n",
            "iteration: 96310 loss: 0.0017 lr: 0.02\n",
            "iteration: 96320 loss: 0.0018 lr: 0.02\n",
            "iteration: 96330 loss: 0.0015 lr: 0.02\n",
            "iteration: 96340 loss: 0.0017 lr: 0.02\n",
            "iteration: 96350 loss: 0.0017 lr: 0.02\n",
            "iteration: 96360 loss: 0.0012 lr: 0.02\n",
            "iteration: 96370 loss: 0.0019 lr: 0.02\n",
            "iteration: 96380 loss: 0.0017 lr: 0.02\n",
            "iteration: 96390 loss: 0.0022 lr: 0.02\n",
            "iteration: 96400 loss: 0.0017 lr: 0.02\n",
            "iteration: 96410 loss: 0.0013 lr: 0.02\n",
            "iteration: 96420 loss: 0.0016 lr: 0.02\n",
            "iteration: 96430 loss: 0.0018 lr: 0.02\n",
            "iteration: 96440 loss: 0.0023 lr: 0.02\n",
            "iteration: 96450 loss: 0.0016 lr: 0.02\n",
            "iteration: 96460 loss: 0.0023 lr: 0.02\n",
            "iteration: 96470 loss: 0.0016 lr: 0.02\n",
            "iteration: 96480 loss: 0.0020 lr: 0.02\n",
            "iteration: 96490 loss: 0.0016 lr: 0.02\n",
            "iteration: 96500 loss: 0.0014 lr: 0.02\n",
            "iteration: 96510 loss: 0.0017 lr: 0.02\n",
            "iteration: 96520 loss: 0.0023 lr: 0.02\n",
            "iteration: 96530 loss: 0.0025 lr: 0.02\n",
            "iteration: 96540 loss: 0.0016 lr: 0.02\n",
            "iteration: 96550 loss: 0.0016 lr: 0.02\n",
            "iteration: 96560 loss: 0.0017 lr: 0.02\n",
            "iteration: 96570 loss: 0.0016 lr: 0.02\n",
            "iteration: 96580 loss: 0.0018 lr: 0.02\n",
            "iteration: 96590 loss: 0.0020 lr: 0.02\n",
            "iteration: 96600 loss: 0.0018 lr: 0.02\n",
            "iteration: 96610 loss: 0.0022 lr: 0.02\n",
            "iteration: 96620 loss: 0.0022 lr: 0.02\n",
            "iteration: 96630 loss: 0.0017 lr: 0.02\n",
            "iteration: 96640 loss: 0.0016 lr: 0.02\n",
            "iteration: 96650 loss: 0.0017 lr: 0.02\n",
            "iteration: 96660 loss: 0.0015 lr: 0.02\n",
            "iteration: 96670 loss: 0.0017 lr: 0.02\n",
            "iteration: 96680 loss: 0.0013 lr: 0.02\n",
            "iteration: 96690 loss: 0.0018 lr: 0.02\n",
            "iteration: 96700 loss: 0.0018 lr: 0.02\n",
            "iteration: 96710 loss: 0.0016 lr: 0.02\n",
            "iteration: 96720 loss: 0.0021 lr: 0.02\n",
            "iteration: 96730 loss: 0.0013 lr: 0.02\n",
            "iteration: 96740 loss: 0.0019 lr: 0.02\n",
            "iteration: 96750 loss: 0.0014 lr: 0.02\n",
            "iteration: 96760 loss: 0.0017 lr: 0.02\n",
            "iteration: 96770 loss: 0.0014 lr: 0.02\n",
            "iteration: 96780 loss: 0.0022 lr: 0.02\n",
            "iteration: 96790 loss: 0.0020 lr: 0.02\n",
            "iteration: 96800 loss: 0.0022 lr: 0.02\n",
            "iteration: 96810 loss: 0.0016 lr: 0.02\n",
            "iteration: 96820 loss: 0.0015 lr: 0.02\n",
            "iteration: 96830 loss: 0.0021 lr: 0.02\n",
            "iteration: 96840 loss: 0.0018 lr: 0.02\n",
            "iteration: 96850 loss: 0.0019 lr: 0.02\n",
            "iteration: 96860 loss: 0.0016 lr: 0.02\n",
            "iteration: 96870 loss: 0.0016 lr: 0.02\n",
            "iteration: 96880 loss: 0.0020 lr: 0.02\n",
            "iteration: 96890 loss: 0.0016 lr: 0.02\n",
            "iteration: 96900 loss: 0.0017 lr: 0.02\n",
            "iteration: 96910 loss: 0.0024 lr: 0.02\n",
            "iteration: 96920 loss: 0.0018 lr: 0.02\n",
            "iteration: 96930 loss: 0.0016 lr: 0.02\n",
            "iteration: 96940 loss: 0.0019 lr: 0.02\n",
            "iteration: 96950 loss: 0.0021 lr: 0.02\n",
            "iteration: 96960 loss: 0.0016 lr: 0.02\n",
            "iteration: 96970 loss: 0.0015 lr: 0.02\n",
            "iteration: 96980 loss: 0.0016 lr: 0.02\n",
            "iteration: 96990 loss: 0.0021 lr: 0.02\n",
            "iteration: 97000 loss: 0.0018 lr: 0.02\n",
            "iteration: 97010 loss: 0.0014 lr: 0.02\n",
            "iteration: 97020 loss: 0.0018 lr: 0.02\n",
            "iteration: 97030 loss: 0.0018 lr: 0.02\n",
            "iteration: 97040 loss: 0.0023 lr: 0.02\n",
            "iteration: 97050 loss: 0.0017 lr: 0.02\n",
            "iteration: 97060 loss: 0.0019 lr: 0.02\n",
            "iteration: 97070 loss: 0.0020 lr: 0.02\n",
            "iteration: 97080 loss: 0.0016 lr: 0.02\n",
            "iteration: 97090 loss: 0.0019 lr: 0.02\n",
            "iteration: 97100 loss: 0.0021 lr: 0.02\n",
            "iteration: 97110 loss: 0.0021 lr: 0.02\n",
            "iteration: 97120 loss: 0.0020 lr: 0.02\n",
            "iteration: 97130 loss: 0.0016 lr: 0.02\n",
            "iteration: 97140 loss: 0.0017 lr: 0.02\n",
            "iteration: 97150 loss: 0.0020 lr: 0.02\n",
            "iteration: 97160 loss: 0.0014 lr: 0.02\n",
            "iteration: 97170 loss: 0.0016 lr: 0.02\n",
            "iteration: 97180 loss: 0.0018 lr: 0.02\n",
            "iteration: 97190 loss: 0.0019 lr: 0.02\n",
            "iteration: 97200 loss: 0.0015 lr: 0.02\n",
            "iteration: 97210 loss: 0.0015 lr: 0.02\n",
            "iteration: 97220 loss: 0.0021 lr: 0.02\n",
            "iteration: 97230 loss: 0.0017 lr: 0.02\n",
            "iteration: 97240 loss: 0.0018 lr: 0.02\n",
            "iteration: 97250 loss: 0.0017 lr: 0.02\n",
            "iteration: 97260 loss: 0.0018 lr: 0.02\n",
            "iteration: 97270 loss: 0.0018 lr: 0.02\n",
            "iteration: 97280 loss: 0.0018 lr: 0.02\n",
            "iteration: 97290 loss: 0.0020 lr: 0.02\n",
            "iteration: 97300 loss: 0.0015 lr: 0.02\n",
            "iteration: 97310 loss: 0.0014 lr: 0.02\n",
            "iteration: 97320 loss: 0.0020 lr: 0.02\n",
            "iteration: 97330 loss: 0.0023 lr: 0.02\n",
            "iteration: 97340 loss: 0.0018 lr: 0.02\n",
            "iteration: 97350 loss: 0.0012 lr: 0.02\n",
            "iteration: 97360 loss: 0.0017 lr: 0.02\n",
            "iteration: 97370 loss: 0.0017 lr: 0.02\n",
            "iteration: 97380 loss: 0.0020 lr: 0.02\n",
            "iteration: 97390 loss: 0.0015 lr: 0.02\n",
            "iteration: 97400 loss: 0.0018 lr: 0.02\n",
            "iteration: 97410 loss: 0.0021 lr: 0.02\n",
            "iteration: 97420 loss: 0.0014 lr: 0.02\n",
            "iteration: 97430 loss: 0.0017 lr: 0.02\n",
            "iteration: 97440 loss: 0.0017 lr: 0.02\n",
            "iteration: 97450 loss: 0.0019 lr: 0.02\n",
            "iteration: 97460 loss: 0.0013 lr: 0.02\n",
            "iteration: 97470 loss: 0.0019 lr: 0.02\n",
            "iteration: 97480 loss: 0.0014 lr: 0.02\n",
            "iteration: 97490 loss: 0.0018 lr: 0.02\n",
            "iteration: 97500 loss: 0.0015 lr: 0.02\n",
            "iteration: 97510 loss: 0.0015 lr: 0.02\n",
            "iteration: 97520 loss: 0.0020 lr: 0.02\n",
            "iteration: 97530 loss: 0.0020 lr: 0.02\n",
            "iteration: 97540 loss: 0.0017 lr: 0.02\n",
            "iteration: 97550 loss: 0.0019 lr: 0.02\n",
            "iteration: 97560 loss: 0.0015 lr: 0.02\n",
            "iteration: 97570 loss: 0.0012 lr: 0.02\n",
            "iteration: 97580 loss: 0.0021 lr: 0.02\n",
            "iteration: 97590 loss: 0.0019 lr: 0.02\n",
            "iteration: 97600 loss: 0.0016 lr: 0.02\n",
            "iteration: 97610 loss: 0.0017 lr: 0.02\n",
            "iteration: 97620 loss: 0.0017 lr: 0.02\n",
            "iteration: 97630 loss: 0.0017 lr: 0.02\n",
            "iteration: 97640 loss: 0.0015 lr: 0.02\n",
            "iteration: 97650 loss: 0.0016 lr: 0.02\n",
            "iteration: 97660 loss: 0.0016 lr: 0.02\n",
            "iteration: 97670 loss: 0.0022 lr: 0.02\n",
            "iteration: 97680 loss: 0.0016 lr: 0.02\n",
            "iteration: 97690 loss: 0.0021 lr: 0.02\n",
            "iteration: 97700 loss: 0.0017 lr: 0.02\n",
            "iteration: 97710 loss: 0.0016 lr: 0.02\n",
            "iteration: 97720 loss: 0.0017 lr: 0.02\n",
            "iteration: 97730 loss: 0.0014 lr: 0.02\n",
            "iteration: 97740 loss: 0.0016 lr: 0.02\n",
            "iteration: 97750 loss: 0.0018 lr: 0.02\n",
            "iteration: 97760 loss: 0.0020 lr: 0.02\n",
            "iteration: 97770 loss: 0.0019 lr: 0.02\n",
            "iteration: 97780 loss: 0.0017 lr: 0.02\n",
            "iteration: 97790 loss: 0.0020 lr: 0.02\n",
            "iteration: 97800 loss: 0.0021 lr: 0.02\n",
            "iteration: 97810 loss: 0.0017 lr: 0.02\n",
            "iteration: 97820 loss: 0.0013 lr: 0.02\n",
            "iteration: 97830 loss: 0.0013 lr: 0.02\n",
            "iteration: 97840 loss: 0.0018 lr: 0.02\n",
            "iteration: 97850 loss: 0.0021 lr: 0.02\n",
            "iteration: 97860 loss: 0.0016 lr: 0.02\n",
            "iteration: 97870 loss: 0.0017 lr: 0.02\n",
            "iteration: 97880 loss: 0.0018 lr: 0.02\n",
            "iteration: 97890 loss: 0.0017 lr: 0.02\n",
            "iteration: 97900 loss: 0.0013 lr: 0.02\n",
            "iteration: 97910 loss: 0.0021 lr: 0.02\n",
            "iteration: 97920 loss: 0.0024 lr: 0.02\n",
            "iteration: 97930 loss: 0.0016 lr: 0.02\n",
            "iteration: 97940 loss: 0.0015 lr: 0.02\n",
            "iteration: 97950 loss: 0.0020 lr: 0.02\n",
            "iteration: 97960 loss: 0.0018 lr: 0.02\n",
            "iteration: 97970 loss: 0.0019 lr: 0.02\n",
            "iteration: 97980 loss: 0.0017 lr: 0.02\n",
            "iteration: 97990 loss: 0.0019 lr: 0.02\n",
            "iteration: 98000 loss: 0.0017 lr: 0.02\n",
            "iteration: 98010 loss: 0.0016 lr: 0.02\n",
            "iteration: 98020 loss: 0.0017 lr: 0.02\n",
            "iteration: 98030 loss: 0.0020 lr: 0.02\n",
            "iteration: 98040 loss: 0.0018 lr: 0.02\n",
            "iteration: 98050 loss: 0.0018 lr: 0.02\n",
            "iteration: 98060 loss: 0.0016 lr: 0.02\n",
            "iteration: 98070 loss: 0.0018 lr: 0.02\n",
            "iteration: 98080 loss: 0.0023 lr: 0.02\n",
            "iteration: 98090 loss: 0.0020 lr: 0.02\n",
            "iteration: 98100 loss: 0.0018 lr: 0.02\n",
            "iteration: 98110 loss: 0.0019 lr: 0.02\n",
            "iteration: 98120 loss: 0.0020 lr: 0.02\n",
            "iteration: 98130 loss: 0.0018 lr: 0.02\n",
            "iteration: 98140 loss: 0.0019 lr: 0.02\n",
            "iteration: 98150 loss: 0.0014 lr: 0.02\n",
            "iteration: 98160 loss: 0.0016 lr: 0.02\n",
            "iteration: 98170 loss: 0.0021 lr: 0.02\n",
            "iteration: 98180 loss: 0.0015 lr: 0.02\n",
            "iteration: 98190 loss: 0.0016 lr: 0.02\n",
            "iteration: 98200 loss: 0.0022 lr: 0.02\n",
            "iteration: 98210 loss: 0.0016 lr: 0.02\n",
            "iteration: 98220 loss: 0.0015 lr: 0.02\n",
            "iteration: 98230 loss: 0.0023 lr: 0.02\n",
            "iteration: 98240 loss: 0.0022 lr: 0.02\n",
            "iteration: 98250 loss: 0.0015 lr: 0.02\n",
            "iteration: 98260 loss: 0.0023 lr: 0.02\n",
            "iteration: 98270 loss: 0.0019 lr: 0.02\n",
            "iteration: 98280 loss: 0.0019 lr: 0.02\n",
            "iteration: 98290 loss: 0.0015 lr: 0.02\n",
            "iteration: 98300 loss: 0.0021 lr: 0.02\n",
            "iteration: 98310 loss: 0.0020 lr: 0.02\n",
            "iteration: 98320 loss: 0.0018 lr: 0.02\n",
            "iteration: 98330 loss: 0.0019 lr: 0.02\n",
            "iteration: 98340 loss: 0.0019 lr: 0.02\n",
            "iteration: 98350 loss: 0.0014 lr: 0.02\n",
            "iteration: 98360 loss: 0.0014 lr: 0.02\n",
            "iteration: 98370 loss: 0.0015 lr: 0.02\n",
            "iteration: 98380 loss: 0.0016 lr: 0.02\n",
            "iteration: 98390 loss: 0.0015 lr: 0.02\n",
            "iteration: 98400 loss: 0.0019 lr: 0.02\n",
            "iteration: 98410 loss: 0.0015 lr: 0.02\n",
            "iteration: 98420 loss: 0.0016 lr: 0.02\n",
            "iteration: 98430 loss: 0.0017 lr: 0.02\n",
            "iteration: 98440 loss: 0.0014 lr: 0.02\n",
            "iteration: 98450 loss: 0.0011 lr: 0.02\n",
            "iteration: 98460 loss: 0.0020 lr: 0.02\n",
            "iteration: 98470 loss: 0.0020 lr: 0.02\n",
            "iteration: 98480 loss: 0.0021 lr: 0.02\n",
            "iteration: 98490 loss: 0.0016 lr: 0.02\n",
            "iteration: 98500 loss: 0.0019 lr: 0.02\n",
            "iteration: 98510 loss: 0.0014 lr: 0.02\n",
            "iteration: 98520 loss: 0.0016 lr: 0.02\n",
            "iteration: 98530 loss: 0.0018 lr: 0.02\n",
            "iteration: 98540 loss: 0.0018 lr: 0.02\n",
            "iteration: 98550 loss: 0.0018 lr: 0.02\n",
            "iteration: 98560 loss: 0.0018 lr: 0.02\n",
            "iteration: 98570 loss: 0.0016 lr: 0.02\n",
            "iteration: 98580 loss: 0.0017 lr: 0.02\n",
            "iteration: 98590 loss: 0.0015 lr: 0.02\n",
            "iteration: 98600 loss: 0.0019 lr: 0.02\n",
            "iteration: 98610 loss: 0.0019 lr: 0.02\n",
            "iteration: 98620 loss: 0.0014 lr: 0.02\n",
            "iteration: 98630 loss: 0.0017 lr: 0.02\n",
            "iteration: 98640 loss: 0.0020 lr: 0.02\n",
            "iteration: 98650 loss: 0.0013 lr: 0.02\n",
            "iteration: 98660 loss: 0.0019 lr: 0.02\n",
            "iteration: 98670 loss: 0.0017 lr: 0.02\n",
            "iteration: 98680 loss: 0.0016 lr: 0.02\n",
            "iteration: 98690 loss: 0.0015 lr: 0.02\n",
            "iteration: 98700 loss: 0.0015 lr: 0.02\n",
            "iteration: 98710 loss: 0.0017 lr: 0.02\n",
            "iteration: 98720 loss: 0.0016 lr: 0.02\n",
            "iteration: 98730 loss: 0.0016 lr: 0.02\n",
            "iteration: 98740 loss: 0.0013 lr: 0.02\n",
            "iteration: 98750 loss: 0.0017 lr: 0.02\n",
            "iteration: 98760 loss: 0.0018 lr: 0.02\n",
            "iteration: 98770 loss: 0.0013 lr: 0.02\n",
            "iteration: 98780 loss: 0.0022 lr: 0.02\n",
            "iteration: 98790 loss: 0.0018 lr: 0.02\n",
            "iteration: 98800 loss: 0.0013 lr: 0.02\n",
            "iteration: 98810 loss: 0.0015 lr: 0.02\n",
            "iteration: 98820 loss: 0.0015 lr: 0.02\n",
            "iteration: 98830 loss: 0.0014 lr: 0.02\n",
            "iteration: 98840 loss: 0.0018 lr: 0.02\n",
            "iteration: 98850 loss: 0.0020 lr: 0.02\n",
            "iteration: 98860 loss: 0.0019 lr: 0.02\n",
            "iteration: 98870 loss: 0.0017 lr: 0.02\n",
            "iteration: 98880 loss: 0.0019 lr: 0.02\n",
            "iteration: 98890 loss: 0.0024 lr: 0.02\n",
            "iteration: 98900 loss: 0.0018 lr: 0.02\n",
            "iteration: 98910 loss: 0.0018 lr: 0.02\n",
            "iteration: 98920 loss: 0.0018 lr: 0.02\n",
            "iteration: 98930 loss: 0.0018 lr: 0.02\n",
            "iteration: 98940 loss: 0.0020 lr: 0.02\n",
            "iteration: 98950 loss: 0.0014 lr: 0.02\n",
            "iteration: 98960 loss: 0.0016 lr: 0.02\n",
            "iteration: 98970 loss: 0.0018 lr: 0.02\n",
            "iteration: 98980 loss: 0.0018 lr: 0.02\n",
            "iteration: 98990 loss: 0.0017 lr: 0.02\n",
            "iteration: 99000 loss: 0.0020 lr: 0.02\n",
            "iteration: 99010 loss: 0.0021 lr: 0.02\n",
            "iteration: 99020 loss: 0.0018 lr: 0.02\n",
            "iteration: 99030 loss: 0.0017 lr: 0.02\n",
            "iteration: 99040 loss: 0.0019 lr: 0.02\n",
            "iteration: 99050 loss: 0.0021 lr: 0.02\n",
            "iteration: 99060 loss: 0.0021 lr: 0.02\n",
            "iteration: 99070 loss: 0.0014 lr: 0.02\n",
            "iteration: 99080 loss: 0.0019 lr: 0.02\n",
            "iteration: 99090 loss: 0.0017 lr: 0.02\n",
            "iteration: 99100 loss: 0.0019 lr: 0.02\n",
            "iteration: 99110 loss: 0.0017 lr: 0.02\n",
            "iteration: 99120 loss: 0.0018 lr: 0.02\n",
            "iteration: 99130 loss: 0.0021 lr: 0.02\n",
            "iteration: 99140 loss: 0.0021 lr: 0.02\n",
            "iteration: 99150 loss: 0.0023 lr: 0.02\n",
            "iteration: 99160 loss: 0.0018 lr: 0.02\n",
            "iteration: 99170 loss: 0.0022 lr: 0.02\n",
            "iteration: 99180 loss: 0.0017 lr: 0.02\n",
            "iteration: 99190 loss: 0.0018 lr: 0.02\n",
            "iteration: 99200 loss: 0.0015 lr: 0.02\n",
            "iteration: 99210 loss: 0.0023 lr: 0.02\n",
            "iteration: 99220 loss: 0.0018 lr: 0.02\n",
            "iteration: 99230 loss: 0.0018 lr: 0.02\n",
            "iteration: 99240 loss: 0.0020 lr: 0.02\n",
            "iteration: 99250 loss: 0.0018 lr: 0.02\n",
            "iteration: 99260 loss: 0.0014 lr: 0.02\n",
            "iteration: 99270 loss: 0.0019 lr: 0.02\n",
            "iteration: 99280 loss: 0.0024 lr: 0.02\n",
            "iteration: 99290 loss: 0.0012 lr: 0.02\n",
            "iteration: 99300 loss: 0.0020 lr: 0.02\n",
            "iteration: 99310 loss: 0.0025 lr: 0.02\n",
            "iteration: 99320 loss: 0.0013 lr: 0.02\n",
            "iteration: 99330 loss: 0.0015 lr: 0.02\n",
            "iteration: 99340 loss: 0.0016 lr: 0.02\n",
            "iteration: 99350 loss: 0.0020 lr: 0.02\n",
            "iteration: 99360 loss: 0.0018 lr: 0.02\n",
            "iteration: 99370 loss: 0.0020 lr: 0.02\n",
            "iteration: 99380 loss: 0.0016 lr: 0.02\n",
            "iteration: 99390 loss: 0.0013 lr: 0.02\n",
            "iteration: 99400 loss: 0.0020 lr: 0.02\n",
            "iteration: 99410 loss: 0.0015 lr: 0.02\n",
            "iteration: 99420 loss: 0.0017 lr: 0.02\n",
            "iteration: 99430 loss: 0.0018 lr: 0.02\n",
            "iteration: 99440 loss: 0.0016 lr: 0.02\n",
            "iteration: 99450 loss: 0.0021 lr: 0.02\n",
            "iteration: 99460 loss: 0.0018 lr: 0.02\n",
            "iteration: 99470 loss: 0.0015 lr: 0.02\n",
            "iteration: 99480 loss: 0.0019 lr: 0.02\n",
            "iteration: 99490 loss: 0.0019 lr: 0.02\n",
            "iteration: 99500 loss: 0.0018 lr: 0.02\n",
            "iteration: 99510 loss: 0.0021 lr: 0.02\n",
            "iteration: 99520 loss: 0.0014 lr: 0.02\n",
            "iteration: 99530 loss: 0.0021 lr: 0.02\n",
            "iteration: 99540 loss: 0.0017 lr: 0.02\n",
            "iteration: 99550 loss: 0.0014 lr: 0.02\n",
            "iteration: 99560 loss: 0.0016 lr: 0.02\n",
            "iteration: 99570 loss: 0.0015 lr: 0.02\n",
            "iteration: 99580 loss: 0.0016 lr: 0.02\n",
            "iteration: 99590 loss: 0.0015 lr: 0.02\n",
            "iteration: 99600 loss: 0.0016 lr: 0.02\n",
            "iteration: 99610 loss: 0.0022 lr: 0.02\n",
            "iteration: 99620 loss: 0.0014 lr: 0.02\n",
            "iteration: 99630 loss: 0.0023 lr: 0.02\n",
            "iteration: 99640 loss: 0.0019 lr: 0.02\n",
            "iteration: 99650 loss: 0.0017 lr: 0.02\n",
            "iteration: 99660 loss: 0.0023 lr: 0.02\n",
            "iteration: 99670 loss: 0.0017 lr: 0.02\n",
            "iteration: 99680 loss: 0.0015 lr: 0.02\n",
            "iteration: 99690 loss: 0.0015 lr: 0.02\n",
            "iteration: 99700 loss: 0.0014 lr: 0.02\n",
            "iteration: 99710 loss: 0.0023 lr: 0.02\n",
            "iteration: 99720 loss: 0.0013 lr: 0.02\n",
            "iteration: 99730 loss: 0.0018 lr: 0.02\n",
            "iteration: 99740 loss: 0.0022 lr: 0.02\n",
            "iteration: 99750 loss: 0.0017 lr: 0.02\n",
            "iteration: 99760 loss: 0.0020 lr: 0.02\n",
            "iteration: 99770 loss: 0.0015 lr: 0.02\n",
            "iteration: 99780 loss: 0.0023 lr: 0.02\n",
            "iteration: 99790 loss: 0.0016 lr: 0.02\n",
            "iteration: 99800 loss: 0.0019 lr: 0.02\n",
            "iteration: 99810 loss: 0.0024 lr: 0.02\n",
            "iteration: 99820 loss: 0.0024 lr: 0.02\n",
            "iteration: 99830 loss: 0.0017 lr: 0.02\n",
            "iteration: 99840 loss: 0.0026 lr: 0.02\n",
            "iteration: 99850 loss: 0.0014 lr: 0.02\n",
            "iteration: 99860 loss: 0.0021 lr: 0.02\n",
            "iteration: 99870 loss: 0.0016 lr: 0.02\n",
            "iteration: 99880 loss: 0.0021 lr: 0.02\n",
            "iteration: 99890 loss: 0.0019 lr: 0.02\n",
            "iteration: 99900 loss: 0.0022 lr: 0.02\n",
            "iteration: 99910 loss: 0.0015 lr: 0.02\n",
            "iteration: 99920 loss: 0.0015 lr: 0.02\n",
            "iteration: 99930 loss: 0.0017 lr: 0.02\n",
            "iteration: 99940 loss: 0.0019 lr: 0.02\n",
            "iteration: 99950 loss: 0.0022 lr: 0.02\n",
            "iteration: 99960 loss: 0.0014 lr: 0.02\n",
            "iteration: 99970 loss: 0.0016 lr: 0.02\n",
            "iteration: 99980 loss: 0.0018 lr: 0.02\n",
            "iteration: 99990 loss: 0.0019 lr: 0.02\n",
            "iteration: 100000 loss: 0.0015 lr: 0.02\n",
            "iteration: 100010 loss: 0.0017 lr: 0.02\n",
            "iteration: 100020 loss: 0.0020 lr: 0.02\n",
            "iteration: 100030 loss: 0.0015 lr: 0.02\n",
            "iteration: 100040 loss: 0.0018 lr: 0.02\n",
            "iteration: 100050 loss: 0.0019 lr: 0.02\n",
            "iteration: 100060 loss: 0.0019 lr: 0.02\n",
            "iteration: 100070 loss: 0.0017 lr: 0.02\n",
            "iteration: 100080 loss: 0.0013 lr: 0.02\n",
            "iteration: 100090 loss: 0.0021 lr: 0.02\n",
            "iteration: 100100 loss: 0.0017 lr: 0.02\n",
            "iteration: 100110 loss: 0.0017 lr: 0.02\n",
            "iteration: 100120 loss: 0.0018 lr: 0.02\n",
            "iteration: 100130 loss: 0.0014 lr: 0.02\n",
            "iteration: 100140 loss: 0.0016 lr: 0.02\n",
            "iteration: 100150 loss: 0.0013 lr: 0.02\n",
            "iteration: 100160 loss: 0.0020 lr: 0.02\n",
            "iteration: 100170 loss: 0.0018 lr: 0.02\n",
            "iteration: 100180 loss: 0.0021 lr: 0.02\n",
            "iteration: 100190 loss: 0.0020 lr: 0.02\n",
            "iteration: 100200 loss: 0.0023 lr: 0.02\n",
            "iteration: 100210 loss: 0.0014 lr: 0.02\n",
            "iteration: 100220 loss: 0.0016 lr: 0.02\n",
            "iteration: 100230 loss: 0.0019 lr: 0.02\n",
            "iteration: 100240 loss: 0.0015 lr: 0.02\n",
            "iteration: 100250 loss: 0.0020 lr: 0.02\n",
            "iteration: 100260 loss: 0.0021 lr: 0.02\n",
            "iteration: 100270 loss: 0.0019 lr: 0.02\n",
            "iteration: 100280 loss: 0.0017 lr: 0.02\n",
            "iteration: 100290 loss: 0.0028 lr: 0.02\n",
            "iteration: 100300 loss: 0.0021 lr: 0.02\n",
            "iteration: 100310 loss: 0.0017 lr: 0.02\n",
            "iteration: 100320 loss: 0.0019 lr: 0.02\n",
            "iteration: 100330 loss: 0.0015 lr: 0.02\n",
            "iteration: 100340 loss: 0.0014 lr: 0.02\n",
            "iteration: 100350 loss: 0.0012 lr: 0.02\n",
            "iteration: 100360 loss: 0.0015 lr: 0.02\n",
            "iteration: 100370 loss: 0.0020 lr: 0.02\n",
            "iteration: 100380 loss: 0.0026 lr: 0.02\n",
            "iteration: 100390 loss: 0.0017 lr: 0.02\n",
            "iteration: 100400 loss: 0.0020 lr: 0.02\n",
            "iteration: 100410 loss: 0.0014 lr: 0.02\n",
            "iteration: 100420 loss: 0.0016 lr: 0.02\n",
            "iteration: 100430 loss: 0.0019 lr: 0.02\n",
            "iteration: 100440 loss: 0.0017 lr: 0.02\n",
            "iteration: 100450 loss: 0.0018 lr: 0.02\n",
            "iteration: 100460 loss: 0.0014 lr: 0.02\n",
            "iteration: 100470 loss: 0.0014 lr: 0.02\n",
            "iteration: 100480 loss: 0.0016 lr: 0.02\n",
            "iteration: 100490 loss: 0.0018 lr: 0.02\n",
            "iteration: 100500 loss: 0.0016 lr: 0.02\n",
            "iteration: 100510 loss: 0.0014 lr: 0.02\n",
            "iteration: 100520 loss: 0.0015 lr: 0.02\n",
            "iteration: 100530 loss: 0.0015 lr: 0.02\n",
            "iteration: 100540 loss: 0.0018 lr: 0.02\n",
            "iteration: 100550 loss: 0.0019 lr: 0.02\n",
            "iteration: 100560 loss: 0.0020 lr: 0.02\n",
            "iteration: 100570 loss: 0.0015 lr: 0.02\n",
            "iteration: 100580 loss: 0.0019 lr: 0.02\n",
            "iteration: 100590 loss: 0.0017 lr: 0.02\n",
            "iteration: 100600 loss: 0.0016 lr: 0.02\n",
            "iteration: 100610 loss: 0.0020 lr: 0.02\n",
            "iteration: 100620 loss: 0.0017 lr: 0.02\n",
            "iteration: 100630 loss: 0.0020 lr: 0.02\n",
            "iteration: 100640 loss: 0.0013 lr: 0.02\n",
            "iteration: 100650 loss: 0.0018 lr: 0.02\n",
            "iteration: 100660 loss: 0.0020 lr: 0.02\n",
            "iteration: 100670 loss: 0.0017 lr: 0.02\n",
            "iteration: 100680 loss: 0.0019 lr: 0.02\n",
            "iteration: 100690 loss: 0.0021 lr: 0.02\n",
            "iteration: 100700 loss: 0.0023 lr: 0.02\n",
            "iteration: 100710 loss: 0.0019 lr: 0.02\n",
            "iteration: 100720 loss: 0.0017 lr: 0.02\n",
            "iteration: 100730 loss: 0.0015 lr: 0.02\n",
            "iteration: 100740 loss: 0.0016 lr: 0.02\n",
            "iteration: 100750 loss: 0.0020 lr: 0.02\n",
            "iteration: 100760 loss: 0.0018 lr: 0.02\n",
            "iteration: 100770 loss: 0.0016 lr: 0.02\n",
            "iteration: 100780 loss: 0.0022 lr: 0.02\n",
            "iteration: 100790 loss: 0.0021 lr: 0.02\n",
            "iteration: 100800 loss: 0.0017 lr: 0.02\n",
            "iteration: 100810 loss: 0.0016 lr: 0.02\n",
            "iteration: 100820 loss: 0.0016 lr: 0.02\n",
            "iteration: 100830 loss: 0.0018 lr: 0.02\n",
            "iteration: 100840 loss: 0.0020 lr: 0.02\n",
            "iteration: 100850 loss: 0.0016 lr: 0.02\n",
            "iteration: 100860 loss: 0.0019 lr: 0.02\n",
            "iteration: 100870 loss: 0.0016 lr: 0.02\n",
            "iteration: 100880 loss: 0.0016 lr: 0.02\n",
            "iteration: 100890 loss: 0.0015 lr: 0.02\n",
            "iteration: 100900 loss: 0.0020 lr: 0.02\n",
            "iteration: 100910 loss: 0.0013 lr: 0.02\n",
            "iteration: 100920 loss: 0.0014 lr: 0.02\n",
            "iteration: 100930 loss: 0.0024 lr: 0.02\n",
            "iteration: 100940 loss: 0.0015 lr: 0.02\n",
            "iteration: 100950 loss: 0.0022 lr: 0.02\n",
            "iteration: 100960 loss: 0.0017 lr: 0.02\n",
            "iteration: 100970 loss: 0.0017 lr: 0.02\n",
            "iteration: 100980 loss: 0.0017 lr: 0.02\n",
            "iteration: 100990 loss: 0.0019 lr: 0.02\n",
            "iteration: 101000 loss: 0.0018 lr: 0.02\n",
            "iteration: 101010 loss: 0.0023 lr: 0.02\n",
            "iteration: 101020 loss: 0.0017 lr: 0.02\n",
            "iteration: 101030 loss: 0.0017 lr: 0.02\n",
            "iteration: 101040 loss: 0.0019 lr: 0.02\n",
            "iteration: 101050 loss: 0.0017 lr: 0.02\n",
            "iteration: 101060 loss: 0.0015 lr: 0.02\n",
            "iteration: 101070 loss: 0.0015 lr: 0.02\n",
            "iteration: 101080 loss: 0.0014 lr: 0.02\n",
            "iteration: 101090 loss: 0.0014 lr: 0.02\n",
            "iteration: 101100 loss: 0.0017 lr: 0.02\n",
            "iteration: 101110 loss: 0.0019 lr: 0.02\n",
            "iteration: 101120 loss: 0.0022 lr: 0.02\n",
            "iteration: 101130 loss: 0.0016 lr: 0.02\n",
            "iteration: 101140 loss: 0.0025 lr: 0.02\n",
            "iteration: 101150 loss: 0.0017 lr: 0.02\n",
            "iteration: 101160 loss: 0.0022 lr: 0.02\n",
            "iteration: 101170 loss: 0.0017 lr: 0.02\n",
            "iteration: 101180 loss: 0.0029 lr: 0.02\n",
            "iteration: 101190 loss: 0.0018 lr: 0.02\n",
            "iteration: 101200 loss: 0.0017 lr: 0.02\n",
            "iteration: 101210 loss: 0.0014 lr: 0.02\n",
            "iteration: 101220 loss: 0.0021 lr: 0.02\n",
            "iteration: 101230 loss: 0.0014 lr: 0.02\n",
            "iteration: 101240 loss: 0.0016 lr: 0.02\n",
            "iteration: 101250 loss: 0.0018 lr: 0.02\n",
            "iteration: 101260 loss: 0.0013 lr: 0.02\n",
            "iteration: 101270 loss: 0.0014 lr: 0.02\n",
            "iteration: 101280 loss: 0.0019 lr: 0.02\n",
            "iteration: 101290 loss: 0.0018 lr: 0.02\n",
            "iteration: 101300 loss: 0.0018 lr: 0.02\n",
            "iteration: 101310 loss: 0.0020 lr: 0.02\n",
            "iteration: 101320 loss: 0.0020 lr: 0.02\n",
            "iteration: 101330 loss: 0.0018 lr: 0.02\n",
            "iteration: 101340 loss: 0.0019 lr: 0.02\n",
            "iteration: 101350 loss: 0.0018 lr: 0.02\n",
            "iteration: 101360 loss: 0.0022 lr: 0.02\n",
            "iteration: 101370 loss: 0.0019 lr: 0.02\n",
            "iteration: 101380 loss: 0.0015 lr: 0.02\n",
            "iteration: 101390 loss: 0.0021 lr: 0.02\n",
            "iteration: 101400 loss: 0.0017 lr: 0.02\n",
            "iteration: 101410 loss: 0.0020 lr: 0.02\n",
            "iteration: 101420 loss: 0.0018 lr: 0.02\n",
            "iteration: 101430 loss: 0.0013 lr: 0.02\n",
            "iteration: 101440 loss: 0.0015 lr: 0.02\n",
            "iteration: 101450 loss: 0.0021 lr: 0.02\n",
            "iteration: 101460 loss: 0.0022 lr: 0.02\n",
            "iteration: 101470 loss: 0.0019 lr: 0.02\n",
            "iteration: 101480 loss: 0.0021 lr: 0.02\n",
            "iteration: 101490 loss: 0.0022 lr: 0.02\n",
            "iteration: 101500 loss: 0.0014 lr: 0.02\n",
            "iteration: 101510 loss: 0.0016 lr: 0.02\n",
            "iteration: 101520 loss: 0.0018 lr: 0.02\n",
            "iteration: 101530 loss: 0.0018 lr: 0.02\n",
            "iteration: 101540 loss: 0.0027 lr: 0.02\n",
            "iteration: 101550 loss: 0.0021 lr: 0.02\n",
            "iteration: 101560 loss: 0.0018 lr: 0.02\n",
            "iteration: 101570 loss: 0.0019 lr: 0.02\n",
            "iteration: 101580 loss: 0.0015 lr: 0.02\n",
            "iteration: 101590 loss: 0.0018 lr: 0.02\n",
            "iteration: 101600 loss: 0.0011 lr: 0.02\n",
            "iteration: 101610 loss: 0.0012 lr: 0.02\n",
            "iteration: 101620 loss: 0.0015 lr: 0.02\n",
            "iteration: 101630 loss: 0.0017 lr: 0.02\n",
            "iteration: 101640 loss: 0.0019 lr: 0.02\n",
            "iteration: 101650 loss: 0.0016 lr: 0.02\n",
            "iteration: 101660 loss: 0.0023 lr: 0.02\n",
            "iteration: 101670 loss: 0.0021 lr: 0.02\n",
            "iteration: 101680 loss: 0.0016 lr: 0.02\n",
            "iteration: 101690 loss: 0.0024 lr: 0.02\n",
            "iteration: 101700 loss: 0.0016 lr: 0.02\n",
            "iteration: 101710 loss: 0.0014 lr: 0.02\n",
            "iteration: 101720 loss: 0.0018 lr: 0.02\n",
            "iteration: 101730 loss: 0.0016 lr: 0.02\n",
            "iteration: 101740 loss: 0.0017 lr: 0.02\n",
            "iteration: 101750 loss: 0.0020 lr: 0.02\n",
            "iteration: 101760 loss: 0.0019 lr: 0.02\n",
            "iteration: 101770 loss: 0.0015 lr: 0.02\n",
            "iteration: 101780 loss: 0.0019 lr: 0.02\n",
            "iteration: 101790 loss: 0.0019 lr: 0.02\n",
            "iteration: 101800 loss: 0.0017 lr: 0.02\n",
            "iteration: 101810 loss: 0.0022 lr: 0.02\n",
            "iteration: 101820 loss: 0.0013 lr: 0.02\n",
            "iteration: 101830 loss: 0.0017 lr: 0.02\n",
            "iteration: 101840 loss: 0.0016 lr: 0.02\n",
            "iteration: 101850 loss: 0.0019 lr: 0.02\n",
            "iteration: 101860 loss: 0.0019 lr: 0.02\n",
            "iteration: 101870 loss: 0.0018 lr: 0.02\n",
            "iteration: 101880 loss: 0.0023 lr: 0.02\n",
            "iteration: 101890 loss: 0.0017 lr: 0.02\n",
            "iteration: 101900 loss: 0.0017 lr: 0.02\n",
            "iteration: 101910 loss: 0.0016 lr: 0.02\n",
            "iteration: 101920 loss: 0.0015 lr: 0.02\n",
            "iteration: 101930 loss: 0.0023 lr: 0.02\n",
            "iteration: 101940 loss: 0.0017 lr: 0.02\n",
            "iteration: 101950 loss: 0.0017 lr: 0.02\n",
            "iteration: 101960 loss: 0.0020 lr: 0.02\n",
            "iteration: 101970 loss: 0.0016 lr: 0.02\n",
            "iteration: 101980 loss: 0.0015 lr: 0.02\n",
            "iteration: 101990 loss: 0.0017 lr: 0.02\n",
            "iteration: 102000 loss: 0.0020 lr: 0.02\n",
            "iteration: 102010 loss: 0.0021 lr: 0.02\n",
            "iteration: 102020 loss: 0.0018 lr: 0.02\n",
            "iteration: 102030 loss: 0.0017 lr: 0.02\n",
            "iteration: 102040 loss: 0.0017 lr: 0.02\n",
            "iteration: 102050 loss: 0.0021 lr: 0.02\n",
            "iteration: 102060 loss: 0.0015 lr: 0.02\n",
            "iteration: 102070 loss: 0.0013 lr: 0.02\n",
            "iteration: 102080 loss: 0.0016 lr: 0.02\n",
            "iteration: 102090 loss: 0.0018 lr: 0.02\n",
            "iteration: 102100 loss: 0.0016 lr: 0.02\n",
            "iteration: 102110 loss: 0.0015 lr: 0.02\n",
            "iteration: 102120 loss: 0.0020 lr: 0.02\n",
            "iteration: 102130 loss: 0.0018 lr: 0.02\n",
            "iteration: 102140 loss: 0.0019 lr: 0.02\n",
            "iteration: 102150 loss: 0.0015 lr: 0.02\n",
            "iteration: 102160 loss: 0.0014 lr: 0.02\n",
            "iteration: 102170 loss: 0.0018 lr: 0.02\n",
            "iteration: 102180 loss: 0.0015 lr: 0.02\n",
            "iteration: 102190 loss: 0.0022 lr: 0.02\n",
            "iteration: 102200 loss: 0.0018 lr: 0.02\n",
            "iteration: 102210 loss: 0.0017 lr: 0.02\n",
            "iteration: 102220 loss: 0.0022 lr: 0.02\n",
            "iteration: 102230 loss: 0.0016 lr: 0.02\n",
            "iteration: 102240 loss: 0.0017 lr: 0.02\n",
            "iteration: 102250 loss: 0.0023 lr: 0.02\n",
            "iteration: 102260 loss: 0.0016 lr: 0.02\n",
            "iteration: 102270 loss: 0.0021 lr: 0.02\n",
            "iteration: 102280 loss: 0.0021 lr: 0.02\n",
            "iteration: 102290 loss: 0.0018 lr: 0.02\n",
            "iteration: 102300 loss: 0.0021 lr: 0.02\n",
            "iteration: 102310 loss: 0.0016 lr: 0.02\n",
            "iteration: 102320 loss: 0.0019 lr: 0.02\n",
            "iteration: 102330 loss: 0.0017 lr: 0.02\n",
            "iteration: 102340 loss: 0.0019 lr: 0.02\n",
            "iteration: 102350 loss: 0.0016 lr: 0.02\n",
            "iteration: 102360 loss: 0.0016 lr: 0.02\n",
            "iteration: 102370 loss: 0.0017 lr: 0.02\n",
            "iteration: 102380 loss: 0.0019 lr: 0.02\n",
            "iteration: 102390 loss: 0.0020 lr: 0.02\n",
            "iteration: 102400 loss: 0.0016 lr: 0.02\n",
            "iteration: 102410 loss: 0.0021 lr: 0.02\n",
            "iteration: 102420 loss: 0.0021 lr: 0.02\n",
            "iteration: 102430 loss: 0.0021 lr: 0.02\n",
            "iteration: 102440 loss: 0.0021 lr: 0.02\n",
            "iteration: 102450 loss: 0.0016 lr: 0.02\n",
            "iteration: 102460 loss: 0.0014 lr: 0.02\n",
            "iteration: 102470 loss: 0.0016 lr: 0.02\n",
            "iteration: 102480 loss: 0.0014 lr: 0.02\n",
            "iteration: 102490 loss: 0.0016 lr: 0.02\n",
            "iteration: 102500 loss: 0.0017 lr: 0.02\n",
            "iteration: 102510 loss: 0.0019 lr: 0.02\n",
            "iteration: 102520 loss: 0.0019 lr: 0.02\n",
            "iteration: 102530 loss: 0.0015 lr: 0.02\n",
            "iteration: 102540 loss: 0.0018 lr: 0.02\n",
            "iteration: 102550 loss: 0.0020 lr: 0.02\n",
            "iteration: 102560 loss: 0.0017 lr: 0.02\n",
            "iteration: 102570 loss: 0.0015 lr: 0.02\n",
            "iteration: 102580 loss: 0.0013 lr: 0.02\n",
            "iteration: 102590 loss: 0.0015 lr: 0.02\n",
            "iteration: 102600 loss: 0.0015 lr: 0.02\n",
            "iteration: 102610 loss: 0.0019 lr: 0.02\n",
            "iteration: 102620 loss: 0.0019 lr: 0.02\n",
            "iteration: 102630 loss: 0.0020 lr: 0.02\n",
            "iteration: 102640 loss: 0.0021 lr: 0.02\n",
            "iteration: 102650 loss: 0.0015 lr: 0.02\n",
            "iteration: 102660 loss: 0.0012 lr: 0.02\n",
            "iteration: 102670 loss: 0.0015 lr: 0.02\n",
            "iteration: 102680 loss: 0.0014 lr: 0.02\n",
            "iteration: 102690 loss: 0.0013 lr: 0.02\n",
            "iteration: 102700 loss: 0.0021 lr: 0.02\n",
            "iteration: 102710 loss: 0.0018 lr: 0.02\n",
            "iteration: 102720 loss: 0.0020 lr: 0.02\n",
            "iteration: 102730 loss: 0.0022 lr: 0.02\n",
            "iteration: 102740 loss: 0.0015 lr: 0.02\n",
            "iteration: 102750 loss: 0.0014 lr: 0.02\n",
            "iteration: 102760 loss: 0.0016 lr: 0.02\n",
            "iteration: 102770 loss: 0.0016 lr: 0.02\n",
            "iteration: 102780 loss: 0.0022 lr: 0.02\n",
            "iteration: 102790 loss: 0.0018 lr: 0.02\n",
            "iteration: 102800 loss: 0.0021 lr: 0.02\n",
            "iteration: 102810 loss: 0.0018 lr: 0.02\n",
            "iteration: 102820 loss: 0.0015 lr: 0.02\n",
            "iteration: 102830 loss: 0.0016 lr: 0.02\n",
            "iteration: 102840 loss: 0.0020 lr: 0.02\n",
            "iteration: 102850 loss: 0.0016 lr: 0.02\n",
            "iteration: 102860 loss: 0.0018 lr: 0.02\n",
            "iteration: 102870 loss: 0.0014 lr: 0.02\n",
            "iteration: 102880 loss: 0.0014 lr: 0.02\n",
            "iteration: 102890 loss: 0.0019 lr: 0.02\n",
            "iteration: 102900 loss: 0.0016 lr: 0.02\n",
            "iteration: 102910 loss: 0.0017 lr: 0.02\n",
            "iteration: 102920 loss: 0.0019 lr: 0.02\n",
            "iteration: 102930 loss: 0.0017 lr: 0.02\n",
            "iteration: 102940 loss: 0.0013 lr: 0.02\n",
            "iteration: 102950 loss: 0.0018 lr: 0.02\n",
            "iteration: 102960 loss: 0.0017 lr: 0.02\n",
            "iteration: 102970 loss: 0.0025 lr: 0.02\n",
            "iteration: 102980 loss: 0.0019 lr: 0.02\n",
            "iteration: 102990 loss: 0.0015 lr: 0.02\n",
            "iteration: 103000 loss: 0.0016 lr: 0.02\n",
            "iteration: 103010 loss: 0.0013 lr: 0.02\n",
            "iteration: 103020 loss: 0.0021 lr: 0.02\n",
            "iteration: 103030 loss: 0.0017 lr: 0.02\n",
            "iteration: 103040 loss: 0.0017 lr: 0.02\n",
            "iteration: 103050 loss: 0.0020 lr: 0.02\n",
            "iteration: 103060 loss: 0.0015 lr: 0.02\n",
            "iteration: 103070 loss: 0.0016 lr: 0.02\n",
            "iteration: 103080 loss: 0.0020 lr: 0.02\n",
            "iteration: 103090 loss: 0.0018 lr: 0.02\n",
            "iteration: 103100 loss: 0.0016 lr: 0.02\n",
            "iteration: 103110 loss: 0.0016 lr: 0.02\n",
            "iteration: 103120 loss: 0.0019 lr: 0.02\n",
            "iteration: 103130 loss: 0.0017 lr: 0.02\n",
            "iteration: 103140 loss: 0.0018 lr: 0.02\n",
            "iteration: 103150 loss: 0.0019 lr: 0.02\n",
            "iteration: 103160 loss: 0.0014 lr: 0.02\n",
            "iteration: 103170 loss: 0.0019 lr: 0.02\n",
            "iteration: 103180 loss: 0.0017 lr: 0.02\n",
            "iteration: 103190 loss: 0.0016 lr: 0.02\n",
            "iteration: 103200 loss: 0.0022 lr: 0.02\n",
            "iteration: 103210 loss: 0.0017 lr: 0.02\n",
            "iteration: 103220 loss: 0.0023 lr: 0.02\n",
            "iteration: 103230 loss: 0.0015 lr: 0.02\n",
            "iteration: 103240 loss: 0.0022 lr: 0.02\n",
            "iteration: 103250 loss: 0.0019 lr: 0.02\n",
            "iteration: 103260 loss: 0.0012 lr: 0.02\n",
            "iteration: 103270 loss: 0.0020 lr: 0.02\n",
            "iteration: 103280 loss: 0.0016 lr: 0.02\n",
            "iteration: 103290 loss: 0.0018 lr: 0.02\n",
            "iteration: 103300 loss: 0.0017 lr: 0.02\n",
            "iteration: 103310 loss: 0.0015 lr: 0.02\n",
            "iteration: 103320 loss: 0.0013 lr: 0.02\n",
            "iteration: 103330 loss: 0.0016 lr: 0.02\n",
            "iteration: 103340 loss: 0.0013 lr: 0.02\n",
            "iteration: 103350 loss: 0.0014 lr: 0.02\n",
            "iteration: 103360 loss: 0.0016 lr: 0.02\n",
            "iteration: 103370 loss: 0.0017 lr: 0.02\n",
            "iteration: 103380 loss: 0.0015 lr: 0.02\n",
            "iteration: 103390 loss: 0.0014 lr: 0.02\n",
            "iteration: 103400 loss: 0.0019 lr: 0.02\n",
            "iteration: 103410 loss: 0.0022 lr: 0.02\n",
            "iteration: 103420 loss: 0.0021 lr: 0.02\n",
            "iteration: 103430 loss: 0.0017 lr: 0.02\n",
            "iteration: 103440 loss: 0.0022 lr: 0.02\n",
            "iteration: 103450 loss: 0.0019 lr: 0.02\n",
            "iteration: 103460 loss: 0.0013 lr: 0.02\n",
            "iteration: 103470 loss: 0.0019 lr: 0.02\n",
            "iteration: 103480 loss: 0.0016 lr: 0.02\n",
            "iteration: 103490 loss: 0.0015 lr: 0.02\n",
            "iteration: 103500 loss: 0.0018 lr: 0.02\n",
            "iteration: 103510 loss: 0.0019 lr: 0.02\n",
            "iteration: 103520 loss: 0.0020 lr: 0.02\n",
            "iteration: 103530 loss: 0.0019 lr: 0.02\n",
            "iteration: 103540 loss: 0.0017 lr: 0.02\n",
            "iteration: 103550 loss: 0.0021 lr: 0.02\n",
            "iteration: 103560 loss: 0.0021 lr: 0.02\n",
            "iteration: 103570 loss: 0.0017 lr: 0.02\n",
            "iteration: 103580 loss: 0.0019 lr: 0.02\n",
            "iteration: 103590 loss: 0.0022 lr: 0.02\n",
            "iteration: 103600 loss: 0.0022 lr: 0.02\n",
            "iteration: 103610 loss: 0.0018 lr: 0.02\n",
            "iteration: 103620 loss: 0.0018 lr: 0.02\n",
            "iteration: 103630 loss: 0.0021 lr: 0.02\n",
            "iteration: 103640 loss: 0.0017 lr: 0.02\n",
            "iteration: 103650 loss: 0.0020 lr: 0.02\n",
            "iteration: 103660 loss: 0.0015 lr: 0.02\n",
            "iteration: 103670 loss: 0.0017 lr: 0.02\n",
            "iteration: 103680 loss: 0.0020 lr: 0.02\n",
            "iteration: 103690 loss: 0.0018 lr: 0.02\n",
            "iteration: 103700 loss: 0.0018 lr: 0.02\n",
            "iteration: 103710 loss: 0.0016 lr: 0.02\n",
            "iteration: 103720 loss: 0.0019 lr: 0.02\n",
            "iteration: 103730 loss: 0.0021 lr: 0.02\n",
            "iteration: 103740 loss: 0.0017 lr: 0.02\n",
            "iteration: 103750 loss: 0.0018 lr: 0.02\n",
            "iteration: 103760 loss: 0.0017 lr: 0.02\n",
            "iteration: 103770 loss: 0.0022 lr: 0.02\n",
            "iteration: 103780 loss: 0.0018 lr: 0.02\n",
            "iteration: 103790 loss: 0.0021 lr: 0.02\n",
            "iteration: 103800 loss: 0.0018 lr: 0.02\n",
            "iteration: 103810 loss: 0.0015 lr: 0.02\n",
            "iteration: 103820 loss: 0.0016 lr: 0.02\n",
            "iteration: 103830 loss: 0.0015 lr: 0.02\n",
            "iteration: 103840 loss: 0.0016 lr: 0.02\n",
            "iteration: 103850 loss: 0.0019 lr: 0.02\n",
            "iteration: 103860 loss: 0.0015 lr: 0.02\n",
            "iteration: 103870 loss: 0.0016 lr: 0.02\n",
            "iteration: 103880 loss: 0.0016 lr: 0.02\n",
            "iteration: 103890 loss: 0.0015 lr: 0.02\n",
            "iteration: 103900 loss: 0.0014 lr: 0.02\n",
            "iteration: 103910 loss: 0.0016 lr: 0.02\n",
            "iteration: 103920 loss: 0.0016 lr: 0.02\n",
            "iteration: 103930 loss: 0.0016 lr: 0.02\n",
            "iteration: 103940 loss: 0.0017 lr: 0.02\n",
            "iteration: 103950 loss: 0.0014 lr: 0.02\n",
            "iteration: 103960 loss: 0.0016 lr: 0.02\n",
            "iteration: 103970 loss: 0.0019 lr: 0.02\n",
            "iteration: 103980 loss: 0.0018 lr: 0.02\n",
            "iteration: 103990 loss: 0.0024 lr: 0.02\n",
            "iteration: 104000 loss: 0.0018 lr: 0.02\n",
            "iteration: 104010 loss: 0.0019 lr: 0.02\n",
            "iteration: 104020 loss: 0.0017 lr: 0.02\n",
            "iteration: 104030 loss: 0.0018 lr: 0.02\n",
            "iteration: 104040 loss: 0.0016 lr: 0.02\n",
            "iteration: 104050 loss: 0.0016 lr: 0.02\n",
            "iteration: 104060 loss: 0.0015 lr: 0.02\n",
            "iteration: 104070 loss: 0.0017 lr: 0.02\n",
            "iteration: 104080 loss: 0.0013 lr: 0.02\n",
            "iteration: 104090 loss: 0.0019 lr: 0.02\n",
            "iteration: 104100 loss: 0.0016 lr: 0.02\n",
            "iteration: 104110 loss: 0.0016 lr: 0.02\n",
            "iteration: 104120 loss: 0.0018 lr: 0.02\n",
            "iteration: 104130 loss: 0.0018 lr: 0.02\n",
            "iteration: 104140 loss: 0.0017 lr: 0.02\n",
            "iteration: 104150 loss: 0.0017 lr: 0.02\n",
            "iteration: 104160 loss: 0.0017 lr: 0.02\n",
            "iteration: 104170 loss: 0.0016 lr: 0.02\n",
            "iteration: 104180 loss: 0.0023 lr: 0.02\n",
            "iteration: 104190 loss: 0.0020 lr: 0.02\n",
            "iteration: 104200 loss: 0.0017 lr: 0.02\n",
            "iteration: 104210 loss: 0.0014 lr: 0.02\n",
            "iteration: 104220 loss: 0.0018 lr: 0.02\n",
            "iteration: 104230 loss: 0.0022 lr: 0.02\n",
            "iteration: 104240 loss: 0.0016 lr: 0.02\n",
            "iteration: 104250 loss: 0.0016 lr: 0.02\n",
            "iteration: 104260 loss: 0.0015 lr: 0.02\n",
            "iteration: 104270 loss: 0.0025 lr: 0.02\n",
            "iteration: 104280 loss: 0.0013 lr: 0.02\n",
            "iteration: 104290 loss: 0.0014 lr: 0.02\n",
            "iteration: 104300 loss: 0.0027 lr: 0.02\n",
            "iteration: 104310 loss: 0.0019 lr: 0.02\n",
            "iteration: 104320 loss: 0.0019 lr: 0.02\n",
            "iteration: 104330 loss: 0.0019 lr: 0.02\n",
            "iteration: 104340 loss: 0.0017 lr: 0.02\n",
            "iteration: 104350 loss: 0.0022 lr: 0.02\n",
            "iteration: 104360 loss: 0.0015 lr: 0.02\n",
            "iteration: 104370 loss: 0.0015 lr: 0.02\n",
            "iteration: 104380 loss: 0.0015 lr: 0.02\n",
            "iteration: 104390 loss: 0.0018 lr: 0.02\n",
            "iteration: 104400 loss: 0.0022 lr: 0.02\n",
            "iteration: 104410 loss: 0.0017 lr: 0.02\n",
            "iteration: 104420 loss: 0.0015 lr: 0.02\n",
            "iteration: 104430 loss: 0.0020 lr: 0.02\n",
            "iteration: 104440 loss: 0.0017 lr: 0.02\n",
            "iteration: 104450 loss: 0.0018 lr: 0.02\n",
            "iteration: 104460 loss: 0.0017 lr: 0.02\n",
            "iteration: 104470 loss: 0.0030 lr: 0.02\n",
            "iteration: 104480 loss: 0.0019 lr: 0.02\n",
            "iteration: 104490 loss: 0.0015 lr: 0.02\n",
            "iteration: 104500 loss: 0.0014 lr: 0.02\n",
            "iteration: 104510 loss: 0.0019 lr: 0.02\n",
            "iteration: 104520 loss: 0.0016 lr: 0.02\n",
            "iteration: 104530 loss: 0.0015 lr: 0.02\n",
            "iteration: 104540 loss: 0.0019 lr: 0.02\n",
            "iteration: 104550 loss: 0.0017 lr: 0.02\n",
            "iteration: 104560 loss: 0.0012 lr: 0.02\n",
            "iteration: 104570 loss: 0.0024 lr: 0.02\n",
            "iteration: 104580 loss: 0.0021 lr: 0.02\n",
            "iteration: 104590 loss: 0.0014 lr: 0.02\n",
            "iteration: 104600 loss: 0.0016 lr: 0.02\n",
            "iteration: 104610 loss: 0.0015 lr: 0.02\n",
            "iteration: 104620 loss: 0.0020 lr: 0.02\n",
            "iteration: 104630 loss: 0.0015 lr: 0.02\n",
            "iteration: 104640 loss: 0.0019 lr: 0.02\n",
            "iteration: 104650 loss: 0.0020 lr: 0.02\n",
            "iteration: 104660 loss: 0.0018 lr: 0.02\n",
            "iteration: 104670 loss: 0.0016 lr: 0.02\n",
            "iteration: 104680 loss: 0.0015 lr: 0.02\n",
            "iteration: 104690 loss: 0.0015 lr: 0.02\n",
            "iteration: 104700 loss: 0.0021 lr: 0.02\n",
            "iteration: 104710 loss: 0.0016 lr: 0.02\n",
            "iteration: 104720 loss: 0.0018 lr: 0.02\n",
            "iteration: 104730 loss: 0.0015 lr: 0.02\n",
            "iteration: 104740 loss: 0.0018 lr: 0.02\n",
            "iteration: 104750 loss: 0.0016 lr: 0.02\n",
            "iteration: 104760 loss: 0.0016 lr: 0.02\n",
            "iteration: 104770 loss: 0.0018 lr: 0.02\n",
            "iteration: 104780 loss: 0.0022 lr: 0.02\n",
            "iteration: 104790 loss: 0.0021 lr: 0.02\n",
            "iteration: 104800 loss: 0.0023 lr: 0.02\n",
            "iteration: 104810 loss: 0.0024 lr: 0.02\n",
            "iteration: 104820 loss: 0.0022 lr: 0.02\n",
            "iteration: 104830 loss: 0.0021 lr: 0.02\n",
            "iteration: 104840 loss: 0.0017 lr: 0.02\n",
            "iteration: 104850 loss: 0.0016 lr: 0.02\n",
            "iteration: 104860 loss: 0.0019 lr: 0.02\n",
            "iteration: 104870 loss: 0.0028 lr: 0.02\n",
            "iteration: 104880 loss: 0.0018 lr: 0.02\n",
            "iteration: 104890 loss: 0.0018 lr: 0.02\n",
            "iteration: 104900 loss: 0.0018 lr: 0.02\n",
            "iteration: 104910 loss: 0.0017 lr: 0.02\n",
            "iteration: 104920 loss: 0.0018 lr: 0.02\n",
            "iteration: 104930 loss: 0.0015 lr: 0.02\n",
            "iteration: 104940 loss: 0.0017 lr: 0.02\n",
            "iteration: 104950 loss: 0.0020 lr: 0.02\n",
            "iteration: 104960 loss: 0.0019 lr: 0.02\n",
            "iteration: 104970 loss: 0.0018 lr: 0.02\n",
            "iteration: 104980 loss: 0.0015 lr: 0.02\n",
            "iteration: 104990 loss: 0.0018 lr: 0.02\n",
            "iteration: 105000 loss: 0.0019 lr: 0.02\n",
            "iteration: 105010 loss: 0.0018 lr: 0.02\n",
            "iteration: 105020 loss: 0.0013 lr: 0.02\n",
            "iteration: 105030 loss: 0.0017 lr: 0.02\n",
            "iteration: 105040 loss: 0.0016 lr: 0.02\n",
            "iteration: 105050 loss: 0.0020 lr: 0.02\n",
            "iteration: 105060 loss: 0.0018 lr: 0.02\n",
            "iteration: 105070 loss: 0.0014 lr: 0.02\n",
            "iteration: 105080 loss: 0.0018 lr: 0.02\n",
            "iteration: 105090 loss: 0.0017 lr: 0.02\n",
            "iteration: 105100 loss: 0.0020 lr: 0.02\n",
            "iteration: 105110 loss: 0.0017 lr: 0.02\n",
            "iteration: 105120 loss: 0.0018 lr: 0.02\n",
            "iteration: 105130 loss: 0.0016 lr: 0.02\n",
            "iteration: 105140 loss: 0.0024 lr: 0.02\n",
            "iteration: 105150 loss: 0.0018 lr: 0.02\n",
            "iteration: 105160 loss: 0.0017 lr: 0.02\n",
            "iteration: 105170 loss: 0.0015 lr: 0.02\n",
            "iteration: 105180 loss: 0.0013 lr: 0.02\n",
            "iteration: 105190 loss: 0.0016 lr: 0.02\n",
            "iteration: 105200 loss: 0.0020 lr: 0.02\n",
            "iteration: 105210 loss: 0.0019 lr: 0.02\n",
            "iteration: 105220 loss: 0.0016 lr: 0.02\n",
            "iteration: 105230 loss: 0.0017 lr: 0.02\n",
            "iteration: 105240 loss: 0.0023 lr: 0.02\n",
            "iteration: 105250 loss: 0.0017 lr: 0.02\n",
            "iteration: 105260 loss: 0.0022 lr: 0.02\n",
            "iteration: 105270 loss: 0.0014 lr: 0.02\n",
            "iteration: 105280 loss: 0.0015 lr: 0.02\n",
            "iteration: 105290 loss: 0.0016 lr: 0.02\n",
            "iteration: 105300 loss: 0.0017 lr: 0.02\n",
            "iteration: 105310 loss: 0.0022 lr: 0.02\n",
            "iteration: 105320 loss: 0.0018 lr: 0.02\n",
            "iteration: 105330 loss: 0.0017 lr: 0.02\n",
            "iteration: 105340 loss: 0.0019 lr: 0.02\n",
            "iteration: 105350 loss: 0.0013 lr: 0.02\n",
            "iteration: 105360 loss: 0.0018 lr: 0.02\n",
            "iteration: 105370 loss: 0.0013 lr: 0.02\n",
            "iteration: 105380 loss: 0.0019 lr: 0.02\n",
            "iteration: 105390 loss: 0.0020 lr: 0.02\n",
            "iteration: 105400 loss: 0.0017 lr: 0.02\n",
            "iteration: 105410 loss: 0.0018 lr: 0.02\n",
            "iteration: 105420 loss: 0.0017 lr: 0.02\n",
            "iteration: 105430 loss: 0.0020 lr: 0.02\n",
            "iteration: 105440 loss: 0.0017 lr: 0.02\n",
            "iteration: 105450 loss: 0.0017 lr: 0.02\n",
            "iteration: 105460 loss: 0.0017 lr: 0.02\n",
            "iteration: 105470 loss: 0.0015 lr: 0.02\n",
            "iteration: 105480 loss: 0.0016 lr: 0.02\n",
            "iteration: 105490 loss: 0.0016 lr: 0.02\n",
            "iteration: 105500 loss: 0.0020 lr: 0.02\n",
            "iteration: 105510 loss: 0.0019 lr: 0.02\n",
            "iteration: 105520 loss: 0.0018 lr: 0.02\n",
            "iteration: 105530 loss: 0.0022 lr: 0.02\n",
            "iteration: 105540 loss: 0.0017 lr: 0.02\n",
            "iteration: 105550 loss: 0.0013 lr: 0.02\n",
            "iteration: 105560 loss: 0.0018 lr: 0.02\n",
            "iteration: 105570 loss: 0.0017 lr: 0.02\n",
            "iteration: 105580 loss: 0.0015 lr: 0.02\n",
            "iteration: 105590 loss: 0.0014 lr: 0.02\n",
            "iteration: 105600 loss: 0.0012 lr: 0.02\n",
            "iteration: 105610 loss: 0.0015 lr: 0.02\n",
            "iteration: 105620 loss: 0.0015 lr: 0.02\n",
            "iteration: 105630 loss: 0.0016 lr: 0.02\n",
            "iteration: 105640 loss: 0.0019 lr: 0.02\n",
            "iteration: 105650 loss: 0.0017 lr: 0.02\n",
            "iteration: 105660 loss: 0.0022 lr: 0.02\n",
            "iteration: 105670 loss: 0.0021 lr: 0.02\n",
            "iteration: 105680 loss: 0.0016 lr: 0.02\n",
            "iteration: 105690 loss: 0.0016 lr: 0.02\n",
            "iteration: 105700 loss: 0.0020 lr: 0.02\n",
            "iteration: 105710 loss: 0.0017 lr: 0.02\n",
            "iteration: 105720 loss: 0.0021 lr: 0.02\n",
            "iteration: 105730 loss: 0.0020 lr: 0.02\n",
            "iteration: 105740 loss: 0.0017 lr: 0.02\n",
            "iteration: 105750 loss: 0.0017 lr: 0.02\n",
            "iteration: 105760 loss: 0.0013 lr: 0.02\n",
            "iteration: 105770 loss: 0.0016 lr: 0.02\n",
            "iteration: 105780 loss: 0.0013 lr: 0.02\n",
            "iteration: 105790 loss: 0.0022 lr: 0.02\n",
            "iteration: 105800 loss: 0.0022 lr: 0.02\n",
            "iteration: 105810 loss: 0.0013 lr: 0.02\n",
            "iteration: 105820 loss: 0.0017 lr: 0.02\n",
            "iteration: 105830 loss: 0.0011 lr: 0.02\n",
            "iteration: 105840 loss: 0.0020 lr: 0.02\n",
            "iteration: 105850 loss: 0.0022 lr: 0.02\n",
            "iteration: 105860 loss: 0.0015 lr: 0.02\n",
            "iteration: 105870 loss: 0.0017 lr: 0.02\n",
            "iteration: 105880 loss: 0.0020 lr: 0.02\n",
            "iteration: 105890 loss: 0.0015 lr: 0.02\n",
            "iteration: 105900 loss: 0.0014 lr: 0.02\n",
            "iteration: 105910 loss: 0.0021 lr: 0.02\n",
            "iteration: 105920 loss: 0.0017 lr: 0.02\n",
            "iteration: 105930 loss: 0.0017 lr: 0.02\n",
            "iteration: 105940 loss: 0.0016 lr: 0.02\n",
            "iteration: 105950 loss: 0.0015 lr: 0.02\n",
            "iteration: 105960 loss: 0.0018 lr: 0.02\n",
            "iteration: 105970 loss: 0.0015 lr: 0.02\n",
            "iteration: 105980 loss: 0.0017 lr: 0.02\n",
            "iteration: 105990 loss: 0.0016 lr: 0.02\n",
            "iteration: 106000 loss: 0.0019 lr: 0.02\n",
            "iteration: 106010 loss: 0.0018 lr: 0.02\n",
            "iteration: 106020 loss: 0.0017 lr: 0.02\n",
            "iteration: 106030 loss: 0.0014 lr: 0.02\n",
            "iteration: 106040 loss: 0.0016 lr: 0.02\n",
            "iteration: 106050 loss: 0.0026 lr: 0.02\n",
            "iteration: 106060 loss: 0.0015 lr: 0.02\n",
            "iteration: 106070 loss: 0.0017 lr: 0.02\n",
            "iteration: 106080 loss: 0.0019 lr: 0.02\n",
            "iteration: 106090 loss: 0.0012 lr: 0.02\n",
            "iteration: 106100 loss: 0.0019 lr: 0.02\n",
            "iteration: 106110 loss: 0.0020 lr: 0.02\n",
            "iteration: 106120 loss: 0.0015 lr: 0.02\n",
            "iteration: 106130 loss: 0.0014 lr: 0.02\n",
            "iteration: 106140 loss: 0.0016 lr: 0.02\n",
            "iteration: 106150 loss: 0.0017 lr: 0.02\n",
            "iteration: 106160 loss: 0.0017 lr: 0.02\n",
            "iteration: 106170 loss: 0.0020 lr: 0.02\n",
            "iteration: 106180 loss: 0.0014 lr: 0.02\n",
            "iteration: 106190 loss: 0.0030 lr: 0.02\n",
            "iteration: 106200 loss: 0.0017 lr: 0.02\n",
            "iteration: 106210 loss: 0.0017 lr: 0.02\n",
            "iteration: 106220 loss: 0.0019 lr: 0.02\n",
            "iteration: 106230 loss: 0.0016 lr: 0.02\n",
            "iteration: 106240 loss: 0.0015 lr: 0.02\n",
            "iteration: 106250 loss: 0.0020 lr: 0.02\n",
            "iteration: 106260 loss: 0.0019 lr: 0.02\n",
            "iteration: 106270 loss: 0.0016 lr: 0.02\n",
            "iteration: 106280 loss: 0.0016 lr: 0.02\n",
            "iteration: 106290 loss: 0.0020 lr: 0.02\n",
            "iteration: 106300 loss: 0.0020 lr: 0.02\n",
            "iteration: 106310 loss: 0.0021 lr: 0.02\n",
            "iteration: 106320 loss: 0.0016 lr: 0.02\n",
            "iteration: 106330 loss: 0.0020 lr: 0.02\n",
            "iteration: 106340 loss: 0.0022 lr: 0.02\n",
            "iteration: 106350 loss: 0.0019 lr: 0.02\n",
            "iteration: 106360 loss: 0.0021 lr: 0.02\n",
            "iteration: 106370 loss: 0.0012 lr: 0.02\n",
            "iteration: 106380 loss: 0.0016 lr: 0.02\n",
            "iteration: 106390 loss: 0.0017 lr: 0.02\n",
            "iteration: 106400 loss: 0.0015 lr: 0.02\n",
            "iteration: 106410 loss: 0.0021 lr: 0.02\n",
            "iteration: 106420 loss: 0.0017 lr: 0.02\n",
            "iteration: 106430 loss: 0.0019 lr: 0.02\n",
            "iteration: 106440 loss: 0.0017 lr: 0.02\n",
            "iteration: 106450 loss: 0.0015 lr: 0.02\n",
            "iteration: 106460 loss: 0.0015 lr: 0.02\n",
            "iteration: 106470 loss: 0.0015 lr: 0.02\n",
            "iteration: 106480 loss: 0.0025 lr: 0.02\n",
            "iteration: 106490 loss: 0.0013 lr: 0.02\n",
            "iteration: 106500 loss: 0.0016 lr: 0.02\n",
            "iteration: 106510 loss: 0.0014 lr: 0.02\n",
            "iteration: 106520 loss: 0.0018 lr: 0.02\n",
            "iteration: 106530 loss: 0.0019 lr: 0.02\n",
            "iteration: 106540 loss: 0.0018 lr: 0.02\n",
            "iteration: 106550 loss: 0.0017 lr: 0.02\n",
            "iteration: 106560 loss: 0.0027 lr: 0.02\n",
            "iteration: 106570 loss: 0.0020 lr: 0.02\n",
            "iteration: 106580 loss: 0.0014 lr: 0.02\n",
            "iteration: 106590 loss: 0.0024 lr: 0.02\n",
            "iteration: 106600 loss: 0.0016 lr: 0.02\n",
            "iteration: 106610 loss: 0.0014 lr: 0.02\n",
            "iteration: 106620 loss: 0.0015 lr: 0.02\n",
            "iteration: 106630 loss: 0.0018 lr: 0.02\n",
            "iteration: 106640 loss: 0.0016 lr: 0.02\n",
            "iteration: 106650 loss: 0.0018 lr: 0.02\n",
            "iteration: 106660 loss: 0.0016 lr: 0.02\n",
            "iteration: 106670 loss: 0.0015 lr: 0.02\n",
            "iteration: 106680 loss: 0.0016 lr: 0.02\n",
            "iteration: 106690 loss: 0.0015 lr: 0.02\n",
            "iteration: 106700 loss: 0.0016 lr: 0.02\n",
            "iteration: 106710 loss: 0.0016 lr: 0.02\n",
            "iteration: 106720 loss: 0.0017 lr: 0.02\n",
            "iteration: 106730 loss: 0.0020 lr: 0.02\n",
            "iteration: 106740 loss: 0.0018 lr: 0.02\n",
            "iteration: 106750 loss: 0.0012 lr: 0.02\n",
            "iteration: 106760 loss: 0.0014 lr: 0.02\n",
            "iteration: 106770 loss: 0.0016 lr: 0.02\n",
            "iteration: 106780 loss: 0.0016 lr: 0.02\n",
            "iteration: 106790 loss: 0.0015 lr: 0.02\n",
            "iteration: 106800 loss: 0.0016 lr: 0.02\n",
            "iteration: 106810 loss: 0.0018 lr: 0.02\n",
            "iteration: 106820 loss: 0.0016 lr: 0.02\n",
            "iteration: 106830 loss: 0.0018 lr: 0.02\n",
            "iteration: 106840 loss: 0.0015 lr: 0.02\n",
            "iteration: 106850 loss: 0.0015 lr: 0.02\n",
            "iteration: 106860 loss: 0.0020 lr: 0.02\n",
            "iteration: 106870 loss: 0.0015 lr: 0.02\n",
            "iteration: 106880 loss: 0.0019 lr: 0.02\n",
            "iteration: 106890 loss: 0.0016 lr: 0.02\n",
            "iteration: 106900 loss: 0.0015 lr: 0.02\n",
            "iteration: 106910 loss: 0.0019 lr: 0.02\n",
            "iteration: 106920 loss: 0.0018 lr: 0.02\n",
            "iteration: 106930 loss: 0.0015 lr: 0.02\n",
            "iteration: 106940 loss: 0.0018 lr: 0.02\n",
            "iteration: 106950 loss: 0.0017 lr: 0.02\n",
            "iteration: 106960 loss: 0.0017 lr: 0.02\n",
            "iteration: 106970 loss: 0.0019 lr: 0.02\n",
            "iteration: 106980 loss: 0.0019 lr: 0.02\n",
            "iteration: 106990 loss: 0.0015 lr: 0.02\n",
            "iteration: 107000 loss: 0.0021 lr: 0.02\n",
            "iteration: 107010 loss: 0.0013 lr: 0.02\n",
            "iteration: 107020 loss: 0.0015 lr: 0.02\n",
            "iteration: 107030 loss: 0.0015 lr: 0.02\n",
            "iteration: 107040 loss: 0.0023 lr: 0.02\n",
            "iteration: 107050 loss: 0.0020 lr: 0.02\n",
            "iteration: 107060 loss: 0.0018 lr: 0.02\n",
            "iteration: 107070 loss: 0.0016 lr: 0.02\n",
            "iteration: 107080 loss: 0.0023 lr: 0.02\n",
            "iteration: 107090 loss: 0.0016 lr: 0.02\n",
            "iteration: 107100 loss: 0.0017 lr: 0.02\n",
            "iteration: 107110 loss: 0.0016 lr: 0.02\n",
            "iteration: 107120 loss: 0.0014 lr: 0.02\n",
            "iteration: 107130 loss: 0.0020 lr: 0.02\n",
            "iteration: 107140 loss: 0.0016 lr: 0.02\n",
            "iteration: 107150 loss: 0.0017 lr: 0.02\n",
            "iteration: 107160 loss: 0.0013 lr: 0.02\n",
            "iteration: 107170 loss: 0.0017 lr: 0.02\n",
            "iteration: 107180 loss: 0.0013 lr: 0.02\n",
            "iteration: 107190 loss: 0.0016 lr: 0.02\n",
            "iteration: 107200 loss: 0.0015 lr: 0.02\n",
            "iteration: 107210 loss: 0.0016 lr: 0.02\n",
            "iteration: 107220 loss: 0.0017 lr: 0.02\n",
            "iteration: 107230 loss: 0.0023 lr: 0.02\n",
            "iteration: 107240 loss: 0.0014 lr: 0.02\n",
            "iteration: 107250 loss: 0.0015 lr: 0.02\n",
            "iteration: 107260 loss: 0.0017 lr: 0.02\n",
            "iteration: 107270 loss: 0.0021 lr: 0.02\n",
            "iteration: 107280 loss: 0.0016 lr: 0.02\n",
            "iteration: 107290 loss: 0.0018 lr: 0.02\n",
            "iteration: 107300 loss: 0.0017 lr: 0.02\n",
            "iteration: 107310 loss: 0.0019 lr: 0.02\n",
            "iteration: 107320 loss: 0.0017 lr: 0.02\n",
            "iteration: 107330 loss: 0.0022 lr: 0.02\n",
            "iteration: 107340 loss: 0.0018 lr: 0.02\n",
            "iteration: 107350 loss: 0.0018 lr: 0.02\n",
            "iteration: 107360 loss: 0.0016 lr: 0.02\n",
            "iteration: 107370 loss: 0.0022 lr: 0.02\n",
            "iteration: 107380 loss: 0.0017 lr: 0.02\n",
            "iteration: 107390 loss: 0.0016 lr: 0.02\n",
            "iteration: 107400 loss: 0.0022 lr: 0.02\n",
            "iteration: 107410 loss: 0.0014 lr: 0.02\n",
            "iteration: 107420 loss: 0.0016 lr: 0.02\n",
            "iteration: 107430 loss: 0.0021 lr: 0.02\n",
            "iteration: 107440 loss: 0.0013 lr: 0.02\n",
            "iteration: 107450 loss: 0.0017 lr: 0.02\n",
            "iteration: 107460 loss: 0.0023 lr: 0.02\n",
            "iteration: 107470 loss: 0.0013 lr: 0.02\n",
            "iteration: 107480 loss: 0.0017 lr: 0.02\n",
            "iteration: 107490 loss: 0.0017 lr: 0.02\n",
            "iteration: 107500 loss: 0.0014 lr: 0.02\n",
            "iteration: 107510 loss: 0.0017 lr: 0.02\n",
            "iteration: 107520 loss: 0.0014 lr: 0.02\n",
            "iteration: 107530 loss: 0.0016 lr: 0.02\n",
            "iteration: 107540 loss: 0.0018 lr: 0.02\n",
            "iteration: 107550 loss: 0.0014 lr: 0.02\n",
            "iteration: 107560 loss: 0.0018 lr: 0.02\n",
            "iteration: 107570 loss: 0.0016 lr: 0.02\n",
            "iteration: 107580 loss: 0.0015 lr: 0.02\n",
            "iteration: 107590 loss: 0.0023 lr: 0.02\n",
            "iteration: 107600 loss: 0.0017 lr: 0.02\n",
            "iteration: 107610 loss: 0.0025 lr: 0.02\n",
            "iteration: 107620 loss: 0.0019 lr: 0.02\n",
            "iteration: 107630 loss: 0.0016 lr: 0.02\n",
            "iteration: 107640 loss: 0.0015 lr: 0.02\n",
            "iteration: 107650 loss: 0.0026 lr: 0.02\n",
            "iteration: 107660 loss: 0.0017 lr: 0.02\n",
            "iteration: 107670 loss: 0.0017 lr: 0.02\n",
            "iteration: 107680 loss: 0.0014 lr: 0.02\n",
            "iteration: 107690 loss: 0.0017 lr: 0.02\n",
            "iteration: 107700 loss: 0.0019 lr: 0.02\n",
            "iteration: 107710 loss: 0.0018 lr: 0.02\n",
            "iteration: 107720 loss: 0.0016 lr: 0.02\n",
            "iteration: 107730 loss: 0.0018 lr: 0.02\n",
            "iteration: 107740 loss: 0.0013 lr: 0.02\n",
            "iteration: 107750 loss: 0.0019 lr: 0.02\n",
            "iteration: 107760 loss: 0.0023 lr: 0.02\n",
            "iteration: 107770 loss: 0.0018 lr: 0.02\n",
            "iteration: 107780 loss: 0.0018 lr: 0.02\n",
            "iteration: 107790 loss: 0.0016 lr: 0.02\n",
            "iteration: 107800 loss: 0.0026 lr: 0.02\n",
            "iteration: 107810 loss: 0.0017 lr: 0.02\n",
            "iteration: 107820 loss: 0.0017 lr: 0.02\n",
            "iteration: 107830 loss: 0.0019 lr: 0.02\n",
            "iteration: 107840 loss: 0.0021 lr: 0.02\n",
            "iteration: 107850 loss: 0.0014 lr: 0.02\n",
            "iteration: 107860 loss: 0.0014 lr: 0.02\n",
            "iteration: 107870 loss: 0.0020 lr: 0.02\n",
            "iteration: 107880 loss: 0.0019 lr: 0.02\n",
            "iteration: 107890 loss: 0.0023 lr: 0.02\n",
            "iteration: 107900 loss: 0.0022 lr: 0.02\n",
            "iteration: 107910 loss: 0.0019 lr: 0.02\n",
            "iteration: 107920 loss: 0.0017 lr: 0.02\n",
            "iteration: 107930 loss: 0.0015 lr: 0.02\n",
            "iteration: 107940 loss: 0.0016 lr: 0.02\n",
            "iteration: 107950 loss: 0.0019 lr: 0.02\n",
            "iteration: 107960 loss: 0.0018 lr: 0.02\n",
            "iteration: 107970 loss: 0.0017 lr: 0.02\n",
            "iteration: 107980 loss: 0.0016 lr: 0.02\n",
            "iteration: 107990 loss: 0.0014 lr: 0.02\n",
            "iteration: 108000 loss: 0.0016 lr: 0.02\n",
            "iteration: 108010 loss: 0.0017 lr: 0.02\n",
            "iteration: 108020 loss: 0.0018 lr: 0.02\n",
            "iteration: 108030 loss: 0.0015 lr: 0.02\n",
            "iteration: 108040 loss: 0.0020 lr: 0.02\n",
            "iteration: 108050 loss: 0.0015 lr: 0.02\n",
            "iteration: 108060 loss: 0.0018 lr: 0.02\n",
            "iteration: 108070 loss: 0.0015 lr: 0.02\n",
            "iteration: 108080 loss: 0.0021 lr: 0.02\n",
            "iteration: 108090 loss: 0.0019 lr: 0.02\n",
            "iteration: 108100 loss: 0.0016 lr: 0.02\n",
            "iteration: 108110 loss: 0.0018 lr: 0.02\n",
            "iteration: 108120 loss: 0.0013 lr: 0.02\n",
            "iteration: 108130 loss: 0.0022 lr: 0.02\n",
            "iteration: 108140 loss: 0.0016 lr: 0.02\n",
            "iteration: 108150 loss: 0.0018 lr: 0.02\n",
            "iteration: 108160 loss: 0.0017 lr: 0.02\n",
            "iteration: 108170 loss: 0.0024 lr: 0.02\n",
            "iteration: 108180 loss: 0.0017 lr: 0.02\n",
            "iteration: 108190 loss: 0.0015 lr: 0.02\n",
            "iteration: 108200 loss: 0.0021 lr: 0.02\n",
            "iteration: 108210 loss: 0.0017 lr: 0.02\n",
            "iteration: 108220 loss: 0.0015 lr: 0.02\n",
            "iteration: 108230 loss: 0.0020 lr: 0.02\n",
            "iteration: 108240 loss: 0.0013 lr: 0.02\n",
            "iteration: 108250 loss: 0.0018 lr: 0.02\n",
            "iteration: 108260 loss: 0.0015 lr: 0.02\n",
            "iteration: 108270 loss: 0.0016 lr: 0.02\n",
            "iteration: 108280 loss: 0.0017 lr: 0.02\n",
            "iteration: 108290 loss: 0.0019 lr: 0.02\n",
            "iteration: 108300 loss: 0.0016 lr: 0.02\n",
            "iteration: 108310 loss: 0.0023 lr: 0.02\n",
            "iteration: 108320 loss: 0.0025 lr: 0.02\n",
            "iteration: 108330 loss: 0.0013 lr: 0.02\n",
            "iteration: 108340 loss: 0.0018 lr: 0.02\n",
            "iteration: 108350 loss: 0.0023 lr: 0.02\n",
            "iteration: 108360 loss: 0.0016 lr: 0.02\n",
            "iteration: 108370 loss: 0.0017 lr: 0.02\n",
            "iteration: 108380 loss: 0.0018 lr: 0.02\n",
            "iteration: 108390 loss: 0.0017 lr: 0.02\n",
            "iteration: 108400 loss: 0.0013 lr: 0.02\n",
            "iteration: 108410 loss: 0.0023 lr: 0.02\n",
            "iteration: 108420 loss: 0.0018 lr: 0.02\n",
            "iteration: 108430 loss: 0.0017 lr: 0.02\n",
            "iteration: 108440 loss: 0.0014 lr: 0.02\n",
            "iteration: 108450 loss: 0.0017 lr: 0.02\n",
            "iteration: 108460 loss: 0.0019 lr: 0.02\n",
            "iteration: 108470 loss: 0.0017 lr: 0.02\n",
            "iteration: 108480 loss: 0.0018 lr: 0.02\n",
            "iteration: 108490 loss: 0.0020 lr: 0.02\n",
            "iteration: 108500 loss: 0.0016 lr: 0.02\n",
            "iteration: 108510 loss: 0.0015 lr: 0.02\n",
            "iteration: 108520 loss: 0.0018 lr: 0.02\n",
            "iteration: 108530 loss: 0.0017 lr: 0.02\n",
            "iteration: 108540 loss: 0.0013 lr: 0.02\n",
            "iteration: 108550 loss: 0.0015 lr: 0.02\n",
            "iteration: 108560 loss: 0.0017 lr: 0.02\n",
            "iteration: 108570 loss: 0.0016 lr: 0.02\n",
            "iteration: 108580 loss: 0.0017 lr: 0.02\n",
            "iteration: 108590 loss: 0.0021 lr: 0.02\n",
            "iteration: 108600 loss: 0.0013 lr: 0.02\n",
            "iteration: 108610 loss: 0.0017 lr: 0.02\n",
            "iteration: 108620 loss: 0.0017 lr: 0.02\n",
            "iteration: 108630 loss: 0.0018 lr: 0.02\n",
            "iteration: 108640 loss: 0.0016 lr: 0.02\n",
            "iteration: 108650 loss: 0.0017 lr: 0.02\n",
            "iteration: 108660 loss: 0.0021 lr: 0.02\n",
            "iteration: 108670 loss: 0.0016 lr: 0.02\n",
            "iteration: 108680 loss: 0.0015 lr: 0.02\n",
            "iteration: 108690 loss: 0.0015 lr: 0.02\n",
            "iteration: 108700 loss: 0.0017 lr: 0.02\n",
            "iteration: 108710 loss: 0.0021 lr: 0.02\n",
            "iteration: 108720 loss: 0.0021 lr: 0.02\n",
            "iteration: 108730 loss: 0.0013 lr: 0.02\n",
            "iteration: 108740 loss: 0.0019 lr: 0.02\n",
            "iteration: 108750 loss: 0.0018 lr: 0.02\n",
            "iteration: 108760 loss: 0.0018 lr: 0.02\n",
            "iteration: 108770 loss: 0.0017 lr: 0.02\n",
            "iteration: 108780 loss: 0.0020 lr: 0.02\n",
            "iteration: 108790 loss: 0.0018 lr: 0.02\n",
            "iteration: 108800 loss: 0.0025 lr: 0.02\n",
            "iteration: 108810 loss: 0.0018 lr: 0.02\n",
            "iteration: 108820 loss: 0.0019 lr: 0.02\n",
            "iteration: 108830 loss: 0.0019 lr: 0.02\n",
            "iteration: 108840 loss: 0.0016 lr: 0.02\n",
            "iteration: 108850 loss: 0.0021 lr: 0.02\n",
            "iteration: 108860 loss: 0.0015 lr: 0.02\n",
            "iteration: 108870 loss: 0.0019 lr: 0.02\n",
            "iteration: 108880 loss: 0.0017 lr: 0.02\n",
            "iteration: 108890 loss: 0.0015 lr: 0.02\n",
            "iteration: 108900 loss: 0.0018 lr: 0.02\n",
            "iteration: 108910 loss: 0.0017 lr: 0.02\n",
            "iteration: 108920 loss: 0.0016 lr: 0.02\n",
            "iteration: 108930 loss: 0.0017 lr: 0.02\n",
            "iteration: 108940 loss: 0.0018 lr: 0.02\n",
            "iteration: 108950 loss: 0.0015 lr: 0.02\n",
            "iteration: 108960 loss: 0.0018 lr: 0.02\n",
            "iteration: 108970 loss: 0.0018 lr: 0.02\n",
            "iteration: 108980 loss: 0.0018 lr: 0.02\n",
            "iteration: 108990 loss: 0.0019 lr: 0.02\n",
            "iteration: 109000 loss: 0.0021 lr: 0.02\n",
            "iteration: 109010 loss: 0.0020 lr: 0.02\n",
            "iteration: 109020 loss: 0.0019 lr: 0.02\n",
            "iteration: 109030 loss: 0.0016 lr: 0.02\n",
            "iteration: 109040 loss: 0.0020 lr: 0.02\n",
            "iteration: 109050 loss: 0.0019 lr: 0.02\n",
            "iteration: 109060 loss: 0.0018 lr: 0.02\n",
            "iteration: 109070 loss: 0.0015 lr: 0.02\n",
            "iteration: 109080 loss: 0.0019 lr: 0.02\n",
            "iteration: 109090 loss: 0.0018 lr: 0.02\n",
            "iteration: 109100 loss: 0.0020 lr: 0.02\n",
            "iteration: 109110 loss: 0.0017 lr: 0.02\n",
            "iteration: 109120 loss: 0.0017 lr: 0.02\n",
            "iteration: 109130 loss: 0.0021 lr: 0.02\n",
            "iteration: 109140 loss: 0.0013 lr: 0.02\n",
            "iteration: 109150 loss: 0.0015 lr: 0.02\n",
            "iteration: 109160 loss: 0.0022 lr: 0.02\n",
            "iteration: 109170 loss: 0.0018 lr: 0.02\n",
            "iteration: 109180 loss: 0.0016 lr: 0.02\n",
            "iteration: 109190 loss: 0.0020 lr: 0.02\n",
            "iteration: 109200 loss: 0.0016 lr: 0.02\n",
            "iteration: 109210 loss: 0.0015 lr: 0.02\n",
            "iteration: 109220 loss: 0.0019 lr: 0.02\n",
            "iteration: 109230 loss: 0.0017 lr: 0.02\n",
            "iteration: 109240 loss: 0.0019 lr: 0.02\n",
            "iteration: 109250 loss: 0.0024 lr: 0.02\n",
            "iteration: 109260 loss: 0.0016 lr: 0.02\n",
            "iteration: 109270 loss: 0.0018 lr: 0.02\n",
            "iteration: 109280 loss: 0.0015 lr: 0.02\n",
            "iteration: 109290 loss: 0.0013 lr: 0.02\n",
            "iteration: 109300 loss: 0.0017 lr: 0.02\n",
            "iteration: 109310 loss: 0.0018 lr: 0.02\n",
            "iteration: 109320 loss: 0.0016 lr: 0.02\n",
            "iteration: 109330 loss: 0.0017 lr: 0.02\n",
            "iteration: 109340 loss: 0.0016 lr: 0.02\n",
            "iteration: 109350 loss: 0.0014 lr: 0.02\n",
            "iteration: 109360 loss: 0.0021 lr: 0.02\n",
            "iteration: 109370 loss: 0.0021 lr: 0.02\n",
            "iteration: 109380 loss: 0.0016 lr: 0.02\n",
            "iteration: 109390 loss: 0.0019 lr: 0.02\n",
            "iteration: 109400 loss: 0.0016 lr: 0.02\n",
            "iteration: 109410 loss: 0.0014 lr: 0.02\n",
            "iteration: 109420 loss: 0.0016 lr: 0.02\n",
            "iteration: 109430 loss: 0.0016 lr: 0.02\n",
            "iteration: 109440 loss: 0.0020 lr: 0.02\n",
            "iteration: 109450 loss: 0.0018 lr: 0.02\n",
            "iteration: 109460 loss: 0.0015 lr: 0.02\n",
            "iteration: 109470 loss: 0.0012 lr: 0.02\n",
            "iteration: 109480 loss: 0.0017 lr: 0.02\n",
            "iteration: 109490 loss: 0.0015 lr: 0.02\n",
            "iteration: 109500 loss: 0.0019 lr: 0.02\n",
            "iteration: 109510 loss: 0.0022 lr: 0.02\n",
            "iteration: 109520 loss: 0.0016 lr: 0.02\n",
            "iteration: 109530 loss: 0.0014 lr: 0.02\n",
            "iteration: 109540 loss: 0.0015 lr: 0.02\n",
            "iteration: 109550 loss: 0.0018 lr: 0.02\n",
            "iteration: 109560 loss: 0.0017 lr: 0.02\n",
            "iteration: 109570 loss: 0.0020 lr: 0.02\n",
            "iteration: 109580 loss: 0.0017 lr: 0.02\n",
            "iteration: 109590 loss: 0.0021 lr: 0.02\n",
            "iteration: 109600 loss: 0.0022 lr: 0.02\n",
            "iteration: 109610 loss: 0.0019 lr: 0.02\n",
            "iteration: 109620 loss: 0.0015 lr: 0.02\n",
            "iteration: 109630 loss: 0.0014 lr: 0.02\n",
            "iteration: 109640 loss: 0.0015 lr: 0.02\n",
            "iteration: 109650 loss: 0.0015 lr: 0.02\n",
            "iteration: 109660 loss: 0.0021 lr: 0.02\n",
            "iteration: 109670 loss: 0.0016 lr: 0.02\n",
            "iteration: 109680 loss: 0.0016 lr: 0.02\n",
            "iteration: 109690 loss: 0.0014 lr: 0.02\n",
            "iteration: 109700 loss: 0.0014 lr: 0.02\n",
            "iteration: 109710 loss: 0.0019 lr: 0.02\n",
            "iteration: 109720 loss: 0.0016 lr: 0.02\n",
            "iteration: 109730 loss: 0.0019 lr: 0.02\n",
            "iteration: 109740 loss: 0.0016 lr: 0.02\n",
            "iteration: 109750 loss: 0.0018 lr: 0.02\n",
            "iteration: 109760 loss: 0.0019 lr: 0.02\n",
            "iteration: 109770 loss: 0.0020 lr: 0.02\n",
            "iteration: 109780 loss: 0.0014 lr: 0.02\n",
            "iteration: 109790 loss: 0.0018 lr: 0.02\n",
            "iteration: 109800 loss: 0.0016 lr: 0.02\n",
            "iteration: 109810 loss: 0.0016 lr: 0.02\n",
            "iteration: 109820 loss: 0.0018 lr: 0.02\n",
            "iteration: 109830 loss: 0.0015 lr: 0.02\n",
            "iteration: 109840 loss: 0.0023 lr: 0.02\n",
            "iteration: 109850 loss: 0.0018 lr: 0.02\n",
            "iteration: 109860 loss: 0.0017 lr: 0.02\n",
            "iteration: 109870 loss: 0.0018 lr: 0.02\n",
            "iteration: 109880 loss: 0.0015 lr: 0.02\n",
            "iteration: 109890 loss: 0.0015 lr: 0.02\n",
            "iteration: 109900 loss: 0.0020 lr: 0.02\n",
            "iteration: 109910 loss: 0.0013 lr: 0.02\n",
            "iteration: 109920 loss: 0.0018 lr: 0.02\n",
            "iteration: 109930 loss: 0.0016 lr: 0.02\n",
            "iteration: 109940 loss: 0.0016 lr: 0.02\n",
            "iteration: 109950 loss: 0.0019 lr: 0.02\n",
            "iteration: 109960 loss: 0.0017 lr: 0.02\n",
            "iteration: 109970 loss: 0.0019 lr: 0.02\n",
            "iteration: 109980 loss: 0.0019 lr: 0.02\n",
            "iteration: 109990 loss: 0.0024 lr: 0.02\n",
            "iteration: 110000 loss: 0.0013 lr: 0.02\n",
            "iteration: 110010 loss: 0.0018 lr: 0.02\n",
            "iteration: 110020 loss: 0.0016 lr: 0.02\n",
            "iteration: 110030 loss: 0.0022 lr: 0.02\n",
            "iteration: 110040 loss: 0.0019 lr: 0.02\n",
            "iteration: 110050 loss: 0.0013 lr: 0.02\n",
            "iteration: 110060 loss: 0.0015 lr: 0.02\n",
            "iteration: 110070 loss: 0.0018 lr: 0.02\n",
            "iteration: 110080 loss: 0.0015 lr: 0.02\n",
            "iteration: 110090 loss: 0.0018 lr: 0.02\n",
            "iteration: 110100 loss: 0.0019 lr: 0.02\n",
            "iteration: 110110 loss: 0.0018 lr: 0.02\n",
            "iteration: 110120 loss: 0.0027 lr: 0.02\n",
            "iteration: 110130 loss: 0.0023 lr: 0.02\n",
            "iteration: 110140 loss: 0.0023 lr: 0.02\n",
            "iteration: 110150 loss: 0.0018 lr: 0.02\n",
            "iteration: 110160 loss: 0.0018 lr: 0.02\n",
            "iteration: 110170 loss: 0.0014 lr: 0.02\n",
            "iteration: 110180 loss: 0.0014 lr: 0.02\n",
            "iteration: 110190 loss: 0.0021 lr: 0.02\n",
            "iteration: 110200 loss: 0.0024 lr: 0.02\n",
            "iteration: 110210 loss: 0.0020 lr: 0.02\n",
            "iteration: 110220 loss: 0.0014 lr: 0.02\n",
            "iteration: 110230 loss: 0.0016 lr: 0.02\n",
            "iteration: 110240 loss: 0.0019 lr: 0.02\n",
            "iteration: 110250 loss: 0.0020 lr: 0.02\n",
            "iteration: 110260 loss: 0.0019 lr: 0.02\n",
            "iteration: 110270 loss: 0.0018 lr: 0.02\n",
            "iteration: 110280 loss: 0.0018 lr: 0.02\n",
            "iteration: 110290 loss: 0.0016 lr: 0.02\n",
            "iteration: 110300 loss: 0.0018 lr: 0.02\n",
            "iteration: 110310 loss: 0.0023 lr: 0.02\n",
            "iteration: 110320 loss: 0.0018 lr: 0.02\n",
            "iteration: 110330 loss: 0.0017 lr: 0.02\n",
            "iteration: 110340 loss: 0.0018 lr: 0.02\n",
            "iteration: 110350 loss: 0.0015 lr: 0.02\n",
            "iteration: 110360 loss: 0.0013 lr: 0.02\n",
            "iteration: 110370 loss: 0.0015 lr: 0.02\n",
            "iteration: 110380 loss: 0.0022 lr: 0.02\n",
            "iteration: 110390 loss: 0.0016 lr: 0.02\n",
            "iteration: 110400 loss: 0.0014 lr: 0.02\n",
            "iteration: 110410 loss: 0.0016 lr: 0.02\n",
            "iteration: 110420 loss: 0.0016 lr: 0.02\n",
            "iteration: 110430 loss: 0.0017 lr: 0.02\n",
            "iteration: 110440 loss: 0.0017 lr: 0.02\n",
            "iteration: 110450 loss: 0.0017 lr: 0.02\n",
            "iteration: 110460 loss: 0.0021 lr: 0.02\n",
            "iteration: 110470 loss: 0.0032 lr: 0.02\n",
            "iteration: 110480 loss: 0.0019 lr: 0.02\n",
            "iteration: 110490 loss: 0.0017 lr: 0.02\n",
            "iteration: 110500 loss: 0.0020 lr: 0.02\n",
            "iteration: 110510 loss: 0.0015 lr: 0.02\n",
            "iteration: 110520 loss: 0.0017 lr: 0.02\n",
            "iteration: 110530 loss: 0.0015 lr: 0.02\n",
            "iteration: 110540 loss: 0.0017 lr: 0.02\n",
            "iteration: 110550 loss: 0.0016 lr: 0.02\n",
            "iteration: 110560 loss: 0.0016 lr: 0.02\n",
            "iteration: 110570 loss: 0.0019 lr: 0.02\n",
            "iteration: 110580 loss: 0.0018 lr: 0.02\n",
            "iteration: 110590 loss: 0.0021 lr: 0.02\n",
            "iteration: 110600 loss: 0.0018 lr: 0.02\n",
            "iteration: 110610 loss: 0.0015 lr: 0.02\n",
            "iteration: 110620 loss: 0.0017 lr: 0.02\n",
            "iteration: 110630 loss: 0.0016 lr: 0.02\n",
            "iteration: 110640 loss: 0.0016 lr: 0.02\n",
            "iteration: 110650 loss: 0.0024 lr: 0.02\n",
            "iteration: 110660 loss: 0.0016 lr: 0.02\n",
            "iteration: 110670 loss: 0.0023 lr: 0.02\n",
            "iteration: 110680 loss: 0.0016 lr: 0.02\n",
            "iteration: 110690 loss: 0.0017 lr: 0.02\n",
            "iteration: 110700 loss: 0.0020 lr: 0.02\n",
            "iteration: 110710 loss: 0.0012 lr: 0.02\n",
            "iteration: 110720 loss: 0.0019 lr: 0.02\n",
            "iteration: 110730 loss: 0.0015 lr: 0.02\n",
            "iteration: 110740 loss: 0.0022 lr: 0.02\n",
            "iteration: 110750 loss: 0.0013 lr: 0.02\n",
            "iteration: 110760 loss: 0.0019 lr: 0.02\n",
            "iteration: 110770 loss: 0.0017 lr: 0.02\n",
            "iteration: 110780 loss: 0.0015 lr: 0.02\n",
            "iteration: 110790 loss: 0.0018 lr: 0.02\n",
            "iteration: 110800 loss: 0.0017 lr: 0.02\n",
            "iteration: 110810 loss: 0.0020 lr: 0.02\n",
            "iteration: 110820 loss: 0.0016 lr: 0.02\n",
            "iteration: 110830 loss: 0.0016 lr: 0.02\n",
            "iteration: 110840 loss: 0.0019 lr: 0.02\n",
            "iteration: 110850 loss: 0.0016 lr: 0.02\n",
            "iteration: 110860 loss: 0.0021 lr: 0.02\n",
            "iteration: 110870 loss: 0.0017 lr: 0.02\n",
            "iteration: 110880 loss: 0.0019 lr: 0.02\n",
            "iteration: 110890 loss: 0.0014 lr: 0.02\n",
            "iteration: 110900 loss: 0.0013 lr: 0.02\n",
            "iteration: 110910 loss: 0.0017 lr: 0.02\n",
            "iteration: 110920 loss: 0.0016 lr: 0.02\n",
            "iteration: 110930 loss: 0.0019 lr: 0.02\n",
            "iteration: 110940 loss: 0.0019 lr: 0.02\n",
            "iteration: 110950 loss: 0.0017 lr: 0.02\n",
            "iteration: 110960 loss: 0.0014 lr: 0.02\n",
            "iteration: 110970 loss: 0.0016 lr: 0.02\n",
            "iteration: 110980 loss: 0.0018 lr: 0.02\n",
            "iteration: 110990 loss: 0.0018 lr: 0.02\n",
            "iteration: 111000 loss: 0.0018 lr: 0.02\n",
            "iteration: 111010 loss: 0.0018 lr: 0.02\n",
            "iteration: 111020 loss: 0.0015 lr: 0.02\n",
            "iteration: 111030 loss: 0.0018 lr: 0.02\n",
            "iteration: 111040 loss: 0.0016 lr: 0.02\n",
            "iteration: 111050 loss: 0.0013 lr: 0.02\n",
            "iteration: 111060 loss: 0.0016 lr: 0.02\n",
            "iteration: 111070 loss: 0.0019 lr: 0.02\n",
            "iteration: 111080 loss: 0.0018 lr: 0.02\n",
            "iteration: 111090 loss: 0.0014 lr: 0.02\n",
            "iteration: 111100 loss: 0.0016 lr: 0.02\n",
            "iteration: 111110 loss: 0.0018 lr: 0.02\n",
            "iteration: 111120 loss: 0.0016 lr: 0.02\n",
            "iteration: 111130 loss: 0.0015 lr: 0.02\n",
            "iteration: 111140 loss: 0.0018 lr: 0.02\n",
            "iteration: 111150 loss: 0.0014 lr: 0.02\n",
            "iteration: 111160 loss: 0.0013 lr: 0.02\n",
            "iteration: 111170 loss: 0.0019 lr: 0.02\n",
            "iteration: 111180 loss: 0.0019 lr: 0.02\n",
            "iteration: 111190 loss: 0.0019 lr: 0.02\n",
            "iteration: 111200 loss: 0.0019 lr: 0.02\n",
            "iteration: 111210 loss: 0.0021 lr: 0.02\n",
            "iteration: 111220 loss: 0.0016 lr: 0.02\n",
            "iteration: 111230 loss: 0.0018 lr: 0.02\n",
            "iteration: 111240 loss: 0.0018 lr: 0.02\n",
            "iteration: 111250 loss: 0.0015 lr: 0.02\n",
            "iteration: 111260 loss: 0.0018 lr: 0.02\n",
            "iteration: 111270 loss: 0.0017 lr: 0.02\n",
            "iteration: 111280 loss: 0.0020 lr: 0.02\n",
            "iteration: 111290 loss: 0.0017 lr: 0.02\n",
            "iteration: 111300 loss: 0.0017 lr: 0.02\n",
            "iteration: 111310 loss: 0.0017 lr: 0.02\n",
            "iteration: 111320 loss: 0.0015 lr: 0.02\n",
            "iteration: 111330 loss: 0.0020 lr: 0.02\n",
            "iteration: 111340 loss: 0.0012 lr: 0.02\n",
            "iteration: 111350 loss: 0.0017 lr: 0.02\n",
            "iteration: 111360 loss: 0.0014 lr: 0.02\n",
            "iteration: 111370 loss: 0.0012 lr: 0.02\n",
            "iteration: 111380 loss: 0.0020 lr: 0.02\n",
            "iteration: 111390 loss: 0.0021 lr: 0.02\n",
            "iteration: 111400 loss: 0.0014 lr: 0.02\n",
            "iteration: 111410 loss: 0.0013 lr: 0.02\n",
            "iteration: 111420 loss: 0.0014 lr: 0.02\n",
            "iteration: 111430 loss: 0.0021 lr: 0.02\n",
            "iteration: 111440 loss: 0.0021 lr: 0.02\n",
            "iteration: 111450 loss: 0.0019 lr: 0.02\n",
            "iteration: 111460 loss: 0.0013 lr: 0.02\n",
            "iteration: 111470 loss: 0.0017 lr: 0.02\n",
            "iteration: 111480 loss: 0.0023 lr: 0.02\n",
            "iteration: 111490 loss: 0.0015 lr: 0.02\n",
            "iteration: 111500 loss: 0.0019 lr: 0.02\n",
            "iteration: 111510 loss: 0.0014 lr: 0.02\n",
            "iteration: 111520 loss: 0.0016 lr: 0.02\n",
            "iteration: 111530 loss: 0.0015 lr: 0.02\n",
            "iteration: 111540 loss: 0.0017 lr: 0.02\n",
            "iteration: 111550 loss: 0.0016 lr: 0.02\n",
            "iteration: 111560 loss: 0.0013 lr: 0.02\n",
            "iteration: 111570 loss: 0.0017 lr: 0.02\n",
            "iteration: 111580 loss: 0.0016 lr: 0.02\n",
            "iteration: 111590 loss: 0.0016 lr: 0.02\n",
            "iteration: 111600 loss: 0.0018 lr: 0.02\n",
            "iteration: 111610 loss: 0.0015 lr: 0.02\n",
            "iteration: 111620 loss: 0.0013 lr: 0.02\n",
            "iteration: 111630 loss: 0.0018 lr: 0.02\n",
            "iteration: 111640 loss: 0.0014 lr: 0.02\n",
            "iteration: 111650 loss: 0.0020 lr: 0.02\n",
            "iteration: 111660 loss: 0.0018 lr: 0.02\n",
            "iteration: 111670 loss: 0.0016 lr: 0.02\n",
            "iteration: 111680 loss: 0.0014 lr: 0.02\n",
            "iteration: 111690 loss: 0.0017 lr: 0.02\n",
            "iteration: 111700 loss: 0.0013 lr: 0.02\n",
            "iteration: 111710 loss: 0.0016 lr: 0.02\n",
            "iteration: 111720 loss: 0.0017 lr: 0.02\n",
            "iteration: 111730 loss: 0.0019 lr: 0.02\n",
            "iteration: 111740 loss: 0.0015 lr: 0.02\n",
            "iteration: 111750 loss: 0.0016 lr: 0.02\n",
            "iteration: 111760 loss: 0.0018 lr: 0.02\n",
            "iteration: 111770 loss: 0.0016 lr: 0.02\n",
            "iteration: 111780 loss: 0.0015 lr: 0.02\n",
            "iteration: 111790 loss: 0.0019 lr: 0.02\n",
            "iteration: 111800 loss: 0.0017 lr: 0.02\n",
            "iteration: 111810 loss: 0.0020 lr: 0.02\n",
            "iteration: 111820 loss: 0.0014 lr: 0.02\n",
            "iteration: 111830 loss: 0.0015 lr: 0.02\n",
            "iteration: 111840 loss: 0.0016 lr: 0.02\n",
            "iteration: 111850 loss: 0.0014 lr: 0.02\n",
            "iteration: 111860 loss: 0.0016 lr: 0.02\n",
            "iteration: 111870 loss: 0.0015 lr: 0.02\n",
            "iteration: 111880 loss: 0.0016 lr: 0.02\n",
            "iteration: 111890 loss: 0.0014 lr: 0.02\n",
            "iteration: 111900 loss: 0.0016 lr: 0.02\n",
            "iteration: 111910 loss: 0.0016 lr: 0.02\n",
            "iteration: 111920 loss: 0.0018 lr: 0.02\n",
            "iteration: 111930 loss: 0.0015 lr: 0.02\n",
            "iteration: 111940 loss: 0.0018 lr: 0.02\n",
            "iteration: 111950 loss: 0.0017 lr: 0.02\n",
            "iteration: 111960 loss: 0.0016 lr: 0.02\n",
            "iteration: 111970 loss: 0.0021 lr: 0.02\n",
            "iteration: 111980 loss: 0.0021 lr: 0.02\n",
            "iteration: 111990 loss: 0.0018 lr: 0.02\n",
            "iteration: 112000 loss: 0.0017 lr: 0.02\n",
            "iteration: 112010 loss: 0.0016 lr: 0.02\n",
            "iteration: 112020 loss: 0.0016 lr: 0.02\n",
            "iteration: 112030 loss: 0.0012 lr: 0.02\n",
            "iteration: 112040 loss: 0.0018 lr: 0.02\n",
            "iteration: 112050 loss: 0.0016 lr: 0.02\n",
            "iteration: 112060 loss: 0.0018 lr: 0.02\n",
            "iteration: 112070 loss: 0.0018 lr: 0.02\n",
            "iteration: 112080 loss: 0.0013 lr: 0.02\n",
            "iteration: 112090 loss: 0.0014 lr: 0.02\n",
            "iteration: 112100 loss: 0.0024 lr: 0.02\n",
            "iteration: 112110 loss: 0.0014 lr: 0.02\n",
            "iteration: 112120 loss: 0.0020 lr: 0.02\n",
            "iteration: 112130 loss: 0.0016 lr: 0.02\n",
            "iteration: 112140 loss: 0.0019 lr: 0.02\n",
            "iteration: 112150 loss: 0.0021 lr: 0.02\n",
            "iteration: 112160 loss: 0.0018 lr: 0.02\n",
            "iteration: 112170 loss: 0.0019 lr: 0.02\n",
            "iteration: 112180 loss: 0.0015 lr: 0.02\n",
            "iteration: 112190 loss: 0.0018 lr: 0.02\n",
            "iteration: 112200 loss: 0.0015 lr: 0.02\n",
            "iteration: 112210 loss: 0.0021 lr: 0.02\n",
            "iteration: 112220 loss: 0.0020 lr: 0.02\n",
            "iteration: 112230 loss: 0.0019 lr: 0.02\n",
            "iteration: 112240 loss: 0.0016 lr: 0.02\n",
            "iteration: 112250 loss: 0.0016 lr: 0.02\n",
            "iteration: 112260 loss: 0.0019 lr: 0.02\n",
            "iteration: 112270 loss: 0.0015 lr: 0.02\n",
            "iteration: 112280 loss: 0.0015 lr: 0.02\n",
            "iteration: 112290 loss: 0.0018 lr: 0.02\n",
            "iteration: 112300 loss: 0.0016 lr: 0.02\n",
            "iteration: 112310 loss: 0.0021 lr: 0.02\n",
            "iteration: 112320 loss: 0.0016 lr: 0.02\n",
            "iteration: 112330 loss: 0.0019 lr: 0.02\n",
            "iteration: 112340 loss: 0.0017 lr: 0.02\n",
            "iteration: 112350 loss: 0.0014 lr: 0.02\n",
            "iteration: 112360 loss: 0.0013 lr: 0.02\n",
            "iteration: 112370 loss: 0.0020 lr: 0.02\n",
            "iteration: 112380 loss: 0.0015 lr: 0.02\n",
            "iteration: 112390 loss: 0.0016 lr: 0.02\n",
            "iteration: 112400 loss: 0.0012 lr: 0.02\n",
            "iteration: 112410 loss: 0.0020 lr: 0.02\n",
            "iteration: 112420 loss: 0.0016 lr: 0.02\n",
            "iteration: 112430 loss: 0.0013 lr: 0.02\n",
            "iteration: 112440 loss: 0.0019 lr: 0.02\n",
            "iteration: 112450 loss: 0.0013 lr: 0.02\n",
            "iteration: 112460 loss: 0.0013 lr: 0.02\n",
            "iteration: 112470 loss: 0.0016 lr: 0.02\n",
            "iteration: 112480 loss: 0.0022 lr: 0.02\n",
            "iteration: 112490 loss: 0.0017 lr: 0.02\n",
            "iteration: 112500 loss: 0.0015 lr: 0.02\n",
            "iteration: 112510 loss: 0.0016 lr: 0.02\n",
            "iteration: 112520 loss: 0.0019 lr: 0.02\n",
            "iteration: 112530 loss: 0.0015 lr: 0.02\n",
            "iteration: 112540 loss: 0.0018 lr: 0.02\n",
            "iteration: 112550 loss: 0.0022 lr: 0.02\n",
            "iteration: 112560 loss: 0.0015 lr: 0.02\n",
            "iteration: 112570 loss: 0.0015 lr: 0.02\n",
            "iteration: 112580 loss: 0.0020 lr: 0.02\n",
            "iteration: 112590 loss: 0.0019 lr: 0.02\n",
            "iteration: 112600 loss: 0.0016 lr: 0.02\n",
            "iteration: 112610 loss: 0.0017 lr: 0.02\n",
            "iteration: 112620 loss: 0.0016 lr: 0.02\n",
            "iteration: 112630 loss: 0.0021 lr: 0.02\n",
            "iteration: 112640 loss: 0.0015 lr: 0.02\n",
            "iteration: 112650 loss: 0.0014 lr: 0.02\n",
            "iteration: 112660 loss: 0.0022 lr: 0.02\n",
            "iteration: 112670 loss: 0.0015 lr: 0.02\n",
            "iteration: 112680 loss: 0.0016 lr: 0.02\n",
            "iteration: 112690 loss: 0.0019 lr: 0.02\n",
            "iteration: 112700 loss: 0.0016 lr: 0.02\n",
            "iteration: 112710 loss: 0.0020 lr: 0.02\n",
            "iteration: 112720 loss: 0.0018 lr: 0.02\n",
            "iteration: 112730 loss: 0.0018 lr: 0.02\n",
            "iteration: 112740 loss: 0.0015 lr: 0.02\n",
            "iteration: 112750 loss: 0.0013 lr: 0.02\n",
            "iteration: 112760 loss: 0.0015 lr: 0.02\n",
            "iteration: 112770 loss: 0.0017 lr: 0.02\n",
            "iteration: 112780 loss: 0.0017 lr: 0.02\n",
            "iteration: 112790 loss: 0.0016 lr: 0.02\n",
            "iteration: 112800 loss: 0.0022 lr: 0.02\n",
            "iteration: 112810 loss: 0.0016 lr: 0.02\n",
            "iteration: 112820 loss: 0.0014 lr: 0.02\n",
            "iteration: 112830 loss: 0.0015 lr: 0.02\n",
            "iteration: 112840 loss: 0.0022 lr: 0.02\n",
            "iteration: 112850 loss: 0.0016 lr: 0.02\n",
            "iteration: 112860 loss: 0.0018 lr: 0.02\n",
            "iteration: 112870 loss: 0.0018 lr: 0.02\n",
            "iteration: 112880 loss: 0.0013 lr: 0.02\n",
            "iteration: 112890 loss: 0.0014 lr: 0.02\n",
            "iteration: 112900 loss: 0.0022 lr: 0.02\n",
            "iteration: 112910 loss: 0.0017 lr: 0.02\n",
            "iteration: 112920 loss: 0.0012 lr: 0.02\n",
            "iteration: 112930 loss: 0.0022 lr: 0.02\n",
            "iteration: 112940 loss: 0.0016 lr: 0.02\n",
            "iteration: 112950 loss: 0.0016 lr: 0.02\n",
            "iteration: 112960 loss: 0.0019 lr: 0.02\n",
            "iteration: 112970 loss: 0.0017 lr: 0.02\n",
            "iteration: 112980 loss: 0.0017 lr: 0.02\n",
            "iteration: 112990 loss: 0.0014 lr: 0.02\n",
            "iteration: 113000 loss: 0.0018 lr: 0.02\n",
            "iteration: 113010 loss: 0.0015 lr: 0.02\n",
            "iteration: 113020 loss: 0.0020 lr: 0.02\n",
            "iteration: 113030 loss: 0.0013 lr: 0.02\n",
            "iteration: 113040 loss: 0.0019 lr: 0.02\n",
            "iteration: 113050 loss: 0.0015 lr: 0.02\n",
            "iteration: 113060 loss: 0.0018 lr: 0.02\n",
            "iteration: 113070 loss: 0.0017 lr: 0.02\n",
            "iteration: 113080 loss: 0.0023 lr: 0.02\n",
            "iteration: 113090 loss: 0.0023 lr: 0.02\n",
            "iteration: 113100 loss: 0.0014 lr: 0.02\n",
            "iteration: 113110 loss: 0.0018 lr: 0.02\n",
            "iteration: 113120 loss: 0.0017 lr: 0.02\n",
            "iteration: 113130 loss: 0.0016 lr: 0.02\n",
            "iteration: 113140 loss: 0.0018 lr: 0.02\n",
            "iteration: 113150 loss: 0.0019 lr: 0.02\n",
            "iteration: 113160 loss: 0.0015 lr: 0.02\n",
            "iteration: 113170 loss: 0.0013 lr: 0.02\n",
            "iteration: 113180 loss: 0.0016 lr: 0.02\n",
            "iteration: 113190 loss: 0.0017 lr: 0.02\n",
            "iteration: 113200 loss: 0.0016 lr: 0.02\n",
            "iteration: 113210 loss: 0.0015 lr: 0.02\n",
            "iteration: 113220 loss: 0.0019 lr: 0.02\n",
            "iteration: 113230 loss: 0.0016 lr: 0.02\n",
            "iteration: 113240 loss: 0.0017 lr: 0.02\n",
            "iteration: 113250 loss: 0.0014 lr: 0.02\n",
            "iteration: 113260 loss: 0.0019 lr: 0.02\n",
            "iteration: 113270 loss: 0.0015 lr: 0.02\n",
            "iteration: 113280 loss: 0.0022 lr: 0.02\n",
            "iteration: 113290 loss: 0.0017 lr: 0.02\n",
            "iteration: 113300 loss: 0.0014 lr: 0.02\n",
            "iteration: 113310 loss: 0.0019 lr: 0.02\n",
            "iteration: 113320 loss: 0.0015 lr: 0.02\n",
            "iteration: 113330 loss: 0.0020 lr: 0.02\n",
            "iteration: 113340 loss: 0.0017 lr: 0.02\n",
            "iteration: 113350 loss: 0.0017 lr: 0.02\n",
            "iteration: 113360 loss: 0.0016 lr: 0.02\n",
            "iteration: 113370 loss: 0.0017 lr: 0.02\n",
            "iteration: 113380 loss: 0.0021 lr: 0.02\n",
            "iteration: 113390 loss: 0.0017 lr: 0.02\n",
            "iteration: 113400 loss: 0.0016 lr: 0.02\n",
            "iteration: 113410 loss: 0.0019 lr: 0.02\n",
            "iteration: 113420 loss: 0.0018 lr: 0.02\n",
            "iteration: 113430 loss: 0.0021 lr: 0.02\n",
            "iteration: 113440 loss: 0.0016 lr: 0.02\n",
            "iteration: 113450 loss: 0.0018 lr: 0.02\n",
            "iteration: 113460 loss: 0.0013 lr: 0.02\n",
            "iteration: 113470 loss: 0.0017 lr: 0.02\n",
            "iteration: 113480 loss: 0.0019 lr: 0.02\n",
            "iteration: 113490 loss: 0.0017 lr: 0.02\n",
            "iteration: 113500 loss: 0.0017 lr: 0.02\n",
            "iteration: 113510 loss: 0.0016 lr: 0.02\n",
            "iteration: 113520 loss: 0.0019 lr: 0.02\n",
            "iteration: 113530 loss: 0.0018 lr: 0.02\n",
            "iteration: 113540 loss: 0.0016 lr: 0.02\n",
            "iteration: 113550 loss: 0.0016 lr: 0.02\n",
            "iteration: 113560 loss: 0.0018 lr: 0.02\n",
            "iteration: 113570 loss: 0.0018 lr: 0.02\n",
            "iteration: 113580 loss: 0.0020 lr: 0.02\n",
            "iteration: 113590 loss: 0.0015 lr: 0.02\n",
            "iteration: 113600 loss: 0.0019 lr: 0.02\n",
            "iteration: 113610 loss: 0.0013 lr: 0.02\n",
            "iteration: 113620 loss: 0.0015 lr: 0.02\n",
            "iteration: 113630 loss: 0.0020 lr: 0.02\n",
            "iteration: 113640 loss: 0.0019 lr: 0.02\n",
            "iteration: 113650 loss: 0.0013 lr: 0.02\n",
            "iteration: 113660 loss: 0.0016 lr: 0.02\n",
            "iteration: 113670 loss: 0.0017 lr: 0.02\n",
            "iteration: 113680 loss: 0.0014 lr: 0.02\n",
            "iteration: 113690 loss: 0.0016 lr: 0.02\n",
            "iteration: 113700 loss: 0.0015 lr: 0.02\n",
            "iteration: 113710 loss: 0.0018 lr: 0.02\n",
            "iteration: 113720 loss: 0.0022 lr: 0.02\n",
            "iteration: 113730 loss: 0.0015 lr: 0.02\n",
            "iteration: 113740 loss: 0.0016 lr: 0.02\n",
            "iteration: 113750 loss: 0.0021 lr: 0.02\n",
            "iteration: 113760 loss: 0.0019 lr: 0.02\n",
            "iteration: 113770 loss: 0.0016 lr: 0.02\n",
            "iteration: 113780 loss: 0.0015 lr: 0.02\n",
            "iteration: 113790 loss: 0.0020 lr: 0.02\n",
            "iteration: 113800 loss: 0.0014 lr: 0.02\n",
            "iteration: 113810 loss: 0.0016 lr: 0.02\n",
            "iteration: 113820 loss: 0.0015 lr: 0.02\n",
            "iteration: 113830 loss: 0.0017 lr: 0.02\n",
            "iteration: 113840 loss: 0.0016 lr: 0.02\n",
            "iteration: 113850 loss: 0.0015 lr: 0.02\n",
            "iteration: 113860 loss: 0.0023 lr: 0.02\n",
            "iteration: 113870 loss: 0.0015 lr: 0.02\n",
            "iteration: 113880 loss: 0.0015 lr: 0.02\n",
            "iteration: 113890 loss: 0.0017 lr: 0.02\n",
            "iteration: 113900 loss: 0.0015 lr: 0.02\n",
            "iteration: 113910 loss: 0.0018 lr: 0.02\n",
            "iteration: 113920 loss: 0.0017 lr: 0.02\n",
            "iteration: 113930 loss: 0.0017 lr: 0.02\n",
            "iteration: 113940 loss: 0.0015 lr: 0.02\n",
            "iteration: 113950 loss: 0.0015 lr: 0.02\n",
            "iteration: 113960 loss: 0.0017 lr: 0.02\n",
            "iteration: 113970 loss: 0.0014 lr: 0.02\n",
            "iteration: 113980 loss: 0.0013 lr: 0.02\n",
            "iteration: 113990 loss: 0.0016 lr: 0.02\n",
            "iteration: 114000 loss: 0.0013 lr: 0.02\n",
            "iteration: 114010 loss: 0.0013 lr: 0.02\n",
            "iteration: 114020 loss: 0.0014 lr: 0.02\n",
            "iteration: 114030 loss: 0.0013 lr: 0.02\n",
            "iteration: 114040 loss: 0.0018 lr: 0.02\n",
            "iteration: 114050 loss: 0.0018 lr: 0.02\n",
            "iteration: 114060 loss: 0.0014 lr: 0.02\n",
            "iteration: 114070 loss: 0.0017 lr: 0.02\n",
            "iteration: 114080 loss: 0.0017 lr: 0.02\n",
            "iteration: 114090 loss: 0.0016 lr: 0.02\n",
            "iteration: 114100 loss: 0.0013 lr: 0.02\n",
            "iteration: 114110 loss: 0.0016 lr: 0.02\n",
            "iteration: 114120 loss: 0.0019 lr: 0.02\n",
            "iteration: 114130 loss: 0.0015 lr: 0.02\n",
            "iteration: 114140 loss: 0.0017 lr: 0.02\n",
            "iteration: 114150 loss: 0.0014 lr: 0.02\n",
            "iteration: 114160 loss: 0.0016 lr: 0.02\n",
            "iteration: 114170 loss: 0.0014 lr: 0.02\n",
            "iteration: 114180 loss: 0.0019 lr: 0.02\n",
            "iteration: 114190 loss: 0.0015 lr: 0.02\n",
            "iteration: 114200 loss: 0.0018 lr: 0.02\n",
            "iteration: 114210 loss: 0.0017 lr: 0.02\n",
            "iteration: 114220 loss: 0.0023 lr: 0.02\n",
            "iteration: 114230 loss: 0.0023 lr: 0.02\n",
            "iteration: 114240 loss: 0.0023 lr: 0.02\n",
            "iteration: 114250 loss: 0.0014 lr: 0.02\n",
            "iteration: 114260 loss: 0.0017 lr: 0.02\n",
            "iteration: 114270 loss: 0.0017 lr: 0.02\n",
            "iteration: 114280 loss: 0.0017 lr: 0.02\n",
            "iteration: 114290 loss: 0.0020 lr: 0.02\n",
            "iteration: 114300 loss: 0.0017 lr: 0.02\n",
            "iteration: 114310 loss: 0.0016 lr: 0.02\n",
            "iteration: 114320 loss: 0.0016 lr: 0.02\n",
            "iteration: 114330 loss: 0.0011 lr: 0.02\n",
            "iteration: 114340 loss: 0.0017 lr: 0.02\n",
            "iteration: 114350 loss: 0.0013 lr: 0.02\n",
            "iteration: 114360 loss: 0.0016 lr: 0.02\n",
            "iteration: 114370 loss: 0.0018 lr: 0.02\n",
            "iteration: 114380 loss: 0.0018 lr: 0.02\n",
            "iteration: 114390 loss: 0.0014 lr: 0.02\n",
            "iteration: 114400 loss: 0.0019 lr: 0.02\n",
            "iteration: 114410 loss: 0.0017 lr: 0.02\n",
            "iteration: 114420 loss: 0.0018 lr: 0.02\n",
            "iteration: 114430 loss: 0.0016 lr: 0.02\n",
            "iteration: 114440 loss: 0.0014 lr: 0.02\n",
            "iteration: 114450 loss: 0.0015 lr: 0.02\n",
            "iteration: 114460 loss: 0.0015 lr: 0.02\n",
            "iteration: 114470 loss: 0.0018 lr: 0.02\n",
            "iteration: 114480 loss: 0.0019 lr: 0.02\n",
            "iteration: 114490 loss: 0.0015 lr: 0.02\n",
            "iteration: 114500 loss: 0.0018 lr: 0.02\n",
            "iteration: 114510 loss: 0.0016 lr: 0.02\n",
            "iteration: 114520 loss: 0.0013 lr: 0.02\n",
            "iteration: 114530 loss: 0.0014 lr: 0.02\n",
            "iteration: 114540 loss: 0.0017 lr: 0.02\n",
            "iteration: 114550 loss: 0.0016 lr: 0.02\n",
            "iteration: 114560 loss: 0.0017 lr: 0.02\n",
            "iteration: 114570 loss: 0.0016 lr: 0.02\n",
            "iteration: 114580 loss: 0.0015 lr: 0.02\n",
            "iteration: 114590 loss: 0.0015 lr: 0.02\n",
            "iteration: 114600 loss: 0.0014 lr: 0.02\n",
            "iteration: 114610 loss: 0.0014 lr: 0.02\n",
            "iteration: 114620 loss: 0.0015 lr: 0.02\n",
            "iteration: 114630 loss: 0.0016 lr: 0.02\n",
            "iteration: 114640 loss: 0.0016 lr: 0.02\n",
            "iteration: 114650 loss: 0.0018 lr: 0.02\n",
            "iteration: 114660 loss: 0.0013 lr: 0.02\n",
            "iteration: 114670 loss: 0.0021 lr: 0.02\n",
            "iteration: 114680 loss: 0.0016 lr: 0.02\n",
            "iteration: 114690 loss: 0.0013 lr: 0.02\n",
            "iteration: 114700 loss: 0.0016 lr: 0.02\n",
            "iteration: 114710 loss: 0.0015 lr: 0.02\n",
            "iteration: 114720 loss: 0.0018 lr: 0.02\n",
            "iteration: 114730 loss: 0.0014 lr: 0.02\n",
            "iteration: 114740 loss: 0.0019 lr: 0.02\n",
            "iteration: 114750 loss: 0.0015 lr: 0.02\n",
            "iteration: 114760 loss: 0.0017 lr: 0.02\n",
            "iteration: 114770 loss: 0.0011 lr: 0.02\n",
            "iteration: 114780 loss: 0.0014 lr: 0.02\n",
            "iteration: 114790 loss: 0.0013 lr: 0.02\n",
            "iteration: 114800 loss: 0.0016 lr: 0.02\n",
            "iteration: 114810 loss: 0.0016 lr: 0.02\n",
            "iteration: 114820 loss: 0.0019 lr: 0.02\n",
            "iteration: 114830 loss: 0.0014 lr: 0.02\n",
            "iteration: 114840 loss: 0.0014 lr: 0.02\n",
            "iteration: 114850 loss: 0.0013 lr: 0.02\n",
            "iteration: 114860 loss: 0.0015 lr: 0.02\n",
            "iteration: 114870 loss: 0.0017 lr: 0.02\n",
            "iteration: 114880 loss: 0.0016 lr: 0.02\n",
            "iteration: 114890 loss: 0.0020 lr: 0.02\n",
            "iteration: 114900 loss: 0.0018 lr: 0.02\n",
            "iteration: 114910 loss: 0.0017 lr: 0.02\n",
            "iteration: 114920 loss: 0.0015 lr: 0.02\n",
            "iteration: 114930 loss: 0.0015 lr: 0.02\n",
            "iteration: 114940 loss: 0.0018 lr: 0.02\n",
            "iteration: 114950 loss: 0.0017 lr: 0.02\n",
            "iteration: 114960 loss: 0.0015 lr: 0.02\n",
            "iteration: 114970 loss: 0.0018 lr: 0.02\n",
            "iteration: 114980 loss: 0.0011 lr: 0.02\n",
            "iteration: 114990 loss: 0.0017 lr: 0.02\n",
            "iteration: 115000 loss: 0.0017 lr: 0.02\n",
            "iteration: 115010 loss: 0.0015 lr: 0.02\n",
            "iteration: 115020 loss: 0.0014 lr: 0.02\n",
            "iteration: 115030 loss: 0.0016 lr: 0.02\n",
            "iteration: 115040 loss: 0.0013 lr: 0.02\n",
            "iteration: 115050 loss: 0.0016 lr: 0.02\n",
            "iteration: 115060 loss: 0.0016 lr: 0.02\n",
            "iteration: 115070 loss: 0.0019 lr: 0.02\n",
            "iteration: 115080 loss: 0.0018 lr: 0.02\n",
            "iteration: 115090 loss: 0.0019 lr: 0.02\n",
            "iteration: 115100 loss: 0.0019 lr: 0.02\n",
            "iteration: 115110 loss: 0.0018 lr: 0.02\n",
            "iteration: 115120 loss: 0.0017 lr: 0.02\n",
            "iteration: 115130 loss: 0.0018 lr: 0.02\n",
            "iteration: 115140 loss: 0.0019 lr: 0.02\n",
            "iteration: 115150 loss: 0.0019 lr: 0.02\n",
            "iteration: 115160 loss: 0.0017 lr: 0.02\n",
            "iteration: 115170 loss: 0.0018 lr: 0.02\n",
            "iteration: 115180 loss: 0.0015 lr: 0.02\n",
            "iteration: 115190 loss: 0.0011 lr: 0.02\n",
            "iteration: 115200 loss: 0.0014 lr: 0.02\n",
            "iteration: 115210 loss: 0.0011 lr: 0.02\n",
            "iteration: 115220 loss: 0.0017 lr: 0.02\n",
            "iteration: 115230 loss: 0.0016 lr: 0.02\n",
            "iteration: 115240 loss: 0.0018 lr: 0.02\n",
            "iteration: 115250 loss: 0.0013 lr: 0.02\n",
            "iteration: 115260 loss: 0.0013 lr: 0.02\n",
            "iteration: 115270 loss: 0.0015 lr: 0.02\n",
            "iteration: 115280 loss: 0.0015 lr: 0.02\n",
            "iteration: 115290 loss: 0.0019 lr: 0.02\n",
            "iteration: 115300 loss: 0.0015 lr: 0.02\n",
            "iteration: 115310 loss: 0.0016 lr: 0.02\n",
            "iteration: 115320 loss: 0.0016 lr: 0.02\n",
            "iteration: 115330 loss: 0.0020 lr: 0.02\n",
            "iteration: 115340 loss: 0.0018 lr: 0.02\n",
            "iteration: 115350 loss: 0.0017 lr: 0.02\n",
            "iteration: 115360 loss: 0.0019 lr: 0.02\n",
            "iteration: 115370 loss: 0.0020 lr: 0.02\n",
            "iteration: 115380 loss: 0.0010 lr: 0.02\n",
            "iteration: 115390 loss: 0.0016 lr: 0.02\n",
            "iteration: 115400 loss: 0.0022 lr: 0.02\n",
            "iteration: 115410 loss: 0.0015 lr: 0.02\n",
            "iteration: 115420 loss: 0.0016 lr: 0.02\n",
            "iteration: 115430 loss: 0.0016 lr: 0.02\n",
            "iteration: 115440 loss: 0.0021 lr: 0.02\n",
            "iteration: 115450 loss: 0.0017 lr: 0.02\n",
            "iteration: 115460 loss: 0.0020 lr: 0.02\n",
            "iteration: 115470 loss: 0.0021 lr: 0.02\n",
            "iteration: 115480 loss: 0.0013 lr: 0.02\n",
            "iteration: 115490 loss: 0.0019 lr: 0.02\n",
            "iteration: 115500 loss: 0.0014 lr: 0.02\n",
            "iteration: 115510 loss: 0.0015 lr: 0.02\n",
            "iteration: 115520 loss: 0.0014 lr: 0.02\n",
            "iteration: 115530 loss: 0.0020 lr: 0.02\n",
            "iteration: 115540 loss: 0.0018 lr: 0.02\n",
            "iteration: 115550 loss: 0.0020 lr: 0.02\n",
            "iteration: 115560 loss: 0.0017 lr: 0.02\n",
            "iteration: 115570 loss: 0.0017 lr: 0.02\n",
            "iteration: 115580 loss: 0.0016 lr: 0.02\n",
            "iteration: 115590 loss: 0.0027 lr: 0.02\n",
            "iteration: 115600 loss: 0.0019 lr: 0.02\n",
            "iteration: 115610 loss: 0.0019 lr: 0.02\n",
            "iteration: 115620 loss: 0.0015 lr: 0.02\n",
            "iteration: 115630 loss: 0.0014 lr: 0.02\n",
            "iteration: 115640 loss: 0.0023 lr: 0.02\n",
            "iteration: 115650 loss: 0.0018 lr: 0.02\n",
            "iteration: 115660 loss: 0.0017 lr: 0.02\n",
            "iteration: 115670 loss: 0.0021 lr: 0.02\n",
            "iteration: 115680 loss: 0.0016 lr: 0.02\n",
            "iteration: 115690 loss: 0.0021 lr: 0.02\n",
            "iteration: 115700 loss: 0.0019 lr: 0.02\n",
            "iteration: 115710 loss: 0.0017 lr: 0.02\n",
            "iteration: 115720 loss: 0.0018 lr: 0.02\n",
            "iteration: 115730 loss: 0.0019 lr: 0.02\n",
            "iteration: 115740 loss: 0.0015 lr: 0.02\n",
            "iteration: 115750 loss: 0.0014 lr: 0.02\n",
            "iteration: 115760 loss: 0.0017 lr: 0.02\n",
            "iteration: 115770 loss: 0.0021 lr: 0.02\n",
            "iteration: 115780 loss: 0.0013 lr: 0.02\n",
            "iteration: 115790 loss: 0.0018 lr: 0.02\n",
            "iteration: 115800 loss: 0.0015 lr: 0.02\n",
            "iteration: 115810 loss: 0.0014 lr: 0.02\n",
            "iteration: 115820 loss: 0.0013 lr: 0.02\n",
            "iteration: 115830 loss: 0.0014 lr: 0.02\n",
            "iteration: 115840 loss: 0.0018 lr: 0.02\n",
            "iteration: 115850 loss: 0.0024 lr: 0.02\n",
            "iteration: 115860 loss: 0.0017 lr: 0.02\n",
            "iteration: 115870 loss: 0.0020 lr: 0.02\n",
            "iteration: 115880 loss: 0.0018 lr: 0.02\n",
            "iteration: 115890 loss: 0.0015 lr: 0.02\n",
            "iteration: 115900 loss: 0.0014 lr: 0.02\n",
            "iteration: 115910 loss: 0.0020 lr: 0.02\n",
            "iteration: 115920 loss: 0.0016 lr: 0.02\n",
            "iteration: 115930 loss: 0.0015 lr: 0.02\n",
            "iteration: 115940 loss: 0.0022 lr: 0.02\n",
            "iteration: 115950 loss: 0.0017 lr: 0.02\n",
            "iteration: 115960 loss: 0.0018 lr: 0.02\n",
            "iteration: 115970 loss: 0.0015 lr: 0.02\n",
            "iteration: 115980 loss: 0.0017 lr: 0.02\n",
            "iteration: 115990 loss: 0.0014 lr: 0.02\n",
            "iteration: 116000 loss: 0.0012 lr: 0.02\n",
            "iteration: 116010 loss: 0.0017 lr: 0.02\n",
            "iteration: 116020 loss: 0.0019 lr: 0.02\n",
            "iteration: 116030 loss: 0.0015 lr: 0.02\n",
            "iteration: 116040 loss: 0.0014 lr: 0.02\n",
            "iteration: 116050 loss: 0.0014 lr: 0.02\n",
            "iteration: 116060 loss: 0.0023 lr: 0.02\n",
            "iteration: 116070 loss: 0.0019 lr: 0.02\n",
            "iteration: 116080 loss: 0.0019 lr: 0.02\n",
            "iteration: 116090 loss: 0.0022 lr: 0.02\n",
            "iteration: 116100 loss: 0.0017 lr: 0.02\n",
            "iteration: 116110 loss: 0.0015 lr: 0.02\n",
            "iteration: 116120 loss: 0.0017 lr: 0.02\n",
            "iteration: 116130 loss: 0.0016 lr: 0.02\n",
            "iteration: 116140 loss: 0.0020 lr: 0.02\n",
            "iteration: 116150 loss: 0.0017 lr: 0.02\n",
            "iteration: 116160 loss: 0.0016 lr: 0.02\n",
            "iteration: 116170 loss: 0.0016 lr: 0.02\n",
            "iteration: 116180 loss: 0.0016 lr: 0.02\n",
            "iteration: 116190 loss: 0.0014 lr: 0.02\n",
            "iteration: 116200 loss: 0.0016 lr: 0.02\n",
            "iteration: 116210 loss: 0.0023 lr: 0.02\n",
            "iteration: 116220 loss: 0.0021 lr: 0.02\n",
            "iteration: 116230 loss: 0.0014 lr: 0.02\n",
            "iteration: 116240 loss: 0.0018 lr: 0.02\n",
            "iteration: 116250 loss: 0.0019 lr: 0.02\n",
            "iteration: 116260 loss: 0.0011 lr: 0.02\n",
            "iteration: 116270 loss: 0.0013 lr: 0.02\n",
            "iteration: 116280 loss: 0.0017 lr: 0.02\n",
            "iteration: 116290 loss: 0.0023 lr: 0.02\n",
            "iteration: 116300 loss: 0.0015 lr: 0.02\n",
            "iteration: 116310 loss: 0.0021 lr: 0.02\n",
            "iteration: 116320 loss: 0.0017 lr: 0.02\n",
            "iteration: 116330 loss: 0.0018 lr: 0.02\n",
            "iteration: 116340 loss: 0.0015 lr: 0.02\n",
            "iteration: 116350 loss: 0.0016 lr: 0.02\n",
            "iteration: 116360 loss: 0.0020 lr: 0.02\n",
            "iteration: 116370 loss: 0.0019 lr: 0.02\n",
            "iteration: 116380 loss: 0.0017 lr: 0.02\n",
            "iteration: 116390 loss: 0.0015 lr: 0.02\n",
            "iteration: 116400 loss: 0.0016 lr: 0.02\n",
            "iteration: 116410 loss: 0.0017 lr: 0.02\n",
            "iteration: 116420 loss: 0.0013 lr: 0.02\n",
            "iteration: 116430 loss: 0.0016 lr: 0.02\n",
            "iteration: 116440 loss: 0.0017 lr: 0.02\n",
            "iteration: 116450 loss: 0.0018 lr: 0.02\n",
            "iteration: 116460 loss: 0.0017 lr: 0.02\n",
            "iteration: 116470 loss: 0.0015 lr: 0.02\n",
            "iteration: 116480 loss: 0.0017 lr: 0.02\n",
            "iteration: 116490 loss: 0.0017 lr: 0.02\n",
            "iteration: 116500 loss: 0.0016 lr: 0.02\n",
            "iteration: 116510 loss: 0.0019 lr: 0.02\n",
            "iteration: 116520 loss: 0.0021 lr: 0.02\n",
            "iteration: 116530 loss: 0.0019 lr: 0.02\n",
            "iteration: 116540 loss: 0.0019 lr: 0.02\n",
            "iteration: 116550 loss: 0.0017 lr: 0.02\n",
            "iteration: 116560 loss: 0.0017 lr: 0.02\n",
            "iteration: 116570 loss: 0.0019 lr: 0.02\n",
            "iteration: 116580 loss: 0.0014 lr: 0.02\n",
            "iteration: 116590 loss: 0.0013 lr: 0.02\n",
            "iteration: 116600 loss: 0.0019 lr: 0.02\n",
            "iteration: 116610 loss: 0.0016 lr: 0.02\n",
            "iteration: 116620 loss: 0.0015 lr: 0.02\n",
            "iteration: 116630 loss: 0.0014 lr: 0.02\n",
            "iteration: 116640 loss: 0.0018 lr: 0.02\n",
            "iteration: 116650 loss: 0.0015 lr: 0.02\n",
            "iteration: 116660 loss: 0.0015 lr: 0.02\n",
            "iteration: 116670 loss: 0.0015 lr: 0.02\n",
            "iteration: 116680 loss: 0.0018 lr: 0.02\n",
            "iteration: 116690 loss: 0.0013 lr: 0.02\n",
            "iteration: 116700 loss: 0.0020 lr: 0.02\n",
            "iteration: 116710 loss: 0.0012 lr: 0.02\n",
            "iteration: 116720 loss: 0.0016 lr: 0.02\n",
            "iteration: 116730 loss: 0.0021 lr: 0.02\n",
            "iteration: 116740 loss: 0.0016 lr: 0.02\n",
            "iteration: 116750 loss: 0.0014 lr: 0.02\n",
            "iteration: 116760 loss: 0.0024 lr: 0.02\n",
            "iteration: 116770 loss: 0.0016 lr: 0.02\n",
            "iteration: 116780 loss: 0.0020 lr: 0.02\n",
            "iteration: 116790 loss: 0.0019 lr: 0.02\n",
            "iteration: 116800 loss: 0.0014 lr: 0.02\n",
            "iteration: 116810 loss: 0.0021 lr: 0.02\n",
            "iteration: 116820 loss: 0.0019 lr: 0.02\n",
            "iteration: 116830 loss: 0.0016 lr: 0.02\n",
            "iteration: 116840 loss: 0.0014 lr: 0.02\n",
            "iteration: 116850 loss: 0.0016 lr: 0.02\n",
            "iteration: 116860 loss: 0.0017 lr: 0.02\n",
            "iteration: 116870 loss: 0.0020 lr: 0.02\n",
            "iteration: 116880 loss: 0.0014 lr: 0.02\n",
            "iteration: 116890 loss: 0.0014 lr: 0.02\n",
            "iteration: 116900 loss: 0.0018 lr: 0.02\n",
            "iteration: 116910 loss: 0.0011 lr: 0.02\n",
            "iteration: 116920 loss: 0.0014 lr: 0.02\n",
            "iteration: 116930 loss: 0.0019 lr: 0.02\n",
            "iteration: 116940 loss: 0.0016 lr: 0.02\n",
            "iteration: 116950 loss: 0.0012 lr: 0.02\n",
            "iteration: 116960 loss: 0.0014 lr: 0.02\n",
            "iteration: 116970 loss: 0.0020 lr: 0.02\n",
            "iteration: 116980 loss: 0.0017 lr: 0.02\n",
            "iteration: 116990 loss: 0.0015 lr: 0.02\n",
            "iteration: 117000 loss: 0.0013 lr: 0.02\n",
            "iteration: 117010 loss: 0.0016 lr: 0.02\n",
            "iteration: 117020 loss: 0.0020 lr: 0.02\n",
            "iteration: 117030 loss: 0.0016 lr: 0.02\n",
            "iteration: 117040 loss: 0.0014 lr: 0.02\n",
            "iteration: 117050 loss: 0.0015 lr: 0.02\n",
            "iteration: 117060 loss: 0.0018 lr: 0.02\n",
            "iteration: 117070 loss: 0.0017 lr: 0.02\n",
            "iteration: 117080 loss: 0.0015 lr: 0.02\n",
            "iteration: 117090 loss: 0.0014 lr: 0.02\n",
            "iteration: 117100 loss: 0.0014 lr: 0.02\n",
            "iteration: 117110 loss: 0.0014 lr: 0.02\n",
            "iteration: 117120 loss: 0.0013 lr: 0.02\n",
            "iteration: 117130 loss: 0.0020 lr: 0.02\n",
            "iteration: 117140 loss: 0.0016 lr: 0.02\n",
            "iteration: 117150 loss: 0.0018 lr: 0.02\n",
            "iteration: 117160 loss: 0.0016 lr: 0.02\n",
            "iteration: 117170 loss: 0.0021 lr: 0.02\n",
            "iteration: 117180 loss: 0.0018 lr: 0.02\n",
            "iteration: 117190 loss: 0.0014 lr: 0.02\n",
            "iteration: 117200 loss: 0.0015 lr: 0.02\n",
            "iteration: 117210 loss: 0.0026 lr: 0.02\n",
            "iteration: 117220 loss: 0.0020 lr: 0.02\n",
            "iteration: 117230 loss: 0.0017 lr: 0.02\n",
            "iteration: 117240 loss: 0.0021 lr: 0.02\n",
            "iteration: 117250 loss: 0.0023 lr: 0.02\n",
            "iteration: 117260 loss: 0.0016 lr: 0.02\n",
            "iteration: 117270 loss: 0.0014 lr: 0.02\n",
            "iteration: 117280 loss: 0.0014 lr: 0.02\n",
            "iteration: 117290 loss: 0.0017 lr: 0.02\n",
            "iteration: 117300 loss: 0.0021 lr: 0.02\n",
            "iteration: 117310 loss: 0.0016 lr: 0.02\n",
            "iteration: 117320 loss: 0.0020 lr: 0.02\n",
            "iteration: 117330 loss: 0.0018 lr: 0.02\n",
            "iteration: 117340 loss: 0.0022 lr: 0.02\n",
            "iteration: 117350 loss: 0.0014 lr: 0.02\n",
            "iteration: 117360 loss: 0.0017 lr: 0.02\n",
            "iteration: 117370 loss: 0.0013 lr: 0.02\n",
            "iteration: 117380 loss: 0.0014 lr: 0.02\n",
            "iteration: 117390 loss: 0.0017 lr: 0.02\n",
            "iteration: 117400 loss: 0.0015 lr: 0.02\n",
            "iteration: 117410 loss: 0.0018 lr: 0.02\n",
            "iteration: 117420 loss: 0.0018 lr: 0.02\n",
            "iteration: 117430 loss: 0.0016 lr: 0.02\n",
            "iteration: 117440 loss: 0.0016 lr: 0.02\n",
            "iteration: 117450 loss: 0.0016 lr: 0.02\n",
            "iteration: 117460 loss: 0.0013 lr: 0.02\n",
            "iteration: 117470 loss: 0.0016 lr: 0.02\n",
            "iteration: 117480 loss: 0.0017 lr: 0.02\n",
            "iteration: 117490 loss: 0.0017 lr: 0.02\n",
            "iteration: 117500 loss: 0.0019 lr: 0.02\n",
            "iteration: 117510 loss: 0.0016 lr: 0.02\n",
            "iteration: 117520 loss: 0.0021 lr: 0.02\n",
            "iteration: 117530 loss: 0.0017 lr: 0.02\n",
            "iteration: 117540 loss: 0.0015 lr: 0.02\n",
            "iteration: 117550 loss: 0.0023 lr: 0.02\n",
            "iteration: 117560 loss: 0.0016 lr: 0.02\n",
            "iteration: 117570 loss: 0.0016 lr: 0.02\n",
            "iteration: 117580 loss: 0.0014 lr: 0.02\n",
            "iteration: 117590 loss: 0.0014 lr: 0.02\n",
            "iteration: 117600 loss: 0.0019 lr: 0.02\n",
            "iteration: 117610 loss: 0.0017 lr: 0.02\n",
            "iteration: 117620 loss: 0.0018 lr: 0.02\n",
            "iteration: 117630 loss: 0.0017 lr: 0.02\n",
            "iteration: 117640 loss: 0.0016 lr: 0.02\n",
            "iteration: 117650 loss: 0.0020 lr: 0.02\n",
            "iteration: 117660 loss: 0.0018 lr: 0.02\n",
            "iteration: 117670 loss: 0.0017 lr: 0.02\n",
            "iteration: 117680 loss: 0.0019 lr: 0.02\n",
            "iteration: 117690 loss: 0.0017 lr: 0.02\n",
            "iteration: 117700 loss: 0.0019 lr: 0.02\n",
            "iteration: 117710 loss: 0.0015 lr: 0.02\n",
            "iteration: 117720 loss: 0.0014 lr: 0.02\n",
            "iteration: 117730 loss: 0.0017 lr: 0.02\n",
            "iteration: 117740 loss: 0.0014 lr: 0.02\n",
            "iteration: 117750 loss: 0.0017 lr: 0.02\n",
            "iteration: 117760 loss: 0.0014 lr: 0.02\n",
            "iteration: 117770 loss: 0.0015 lr: 0.02\n",
            "iteration: 117780 loss: 0.0014 lr: 0.02\n",
            "iteration: 117790 loss: 0.0024 lr: 0.02\n",
            "iteration: 117800 loss: 0.0019 lr: 0.02\n",
            "iteration: 117810 loss: 0.0014 lr: 0.02\n",
            "iteration: 117820 loss: 0.0019 lr: 0.02\n",
            "iteration: 117830 loss: 0.0013 lr: 0.02\n",
            "iteration: 117840 loss: 0.0017 lr: 0.02\n",
            "iteration: 117850 loss: 0.0019 lr: 0.02\n",
            "iteration: 117860 loss: 0.0017 lr: 0.02\n",
            "iteration: 117870 loss: 0.0015 lr: 0.02\n",
            "iteration: 117880 loss: 0.0020 lr: 0.02\n",
            "iteration: 117890 loss: 0.0016 lr: 0.02\n",
            "iteration: 117900 loss: 0.0014 lr: 0.02\n",
            "iteration: 117910 loss: 0.0026 lr: 0.02\n",
            "iteration: 117920 loss: 0.0014 lr: 0.02\n",
            "iteration: 117930 loss: 0.0016 lr: 0.02\n",
            "iteration: 117940 loss: 0.0015 lr: 0.02\n",
            "iteration: 117950 loss: 0.0018 lr: 0.02\n",
            "iteration: 117960 loss: 0.0014 lr: 0.02\n",
            "iteration: 117970 loss: 0.0014 lr: 0.02\n",
            "iteration: 117980 loss: 0.0024 lr: 0.02\n",
            "iteration: 117990 loss: 0.0017 lr: 0.02\n",
            "iteration: 118000 loss: 0.0017 lr: 0.02\n",
            "iteration: 118010 loss: 0.0022 lr: 0.02\n",
            "iteration: 118020 loss: 0.0015 lr: 0.02\n",
            "iteration: 118030 loss: 0.0021 lr: 0.02\n",
            "iteration: 118040 loss: 0.0015 lr: 0.02\n",
            "iteration: 118050 loss: 0.0018 lr: 0.02\n",
            "iteration: 118060 loss: 0.0014 lr: 0.02\n",
            "iteration: 118070 loss: 0.0019 lr: 0.02\n",
            "iteration: 118080 loss: 0.0015 lr: 0.02\n",
            "iteration: 118090 loss: 0.0015 lr: 0.02\n",
            "iteration: 118100 loss: 0.0017 lr: 0.02\n",
            "iteration: 118110 loss: 0.0014 lr: 0.02\n",
            "iteration: 118120 loss: 0.0017 lr: 0.02\n",
            "iteration: 118130 loss: 0.0012 lr: 0.02\n",
            "iteration: 118140 loss: 0.0021 lr: 0.02\n",
            "iteration: 118150 loss: 0.0016 lr: 0.02\n",
            "iteration: 118160 loss: 0.0014 lr: 0.02\n",
            "iteration: 118170 loss: 0.0018 lr: 0.02\n",
            "iteration: 118180 loss: 0.0014 lr: 0.02\n",
            "iteration: 118190 loss: 0.0016 lr: 0.02\n",
            "iteration: 118200 loss: 0.0021 lr: 0.02\n",
            "iteration: 118210 loss: 0.0013 lr: 0.02\n",
            "iteration: 118220 loss: 0.0016 lr: 0.02\n",
            "iteration: 118230 loss: 0.0017 lr: 0.02\n",
            "iteration: 118240 loss: 0.0015 lr: 0.02\n",
            "iteration: 118250 loss: 0.0020 lr: 0.02\n",
            "iteration: 118260 loss: 0.0012 lr: 0.02\n",
            "iteration: 118270 loss: 0.0020 lr: 0.02\n",
            "iteration: 118280 loss: 0.0020 lr: 0.02\n",
            "iteration: 118290 loss: 0.0016 lr: 0.02\n",
            "iteration: 118300 loss: 0.0018 lr: 0.02\n",
            "iteration: 118310 loss: 0.0014 lr: 0.02\n",
            "iteration: 118320 loss: 0.0018 lr: 0.02\n",
            "iteration: 118330 loss: 0.0015 lr: 0.02\n",
            "iteration: 118340 loss: 0.0015 lr: 0.02\n",
            "iteration: 118350 loss: 0.0014 lr: 0.02\n",
            "iteration: 118360 loss: 0.0014 lr: 0.02\n",
            "iteration: 118370 loss: 0.0016 lr: 0.02\n",
            "iteration: 118380 loss: 0.0017 lr: 0.02\n",
            "iteration: 118390 loss: 0.0014 lr: 0.02\n",
            "iteration: 118400 loss: 0.0018 lr: 0.02\n",
            "iteration: 118410 loss: 0.0026 lr: 0.02\n",
            "iteration: 118420 loss: 0.0017 lr: 0.02\n",
            "iteration: 118430 loss: 0.0017 lr: 0.02\n",
            "iteration: 118440 loss: 0.0015 lr: 0.02\n",
            "iteration: 118450 loss: 0.0013 lr: 0.02\n",
            "iteration: 118460 loss: 0.0020 lr: 0.02\n",
            "iteration: 118470 loss: 0.0015 lr: 0.02\n",
            "iteration: 118480 loss: 0.0015 lr: 0.02\n",
            "iteration: 118490 loss: 0.0017 lr: 0.02\n",
            "iteration: 118500 loss: 0.0015 lr: 0.02\n",
            "iteration: 118510 loss: 0.0014 lr: 0.02\n",
            "iteration: 118520 loss: 0.0016 lr: 0.02\n",
            "iteration: 118530 loss: 0.0021 lr: 0.02\n",
            "iteration: 118540 loss: 0.0018 lr: 0.02\n",
            "iteration: 118550 loss: 0.0022 lr: 0.02\n",
            "iteration: 118560 loss: 0.0020 lr: 0.02\n",
            "iteration: 118570 loss: 0.0019 lr: 0.02\n",
            "iteration: 118580 loss: 0.0018 lr: 0.02\n",
            "iteration: 118590 loss: 0.0015 lr: 0.02\n",
            "iteration: 118600 loss: 0.0016 lr: 0.02\n",
            "iteration: 118610 loss: 0.0012 lr: 0.02\n",
            "iteration: 118620 loss: 0.0017 lr: 0.02\n",
            "iteration: 118630 loss: 0.0018 lr: 0.02\n",
            "iteration: 118640 loss: 0.0015 lr: 0.02\n",
            "iteration: 118650 loss: 0.0019 lr: 0.02\n",
            "iteration: 118660 loss: 0.0016 lr: 0.02\n",
            "iteration: 118670 loss: 0.0014 lr: 0.02\n",
            "iteration: 118680 loss: 0.0018 lr: 0.02\n",
            "iteration: 118690 loss: 0.0016 lr: 0.02\n",
            "iteration: 118700 loss: 0.0016 lr: 0.02\n",
            "iteration: 118710 loss: 0.0014 lr: 0.02\n",
            "iteration: 118720 loss: 0.0017 lr: 0.02\n",
            "iteration: 118730 loss: 0.0015 lr: 0.02\n",
            "iteration: 118740 loss: 0.0016 lr: 0.02\n",
            "iteration: 118750 loss: 0.0014 lr: 0.02\n",
            "iteration: 118760 loss: 0.0022 lr: 0.02\n",
            "iteration: 118770 loss: 0.0020 lr: 0.02\n",
            "iteration: 118780 loss: 0.0017 lr: 0.02\n",
            "iteration: 118790 loss: 0.0013 lr: 0.02\n",
            "iteration: 118800 loss: 0.0021 lr: 0.02\n",
            "iteration: 118810 loss: 0.0019 lr: 0.02\n",
            "iteration: 118820 loss: 0.0016 lr: 0.02\n",
            "iteration: 118830 loss: 0.0012 lr: 0.02\n",
            "iteration: 118840 loss: 0.0017 lr: 0.02\n",
            "iteration: 118850 loss: 0.0017 lr: 0.02\n",
            "iteration: 118860 loss: 0.0016 lr: 0.02\n",
            "iteration: 118870 loss: 0.0022 lr: 0.02\n",
            "iteration: 118880 loss: 0.0021 lr: 0.02\n",
            "iteration: 118890 loss: 0.0017 lr: 0.02\n",
            "iteration: 118900 loss: 0.0020 lr: 0.02\n",
            "iteration: 118910 loss: 0.0014 lr: 0.02\n",
            "iteration: 118920 loss: 0.0024 lr: 0.02\n",
            "iteration: 118930 loss: 0.0017 lr: 0.02\n",
            "iteration: 118940 loss: 0.0023 lr: 0.02\n",
            "iteration: 118950 loss: 0.0013 lr: 0.02\n",
            "iteration: 118960 loss: 0.0019 lr: 0.02\n",
            "iteration: 118970 loss: 0.0015 lr: 0.02\n",
            "iteration: 118980 loss: 0.0016 lr: 0.02\n",
            "iteration: 118990 loss: 0.0016 lr: 0.02\n",
            "iteration: 119000 loss: 0.0011 lr: 0.02\n",
            "iteration: 119010 loss: 0.0018 lr: 0.02\n",
            "iteration: 119020 loss: 0.0019 lr: 0.02\n",
            "iteration: 119030 loss: 0.0017 lr: 0.02\n",
            "iteration: 119040 loss: 0.0015 lr: 0.02\n",
            "iteration: 119050 loss: 0.0020 lr: 0.02\n",
            "iteration: 119060 loss: 0.0015 lr: 0.02\n",
            "iteration: 119070 loss: 0.0018 lr: 0.02\n",
            "iteration: 119080 loss: 0.0017 lr: 0.02\n",
            "iteration: 119090 loss: 0.0016 lr: 0.02\n",
            "iteration: 119100 loss: 0.0018 lr: 0.02\n",
            "iteration: 119110 loss: 0.0015 lr: 0.02\n",
            "iteration: 119120 loss: 0.0014 lr: 0.02\n",
            "iteration: 119130 loss: 0.0017 lr: 0.02\n",
            "iteration: 119140 loss: 0.0015 lr: 0.02\n",
            "iteration: 119150 loss: 0.0013 lr: 0.02\n",
            "iteration: 119160 loss: 0.0020 lr: 0.02\n",
            "iteration: 119170 loss: 0.0016 lr: 0.02\n",
            "iteration: 119180 loss: 0.0020 lr: 0.02\n",
            "iteration: 119190 loss: 0.0019 lr: 0.02\n",
            "iteration: 119200 loss: 0.0015 lr: 0.02\n",
            "iteration: 119210 loss: 0.0020 lr: 0.02\n",
            "iteration: 119220 loss: 0.0019 lr: 0.02\n",
            "iteration: 119230 loss: 0.0022 lr: 0.02\n",
            "iteration: 119240 loss: 0.0017 lr: 0.02\n",
            "iteration: 119250 loss: 0.0018 lr: 0.02\n",
            "iteration: 119260 loss: 0.0018 lr: 0.02\n",
            "iteration: 119270 loss: 0.0024 lr: 0.02\n",
            "iteration: 119280 loss: 0.0018 lr: 0.02\n",
            "iteration: 119290 loss: 0.0019 lr: 0.02\n",
            "iteration: 119300 loss: 0.0019 lr: 0.02\n",
            "iteration: 119310 loss: 0.0020 lr: 0.02\n",
            "iteration: 119320 loss: 0.0023 lr: 0.02\n",
            "iteration: 119330 loss: 0.0016 lr: 0.02\n",
            "iteration: 119340 loss: 0.0017 lr: 0.02\n",
            "iteration: 119350 loss: 0.0016 lr: 0.02\n",
            "iteration: 119360 loss: 0.0017 lr: 0.02\n",
            "iteration: 119370 loss: 0.0017 lr: 0.02\n",
            "iteration: 119380 loss: 0.0017 lr: 0.02\n",
            "iteration: 119390 loss: 0.0019 lr: 0.02\n",
            "iteration: 119400 loss: 0.0015 lr: 0.02\n",
            "iteration: 119410 loss: 0.0017 lr: 0.02\n",
            "iteration: 119420 loss: 0.0021 lr: 0.02\n",
            "iteration: 119430 loss: 0.0017 lr: 0.02\n",
            "iteration: 119440 loss: 0.0020 lr: 0.02\n",
            "iteration: 119450 loss: 0.0013 lr: 0.02\n",
            "iteration: 119460 loss: 0.0014 lr: 0.02\n",
            "iteration: 119470 loss: 0.0018 lr: 0.02\n",
            "iteration: 119480 loss: 0.0014 lr: 0.02\n",
            "iteration: 119490 loss: 0.0017 lr: 0.02\n",
            "iteration: 119500 loss: 0.0016 lr: 0.02\n",
            "iteration: 119510 loss: 0.0016 lr: 0.02\n",
            "iteration: 119520 loss: 0.0017 lr: 0.02\n",
            "iteration: 119530 loss: 0.0019 lr: 0.02\n",
            "iteration: 119540 loss: 0.0017 lr: 0.02\n",
            "iteration: 119550 loss: 0.0015 lr: 0.02\n",
            "iteration: 119560 loss: 0.0018 lr: 0.02\n",
            "iteration: 119570 loss: 0.0017 lr: 0.02\n",
            "iteration: 119580 loss: 0.0021 lr: 0.02\n",
            "iteration: 119590 loss: 0.0018 lr: 0.02\n",
            "iteration: 119600 loss: 0.0015 lr: 0.02\n",
            "iteration: 119610 loss: 0.0017 lr: 0.02\n",
            "iteration: 119620 loss: 0.0013 lr: 0.02\n",
            "iteration: 119630 loss: 0.0014 lr: 0.02\n",
            "iteration: 119640 loss: 0.0015 lr: 0.02\n",
            "iteration: 119650 loss: 0.0014 lr: 0.02\n",
            "iteration: 119660 loss: 0.0016 lr: 0.02\n",
            "iteration: 119670 loss: 0.0014 lr: 0.02\n",
            "iteration: 119680 loss: 0.0014 lr: 0.02\n",
            "iteration: 119690 loss: 0.0013 lr: 0.02\n",
            "iteration: 119700 loss: 0.0013 lr: 0.02\n",
            "iteration: 119710 loss: 0.0018 lr: 0.02\n",
            "iteration: 119720 loss: 0.0016 lr: 0.02\n",
            "iteration: 119730 loss: 0.0015 lr: 0.02\n",
            "iteration: 119740 loss: 0.0016 lr: 0.02\n",
            "iteration: 119750 loss: 0.0017 lr: 0.02\n",
            "iteration: 119760 loss: 0.0012 lr: 0.02\n",
            "iteration: 119770 loss: 0.0019 lr: 0.02\n",
            "iteration: 119780 loss: 0.0020 lr: 0.02\n",
            "iteration: 119790 loss: 0.0019 lr: 0.02\n",
            "iteration: 119800 loss: 0.0014 lr: 0.02\n",
            "iteration: 119810 loss: 0.0017 lr: 0.02\n",
            "iteration: 119820 loss: 0.0014 lr: 0.02\n",
            "iteration: 119830 loss: 0.0018 lr: 0.02\n",
            "iteration: 119840 loss: 0.0016 lr: 0.02\n",
            "iteration: 119850 loss: 0.0013 lr: 0.02\n",
            "iteration: 119860 loss: 0.0014 lr: 0.02\n",
            "iteration: 119870 loss: 0.0016 lr: 0.02\n",
            "iteration: 119880 loss: 0.0025 lr: 0.02\n",
            "iteration: 119890 loss: 0.0016 lr: 0.02\n",
            "iteration: 119900 loss: 0.0018 lr: 0.02\n",
            "iteration: 119910 loss: 0.0015 lr: 0.02\n",
            "iteration: 119920 loss: 0.0017 lr: 0.02\n",
            "iteration: 119930 loss: 0.0018 lr: 0.02\n",
            "iteration: 119940 loss: 0.0019 lr: 0.02\n",
            "iteration: 119950 loss: 0.0017 lr: 0.02\n",
            "iteration: 119960 loss: 0.0013 lr: 0.02\n",
            "iteration: 119970 loss: 0.0016 lr: 0.02\n",
            "iteration: 119980 loss: 0.0015 lr: 0.02\n",
            "iteration: 119990 loss: 0.0015 lr: 0.02\n",
            "iteration: 120000 loss: 0.0016 lr: 0.02\n",
            "iteration: 120010 loss: 0.0019 lr: 0.02\n",
            "iteration: 120020 loss: 0.0015 lr: 0.02\n",
            "iteration: 120030 loss: 0.0019 lr: 0.02\n",
            "iteration: 120040 loss: 0.0017 lr: 0.02\n",
            "iteration: 120050 loss: 0.0016 lr: 0.02\n",
            "iteration: 120060 loss: 0.0014 lr: 0.02\n",
            "iteration: 120070 loss: 0.0018 lr: 0.02\n",
            "iteration: 120080 loss: 0.0013 lr: 0.02\n",
            "iteration: 120090 loss: 0.0016 lr: 0.02\n",
            "iteration: 120100 loss: 0.0015 lr: 0.02\n",
            "iteration: 120110 loss: 0.0014 lr: 0.02\n",
            "iteration: 120120 loss: 0.0013 lr: 0.02\n",
            "iteration: 120130 loss: 0.0016 lr: 0.02\n",
            "iteration: 120140 loss: 0.0017 lr: 0.02\n",
            "iteration: 120150 loss: 0.0013 lr: 0.02\n",
            "iteration: 120160 loss: 0.0026 lr: 0.02\n",
            "iteration: 120170 loss: 0.0020 lr: 0.02\n",
            "iteration: 120180 loss: 0.0017 lr: 0.02\n",
            "iteration: 120190 loss: 0.0021 lr: 0.02\n",
            "iteration: 120200 loss: 0.0022 lr: 0.02\n",
            "iteration: 120210 loss: 0.0016 lr: 0.02\n",
            "iteration: 120220 loss: 0.0016 lr: 0.02\n",
            "iteration: 120230 loss: 0.0019 lr: 0.02\n",
            "iteration: 120240 loss: 0.0014 lr: 0.02\n",
            "iteration: 120250 loss: 0.0015 lr: 0.02\n",
            "iteration: 120260 loss: 0.0018 lr: 0.02\n",
            "iteration: 120270 loss: 0.0017 lr: 0.02\n",
            "iteration: 120280 loss: 0.0017 lr: 0.02\n",
            "iteration: 120290 loss: 0.0016 lr: 0.02\n",
            "iteration: 120300 loss: 0.0016 lr: 0.02\n",
            "iteration: 120310 loss: 0.0017 lr: 0.02\n",
            "iteration: 120320 loss: 0.0017 lr: 0.02\n",
            "iteration: 120330 loss: 0.0016 lr: 0.02\n",
            "iteration: 120340 loss: 0.0017 lr: 0.02\n",
            "iteration: 120350 loss: 0.0014 lr: 0.02\n",
            "iteration: 120360 loss: 0.0016 lr: 0.02\n",
            "iteration: 120370 loss: 0.0020 lr: 0.02\n",
            "iteration: 120380 loss: 0.0016 lr: 0.02\n",
            "iteration: 120390 loss: 0.0014 lr: 0.02\n",
            "iteration: 120400 loss: 0.0015 lr: 0.02\n",
            "iteration: 120410 loss: 0.0015 lr: 0.02\n",
            "iteration: 120420 loss: 0.0015 lr: 0.02\n",
            "iteration: 120430 loss: 0.0019 lr: 0.02\n",
            "iteration: 120440 loss: 0.0019 lr: 0.02\n",
            "iteration: 120450 loss: 0.0017 lr: 0.02\n",
            "iteration: 120460 loss: 0.0012 lr: 0.02\n",
            "iteration: 120470 loss: 0.0014 lr: 0.02\n",
            "iteration: 120480 loss: 0.0014 lr: 0.02\n",
            "iteration: 120490 loss: 0.0014 lr: 0.02\n",
            "iteration: 120500 loss: 0.0014 lr: 0.02\n",
            "iteration: 120510 loss: 0.0015 lr: 0.02\n",
            "iteration: 120520 loss: 0.0017 lr: 0.02\n",
            "iteration: 120530 loss: 0.0015 lr: 0.02\n",
            "iteration: 120540 loss: 0.0020 lr: 0.02\n",
            "iteration: 120550 loss: 0.0017 lr: 0.02\n",
            "iteration: 120560 loss: 0.0016 lr: 0.02\n",
            "iteration: 120570 loss: 0.0017 lr: 0.02\n",
            "iteration: 120580 loss: 0.0013 lr: 0.02\n",
            "iteration: 120590 loss: 0.0017 lr: 0.02\n",
            "iteration: 120600 loss: 0.0015 lr: 0.02\n",
            "iteration: 120610 loss: 0.0021 lr: 0.02\n",
            "iteration: 120620 loss: 0.0015 lr: 0.02\n",
            "iteration: 120630 loss: 0.0021 lr: 0.02\n",
            "iteration: 120640 loss: 0.0013 lr: 0.02\n",
            "iteration: 120650 loss: 0.0019 lr: 0.02\n",
            "iteration: 120660 loss: 0.0019 lr: 0.02\n",
            "iteration: 120670 loss: 0.0015 lr: 0.02\n",
            "iteration: 120680 loss: 0.0014 lr: 0.02\n",
            "iteration: 120690 loss: 0.0018 lr: 0.02\n",
            "iteration: 120700 loss: 0.0013 lr: 0.02\n",
            "iteration: 120710 loss: 0.0017 lr: 0.02\n",
            "iteration: 120720 loss: 0.0016 lr: 0.02\n",
            "iteration: 120730 loss: 0.0015 lr: 0.02\n",
            "iteration: 120740 loss: 0.0016 lr: 0.02\n",
            "iteration: 120750 loss: 0.0014 lr: 0.02\n",
            "iteration: 120760 loss: 0.0014 lr: 0.02\n",
            "iteration: 120770 loss: 0.0020 lr: 0.02\n",
            "iteration: 120780 loss: 0.0014 lr: 0.02\n",
            "iteration: 120790 loss: 0.0023 lr: 0.02\n",
            "iteration: 120800 loss: 0.0020 lr: 0.02\n",
            "iteration: 120810 loss: 0.0020 lr: 0.02\n",
            "iteration: 120820 loss: 0.0018 lr: 0.02\n",
            "iteration: 120830 loss: 0.0015 lr: 0.02\n",
            "iteration: 120840 loss: 0.0015 lr: 0.02\n",
            "iteration: 120850 loss: 0.0019 lr: 0.02\n",
            "iteration: 120860 loss: 0.0018 lr: 0.02\n",
            "iteration: 120870 loss: 0.0018 lr: 0.02\n",
            "iteration: 120880 loss: 0.0021 lr: 0.02\n",
            "iteration: 120890 loss: 0.0016 lr: 0.02\n",
            "iteration: 120900 loss: 0.0021 lr: 0.02\n",
            "iteration: 120910 loss: 0.0017 lr: 0.02\n",
            "iteration: 120920 loss: 0.0015 lr: 0.02\n",
            "iteration: 120930 loss: 0.0015 lr: 0.02\n",
            "iteration: 120940 loss: 0.0017 lr: 0.02\n",
            "iteration: 120950 loss: 0.0015 lr: 0.02\n",
            "iteration: 120960 loss: 0.0016 lr: 0.02\n",
            "iteration: 120970 loss: 0.0023 lr: 0.02\n",
            "iteration: 120980 loss: 0.0014 lr: 0.02\n",
            "iteration: 120990 loss: 0.0018 lr: 0.02\n",
            "iteration: 121000 loss: 0.0016 lr: 0.02\n",
            "iteration: 121010 loss: 0.0017 lr: 0.02\n",
            "iteration: 121020 loss: 0.0030 lr: 0.02\n",
            "iteration: 121030 loss: 0.0015 lr: 0.02\n",
            "iteration: 121040 loss: 0.0018 lr: 0.02\n",
            "iteration: 121050 loss: 0.0016 lr: 0.02\n",
            "iteration: 121060 loss: 0.0019 lr: 0.02\n",
            "iteration: 121070 loss: 0.0014 lr: 0.02\n",
            "iteration: 121080 loss: 0.0020 lr: 0.02\n",
            "iteration: 121090 loss: 0.0015 lr: 0.02\n",
            "iteration: 121100 loss: 0.0016 lr: 0.02\n",
            "iteration: 121110 loss: 0.0016 lr: 0.02\n",
            "iteration: 121120 loss: 0.0017 lr: 0.02\n",
            "iteration: 121130 loss: 0.0020 lr: 0.02\n",
            "iteration: 121140 loss: 0.0022 lr: 0.02\n",
            "iteration: 121150 loss: 0.0020 lr: 0.02\n",
            "iteration: 121160 loss: 0.0016 lr: 0.02\n",
            "iteration: 121170 loss: 0.0013 lr: 0.02\n",
            "iteration: 121180 loss: 0.0015 lr: 0.02\n",
            "iteration: 121190 loss: 0.0016 lr: 0.02\n",
            "iteration: 121200 loss: 0.0015 lr: 0.02\n",
            "iteration: 121210 loss: 0.0025 lr: 0.02\n",
            "iteration: 121220 loss: 0.0017 lr: 0.02\n",
            "iteration: 121230 loss: 0.0018 lr: 0.02\n",
            "iteration: 121240 loss: 0.0016 lr: 0.02\n",
            "iteration: 121250 loss: 0.0017 lr: 0.02\n",
            "iteration: 121260 loss: 0.0015 lr: 0.02\n",
            "iteration: 121270 loss: 0.0016 lr: 0.02\n",
            "iteration: 121280 loss: 0.0018 lr: 0.02\n",
            "iteration: 121290 loss: 0.0021 lr: 0.02\n",
            "iteration: 121300 loss: 0.0015 lr: 0.02\n",
            "iteration: 121310 loss: 0.0017 lr: 0.02\n",
            "iteration: 121320 loss: 0.0015 lr: 0.02\n",
            "iteration: 121330 loss: 0.0015 lr: 0.02\n",
            "iteration: 121340 loss: 0.0016 lr: 0.02\n",
            "iteration: 121350 loss: 0.0016 lr: 0.02\n",
            "iteration: 121360 loss: 0.0017 lr: 0.02\n",
            "iteration: 121370 loss: 0.0016 lr: 0.02\n",
            "iteration: 121380 loss: 0.0020 lr: 0.02\n",
            "iteration: 121390 loss: 0.0019 lr: 0.02\n",
            "iteration: 121400 loss: 0.0010 lr: 0.02\n",
            "iteration: 121410 loss: 0.0016 lr: 0.02\n",
            "iteration: 121420 loss: 0.0019 lr: 0.02\n",
            "iteration: 121430 loss: 0.0013 lr: 0.02\n",
            "iteration: 121440 loss: 0.0019 lr: 0.02\n",
            "iteration: 121450 loss: 0.0015 lr: 0.02\n",
            "iteration: 121460 loss: 0.0020 lr: 0.02\n",
            "iteration: 121470 loss: 0.0019 lr: 0.02\n",
            "iteration: 121480 loss: 0.0015 lr: 0.02\n",
            "iteration: 121490 loss: 0.0018 lr: 0.02\n",
            "iteration: 121500 loss: 0.0016 lr: 0.02\n",
            "iteration: 121510 loss: 0.0016 lr: 0.02\n",
            "iteration: 121520 loss: 0.0015 lr: 0.02\n",
            "iteration: 121530 loss: 0.0015 lr: 0.02\n",
            "iteration: 121540 loss: 0.0015 lr: 0.02\n",
            "iteration: 121550 loss: 0.0018 lr: 0.02\n",
            "iteration: 121560 loss: 0.0018 lr: 0.02\n",
            "iteration: 121570 loss: 0.0014 lr: 0.02\n",
            "iteration: 121580 loss: 0.0014 lr: 0.02\n",
            "iteration: 121590 loss: 0.0019 lr: 0.02\n",
            "iteration: 121600 loss: 0.0014 lr: 0.02\n",
            "iteration: 121610 loss: 0.0017 lr: 0.02\n",
            "iteration: 121620 loss: 0.0019 lr: 0.02\n",
            "iteration: 121630 loss: 0.0016 lr: 0.02\n",
            "iteration: 121640 loss: 0.0017 lr: 0.02\n",
            "iteration: 121650 loss: 0.0013 lr: 0.02\n",
            "iteration: 121660 loss: 0.0020 lr: 0.02\n",
            "iteration: 121670 loss: 0.0018 lr: 0.02\n",
            "iteration: 121680 loss: 0.0015 lr: 0.02\n",
            "iteration: 121690 loss: 0.0016 lr: 0.02\n",
            "iteration: 121700 loss: 0.0016 lr: 0.02\n",
            "iteration: 121710 loss: 0.0016 lr: 0.02\n",
            "iteration: 121720 loss: 0.0012 lr: 0.02\n",
            "iteration: 121730 loss: 0.0015 lr: 0.02\n",
            "iteration: 121740 loss: 0.0015 lr: 0.02\n",
            "iteration: 121750 loss: 0.0016 lr: 0.02\n",
            "iteration: 121760 loss: 0.0018 lr: 0.02\n",
            "iteration: 121770 loss: 0.0013 lr: 0.02\n",
            "iteration: 121780 loss: 0.0019 lr: 0.02\n",
            "iteration: 121790 loss: 0.0020 lr: 0.02\n",
            "iteration: 121800 loss: 0.0019 lr: 0.02\n",
            "iteration: 121810 loss: 0.0016 lr: 0.02\n",
            "iteration: 121820 loss: 0.0015 lr: 0.02\n",
            "iteration: 121830 loss: 0.0016 lr: 0.02\n",
            "iteration: 121840 loss: 0.0012 lr: 0.02\n",
            "iteration: 121850 loss: 0.0013 lr: 0.02\n",
            "iteration: 121860 loss: 0.0013 lr: 0.02\n",
            "iteration: 121870 loss: 0.0018 lr: 0.02\n",
            "iteration: 121880 loss: 0.0020 lr: 0.02\n",
            "iteration: 121890 loss: 0.0018 lr: 0.02\n",
            "iteration: 121900 loss: 0.0015 lr: 0.02\n",
            "iteration: 121910 loss: 0.0014 lr: 0.02\n",
            "iteration: 121920 loss: 0.0018 lr: 0.02\n",
            "iteration: 121930 loss: 0.0018 lr: 0.02\n",
            "iteration: 121940 loss: 0.0017 lr: 0.02\n",
            "iteration: 121950 loss: 0.0016 lr: 0.02\n",
            "iteration: 121960 loss: 0.0019 lr: 0.02\n",
            "iteration: 121970 loss: 0.0016 lr: 0.02\n",
            "iteration: 121980 loss: 0.0018 lr: 0.02\n",
            "iteration: 121990 loss: 0.0020 lr: 0.02\n",
            "iteration: 122000 loss: 0.0019 lr: 0.02\n",
            "iteration: 122010 loss: 0.0017 lr: 0.02\n",
            "iteration: 122020 loss: 0.0016 lr: 0.02\n",
            "iteration: 122030 loss: 0.0023 lr: 0.02\n",
            "iteration: 122040 loss: 0.0018 lr: 0.02\n",
            "iteration: 122050 loss: 0.0013 lr: 0.02\n",
            "iteration: 122060 loss: 0.0018 lr: 0.02\n",
            "iteration: 122070 loss: 0.0015 lr: 0.02\n",
            "iteration: 122080 loss: 0.0017 lr: 0.02\n",
            "iteration: 122090 loss: 0.0015 lr: 0.02\n",
            "iteration: 122100 loss: 0.0016 lr: 0.02\n",
            "iteration: 122110 loss: 0.0015 lr: 0.02\n",
            "iteration: 122120 loss: 0.0014 lr: 0.02\n",
            "iteration: 122130 loss: 0.0015 lr: 0.02\n",
            "iteration: 122140 loss: 0.0018 lr: 0.02\n",
            "iteration: 122150 loss: 0.0015 lr: 0.02\n",
            "iteration: 122160 loss: 0.0020 lr: 0.02\n",
            "iteration: 122170 loss: 0.0021 lr: 0.02\n",
            "iteration: 122180 loss: 0.0019 lr: 0.02\n",
            "iteration: 122190 loss: 0.0020 lr: 0.02\n",
            "iteration: 122200 loss: 0.0014 lr: 0.02\n",
            "iteration: 122210 loss: 0.0018 lr: 0.02\n",
            "iteration: 122220 loss: 0.0019 lr: 0.02\n",
            "iteration: 122230 loss: 0.0014 lr: 0.02\n",
            "iteration: 122240 loss: 0.0014 lr: 0.02\n",
            "iteration: 122250 loss: 0.0015 lr: 0.02\n",
            "iteration: 122260 loss: 0.0020 lr: 0.02\n",
            "iteration: 122270 loss: 0.0016 lr: 0.02\n",
            "iteration: 122280 loss: 0.0016 lr: 0.02\n",
            "iteration: 122290 loss: 0.0017 lr: 0.02\n",
            "iteration: 122300 loss: 0.0019 lr: 0.02\n",
            "iteration: 122310 loss: 0.0017 lr: 0.02\n",
            "iteration: 122320 loss: 0.0017 lr: 0.02\n",
            "iteration: 122330 loss: 0.0016 lr: 0.02\n",
            "iteration: 122340 loss: 0.0016 lr: 0.02\n",
            "iteration: 122350 loss: 0.0020 lr: 0.02\n",
            "iteration: 122360 loss: 0.0022 lr: 0.02\n",
            "iteration: 122370 loss: 0.0014 lr: 0.02\n",
            "iteration: 122380 loss: 0.0014 lr: 0.02\n",
            "iteration: 122390 loss: 0.0020 lr: 0.02\n",
            "iteration: 122400 loss: 0.0015 lr: 0.02\n",
            "iteration: 122410 loss: 0.0017 lr: 0.02\n",
            "iteration: 122420 loss: 0.0015 lr: 0.02\n",
            "iteration: 122430 loss: 0.0014 lr: 0.02\n",
            "iteration: 122440 loss: 0.0016 lr: 0.02\n",
            "iteration: 122450 loss: 0.0012 lr: 0.02\n",
            "iteration: 122460 loss: 0.0016 lr: 0.02\n",
            "iteration: 122470 loss: 0.0015 lr: 0.02\n",
            "iteration: 122480 loss: 0.0019 lr: 0.02\n",
            "iteration: 122490 loss: 0.0017 lr: 0.02\n",
            "iteration: 122500 loss: 0.0017 lr: 0.02\n",
            "iteration: 122510 loss: 0.0020 lr: 0.02\n",
            "iteration: 122520 loss: 0.0020 lr: 0.02\n",
            "iteration: 122530 loss: 0.0017 lr: 0.02\n",
            "iteration: 122540 loss: 0.0020 lr: 0.02\n",
            "iteration: 122550 loss: 0.0017 lr: 0.02\n",
            "iteration: 122560 loss: 0.0021 lr: 0.02\n",
            "iteration: 122570 loss: 0.0013 lr: 0.02\n",
            "iteration: 122580 loss: 0.0016 lr: 0.02\n",
            "iteration: 122590 loss: 0.0013 lr: 0.02\n",
            "iteration: 122600 loss: 0.0018 lr: 0.02\n",
            "iteration: 122610 loss: 0.0014 lr: 0.02\n",
            "iteration: 122620 loss: 0.0024 lr: 0.02\n",
            "iteration: 122630 loss: 0.0013 lr: 0.02\n",
            "iteration: 122640 loss: 0.0018 lr: 0.02\n",
            "iteration: 122650 loss: 0.0023 lr: 0.02\n",
            "iteration: 122660 loss: 0.0013 lr: 0.02\n",
            "iteration: 122670 loss: 0.0014 lr: 0.02\n",
            "iteration: 122680 loss: 0.0015 lr: 0.02\n",
            "iteration: 122690 loss: 0.0017 lr: 0.02\n",
            "iteration: 122700 loss: 0.0014 lr: 0.02\n",
            "iteration: 122710 loss: 0.0021 lr: 0.02\n",
            "iteration: 122720 loss: 0.0014 lr: 0.02\n",
            "iteration: 122730 loss: 0.0019 lr: 0.02\n",
            "iteration: 122740 loss: 0.0018 lr: 0.02\n",
            "iteration: 122750 loss: 0.0020 lr: 0.02\n",
            "iteration: 122760 loss: 0.0020 lr: 0.02\n",
            "iteration: 122770 loss: 0.0017 lr: 0.02\n",
            "iteration: 122780 loss: 0.0021 lr: 0.02\n",
            "iteration: 122790 loss: 0.0017 lr: 0.02\n",
            "iteration: 122800 loss: 0.0015 lr: 0.02\n",
            "iteration: 122810 loss: 0.0017 lr: 0.02\n",
            "iteration: 122820 loss: 0.0017 lr: 0.02\n",
            "iteration: 122830 loss: 0.0019 lr: 0.02\n",
            "iteration: 122840 loss: 0.0016 lr: 0.02\n",
            "iteration: 122850 loss: 0.0015 lr: 0.02\n",
            "iteration: 122860 loss: 0.0013 lr: 0.02\n",
            "iteration: 122870 loss: 0.0018 lr: 0.02\n",
            "iteration: 122880 loss: 0.0017 lr: 0.02\n",
            "iteration: 122890 loss: 0.0023 lr: 0.02\n",
            "iteration: 122900 loss: 0.0015 lr: 0.02\n",
            "iteration: 122910 loss: 0.0019 lr: 0.02\n",
            "iteration: 122920 loss: 0.0018 lr: 0.02\n",
            "iteration: 122930 loss: 0.0012 lr: 0.02\n",
            "iteration: 122940 loss: 0.0016 lr: 0.02\n",
            "iteration: 122950 loss: 0.0014 lr: 0.02\n",
            "iteration: 122960 loss: 0.0019 lr: 0.02\n",
            "iteration: 122970 loss: 0.0028 lr: 0.02\n",
            "iteration: 122980 loss: 0.0016 lr: 0.02\n",
            "iteration: 122990 loss: 0.0023 lr: 0.02\n",
            "iteration: 123000 loss: 0.0019 lr: 0.02\n",
            "iteration: 123010 loss: 0.0016 lr: 0.02\n",
            "iteration: 123020 loss: 0.0016 lr: 0.02\n",
            "iteration: 123030 loss: 0.0018 lr: 0.02\n",
            "iteration: 123040 loss: 0.0019 lr: 0.02\n",
            "iteration: 123050 loss: 0.0020 lr: 0.02\n",
            "iteration: 123060 loss: 0.0014 lr: 0.02\n",
            "iteration: 123070 loss: 0.0016 lr: 0.02\n",
            "iteration: 123080 loss: 0.0022 lr: 0.02\n",
            "iteration: 123090 loss: 0.0017 lr: 0.02\n",
            "iteration: 123100 loss: 0.0018 lr: 0.02\n",
            "iteration: 123110 loss: 0.0018 lr: 0.02\n",
            "iteration: 123120 loss: 0.0017 lr: 0.02\n",
            "iteration: 123130 loss: 0.0016 lr: 0.02\n",
            "iteration: 123140 loss: 0.0017 lr: 0.02\n",
            "iteration: 123150 loss: 0.0017 lr: 0.02\n",
            "iteration: 123160 loss: 0.0017 lr: 0.02\n",
            "iteration: 123170 loss: 0.0021 lr: 0.02\n",
            "iteration: 123180 loss: 0.0019 lr: 0.02\n",
            "iteration: 123190 loss: 0.0018 lr: 0.02\n",
            "iteration: 123200 loss: 0.0017 lr: 0.02\n",
            "iteration: 123210 loss: 0.0011 lr: 0.02\n",
            "iteration: 123220 loss: 0.0017 lr: 0.02\n",
            "iteration: 123230 loss: 0.0022 lr: 0.02\n",
            "iteration: 123240 loss: 0.0016 lr: 0.02\n",
            "iteration: 123250 loss: 0.0016 lr: 0.02\n",
            "iteration: 123260 loss: 0.0021 lr: 0.02\n",
            "iteration: 123270 loss: 0.0015 lr: 0.02\n",
            "iteration: 123280 loss: 0.0017 lr: 0.02\n",
            "iteration: 123290 loss: 0.0018 lr: 0.02\n",
            "iteration: 123300 loss: 0.0019 lr: 0.02\n",
            "iteration: 123310 loss: 0.0021 lr: 0.02\n",
            "iteration: 123320 loss: 0.0019 lr: 0.02\n",
            "iteration: 123330 loss: 0.0013 lr: 0.02\n",
            "iteration: 123340 loss: 0.0013 lr: 0.02\n",
            "iteration: 123350 loss: 0.0016 lr: 0.02\n",
            "iteration: 123360 loss: 0.0018 lr: 0.02\n",
            "iteration: 123370 loss: 0.0016 lr: 0.02\n",
            "iteration: 123380 loss: 0.0020 lr: 0.02\n",
            "iteration: 123390 loss: 0.0017 lr: 0.02\n",
            "iteration: 123400 loss: 0.0022 lr: 0.02\n",
            "iteration: 123410 loss: 0.0020 lr: 0.02\n",
            "iteration: 123420 loss: 0.0015 lr: 0.02\n",
            "iteration: 123430 loss: 0.0015 lr: 0.02\n",
            "iteration: 123440 loss: 0.0019 lr: 0.02\n",
            "iteration: 123450 loss: 0.0017 lr: 0.02\n",
            "iteration: 123460 loss: 0.0017 lr: 0.02\n",
            "iteration: 123470 loss: 0.0018 lr: 0.02\n",
            "iteration: 123480 loss: 0.0018 lr: 0.02\n",
            "iteration: 123490 loss: 0.0018 lr: 0.02\n",
            "iteration: 123500 loss: 0.0015 lr: 0.02\n",
            "iteration: 123510 loss: 0.0019 lr: 0.02\n",
            "iteration: 123520 loss: 0.0014 lr: 0.02\n",
            "iteration: 123530 loss: 0.0014 lr: 0.02\n",
            "iteration: 123540 loss: 0.0014 lr: 0.02\n",
            "iteration: 123550 loss: 0.0016 lr: 0.02\n",
            "iteration: 123560 loss: 0.0019 lr: 0.02\n",
            "iteration: 123570 loss: 0.0018 lr: 0.02\n",
            "iteration: 123580 loss: 0.0017 lr: 0.02\n",
            "iteration: 123590 loss: 0.0020 lr: 0.02\n",
            "iteration: 123600 loss: 0.0017 lr: 0.02\n",
            "iteration: 123610 loss: 0.0013 lr: 0.02\n",
            "iteration: 123620 loss: 0.0017 lr: 0.02\n",
            "iteration: 123630 loss: 0.0027 lr: 0.02\n",
            "iteration: 123640 loss: 0.0017 lr: 0.02\n",
            "iteration: 123650 loss: 0.0012 lr: 0.02\n",
            "iteration: 123660 loss: 0.0014 lr: 0.02\n",
            "iteration: 123670 loss: 0.0015 lr: 0.02\n",
            "iteration: 123680 loss: 0.0015 lr: 0.02\n",
            "iteration: 123690 loss: 0.0021 lr: 0.02\n",
            "iteration: 123700 loss: 0.0015 lr: 0.02\n",
            "iteration: 123710 loss: 0.0023 lr: 0.02\n",
            "iteration: 123720 loss: 0.0016 lr: 0.02\n",
            "iteration: 123730 loss: 0.0014 lr: 0.02\n",
            "iteration: 123740 loss: 0.0022 lr: 0.02\n",
            "iteration: 123750 loss: 0.0016 lr: 0.02\n",
            "iteration: 123760 loss: 0.0015 lr: 0.02\n",
            "iteration: 123770 loss: 0.0016 lr: 0.02\n",
            "iteration: 123780 loss: 0.0013 lr: 0.02\n",
            "iteration: 123790 loss: 0.0020 lr: 0.02\n",
            "iteration: 123800 loss: 0.0019 lr: 0.02\n",
            "iteration: 123810 loss: 0.0018 lr: 0.02\n",
            "iteration: 123820 loss: 0.0015 lr: 0.02\n",
            "iteration: 123830 loss: 0.0017 lr: 0.02\n",
            "iteration: 123840 loss: 0.0020 lr: 0.02\n",
            "iteration: 123850 loss: 0.0013 lr: 0.02\n",
            "iteration: 123860 loss: 0.0014 lr: 0.02\n",
            "iteration: 123870 loss: 0.0015 lr: 0.02\n",
            "iteration: 123880 loss: 0.0016 lr: 0.02\n",
            "iteration: 123890 loss: 0.0016 lr: 0.02\n",
            "iteration: 123900 loss: 0.0019 lr: 0.02\n",
            "iteration: 123910 loss: 0.0012 lr: 0.02\n",
            "iteration: 123920 loss: 0.0021 lr: 0.02\n",
            "iteration: 123930 loss: 0.0017 lr: 0.02\n",
            "iteration: 123940 loss: 0.0013 lr: 0.02\n",
            "iteration: 123950 loss: 0.0019 lr: 0.02\n",
            "iteration: 123960 loss: 0.0014 lr: 0.02\n",
            "iteration: 123970 loss: 0.0016 lr: 0.02\n",
            "iteration: 123980 loss: 0.0013 lr: 0.02\n",
            "iteration: 123990 loss: 0.0014 lr: 0.02\n",
            "iteration: 124000 loss: 0.0019 lr: 0.02\n",
            "iteration: 124010 loss: 0.0012 lr: 0.02\n",
            "iteration: 124020 loss: 0.0015 lr: 0.02\n",
            "iteration: 124030 loss: 0.0015 lr: 0.02\n",
            "iteration: 124040 loss: 0.0016 lr: 0.02\n",
            "iteration: 124050 loss: 0.0017 lr: 0.02\n",
            "iteration: 124060 loss: 0.0015 lr: 0.02\n",
            "iteration: 124070 loss: 0.0016 lr: 0.02\n",
            "iteration: 124080 loss: 0.0016 lr: 0.02\n",
            "iteration: 124090 loss: 0.0016 lr: 0.02\n",
            "iteration: 124100 loss: 0.0015 lr: 0.02\n",
            "iteration: 124110 loss: 0.0020 lr: 0.02\n",
            "iteration: 124120 loss: 0.0018 lr: 0.02\n",
            "iteration: 124130 loss: 0.0014 lr: 0.02\n",
            "iteration: 124140 loss: 0.0021 lr: 0.02\n",
            "iteration: 124150 loss: 0.0014 lr: 0.02\n",
            "iteration: 124160 loss: 0.0019 lr: 0.02\n",
            "iteration: 124170 loss: 0.0014 lr: 0.02\n",
            "iteration: 124180 loss: 0.0015 lr: 0.02\n",
            "iteration: 124190 loss: 0.0017 lr: 0.02\n",
            "iteration: 124200 loss: 0.0023 lr: 0.02\n",
            "iteration: 124210 loss: 0.0015 lr: 0.02\n",
            "iteration: 124220 loss: 0.0017 lr: 0.02\n",
            "iteration: 124230 loss: 0.0015 lr: 0.02\n",
            "iteration: 124240 loss: 0.0016 lr: 0.02\n",
            "iteration: 124250 loss: 0.0013 lr: 0.02\n",
            "iteration: 124260 loss: 0.0020 lr: 0.02\n",
            "iteration: 124270 loss: 0.0012 lr: 0.02\n",
            "iteration: 124280 loss: 0.0022 lr: 0.02\n",
            "iteration: 124290 loss: 0.0018 lr: 0.02\n",
            "iteration: 124300 loss: 0.0015 lr: 0.02\n",
            "iteration: 124310 loss: 0.0018 lr: 0.02\n",
            "iteration: 124320 loss: 0.0012 lr: 0.02\n",
            "iteration: 124330 loss: 0.0012 lr: 0.02\n",
            "iteration: 124340 loss: 0.0022 lr: 0.02\n",
            "iteration: 124350 loss: 0.0017 lr: 0.02\n",
            "iteration: 124360 loss: 0.0023 lr: 0.02\n",
            "iteration: 124370 loss: 0.0017 lr: 0.02\n",
            "iteration: 124380 loss: 0.0020 lr: 0.02\n",
            "iteration: 124390 loss: 0.0020 lr: 0.02\n",
            "iteration: 124400 loss: 0.0019 lr: 0.02\n",
            "iteration: 124410 loss: 0.0021 lr: 0.02\n",
            "iteration: 124420 loss: 0.0015 lr: 0.02\n",
            "iteration: 124430 loss: 0.0018 lr: 0.02\n",
            "iteration: 124440 loss: 0.0014 lr: 0.02\n",
            "iteration: 124450 loss: 0.0016 lr: 0.02\n",
            "iteration: 124460 loss: 0.0013 lr: 0.02\n",
            "iteration: 124470 loss: 0.0018 lr: 0.02\n",
            "iteration: 124480 loss: 0.0013 lr: 0.02\n",
            "iteration: 124490 loss: 0.0016 lr: 0.02\n",
            "iteration: 124500 loss: 0.0017 lr: 0.02\n",
            "iteration: 124510 loss: 0.0014 lr: 0.02\n",
            "iteration: 124520 loss: 0.0019 lr: 0.02\n",
            "iteration: 124530 loss: 0.0014 lr: 0.02\n",
            "iteration: 124540 loss: 0.0012 lr: 0.02\n",
            "iteration: 124550 loss: 0.0014 lr: 0.02\n",
            "iteration: 124560 loss: 0.0022 lr: 0.02\n",
            "iteration: 124570 loss: 0.0017 lr: 0.02\n",
            "iteration: 124580 loss: 0.0015 lr: 0.02\n",
            "iteration: 124590 loss: 0.0017 lr: 0.02\n",
            "iteration: 124600 loss: 0.0018 lr: 0.02\n",
            "iteration: 124610 loss: 0.0016 lr: 0.02\n",
            "iteration: 124620 loss: 0.0016 lr: 0.02\n",
            "iteration: 124630 loss: 0.0015 lr: 0.02\n",
            "iteration: 124640 loss: 0.0015 lr: 0.02\n",
            "iteration: 124650 loss: 0.0020 lr: 0.02\n",
            "iteration: 124660 loss: 0.0016 lr: 0.02\n",
            "iteration: 124670 loss: 0.0018 lr: 0.02\n",
            "iteration: 124680 loss: 0.0014 lr: 0.02\n",
            "iteration: 124690 loss: 0.0014 lr: 0.02\n",
            "iteration: 124700 loss: 0.0013 lr: 0.02\n",
            "iteration: 124710 loss: 0.0017 lr: 0.02\n",
            "iteration: 124720 loss: 0.0019 lr: 0.02\n",
            "iteration: 124730 loss: 0.0018 lr: 0.02\n",
            "iteration: 124740 loss: 0.0015 lr: 0.02\n",
            "iteration: 124750 loss: 0.0019 lr: 0.02\n",
            "iteration: 124760 loss: 0.0013 lr: 0.02\n",
            "iteration: 124770 loss: 0.0015 lr: 0.02\n",
            "iteration: 124780 loss: 0.0020 lr: 0.02\n",
            "iteration: 124790 loss: 0.0015 lr: 0.02\n",
            "iteration: 124800 loss: 0.0016 lr: 0.02\n",
            "iteration: 124810 loss: 0.0011 lr: 0.02\n",
            "iteration: 124820 loss: 0.0019 lr: 0.02\n",
            "iteration: 124830 loss: 0.0017 lr: 0.02\n",
            "iteration: 124840 loss: 0.0019 lr: 0.02\n",
            "iteration: 124850 loss: 0.0015 lr: 0.02\n",
            "iteration: 124860 loss: 0.0017 lr: 0.02\n",
            "iteration: 124870 loss: 0.0018 lr: 0.02\n",
            "iteration: 124880 loss: 0.0015 lr: 0.02\n",
            "iteration: 124890 loss: 0.0016 lr: 0.02\n",
            "iteration: 124900 loss: 0.0014 lr: 0.02\n",
            "iteration: 124910 loss: 0.0018 lr: 0.02\n",
            "iteration: 124920 loss: 0.0014 lr: 0.02\n",
            "iteration: 124930 loss: 0.0018 lr: 0.02\n",
            "iteration: 124940 loss: 0.0017 lr: 0.02\n",
            "iteration: 124950 loss: 0.0015 lr: 0.02\n",
            "iteration: 124960 loss: 0.0013 lr: 0.02\n",
            "iteration: 124970 loss: 0.0014 lr: 0.02\n",
            "iteration: 124980 loss: 0.0016 lr: 0.02\n",
            "iteration: 124990 loss: 0.0014 lr: 0.02\n",
            "iteration: 125000 loss: 0.0019 lr: 0.02\n",
            "iteration: 125010 loss: 0.0013 lr: 0.02\n",
            "iteration: 125020 loss: 0.0013 lr: 0.02\n",
            "iteration: 125030 loss: 0.0018 lr: 0.02\n",
            "iteration: 125040 loss: 0.0015 lr: 0.02\n",
            "iteration: 125050 loss: 0.0017 lr: 0.02\n",
            "iteration: 125060 loss: 0.0018 lr: 0.02\n",
            "iteration: 125070 loss: 0.0014 lr: 0.02\n",
            "iteration: 125080 loss: 0.0016 lr: 0.02\n",
            "iteration: 125090 loss: 0.0014 lr: 0.02\n",
            "iteration: 125100 loss: 0.0018 lr: 0.02\n",
            "iteration: 125110 loss: 0.0011 lr: 0.02\n",
            "iteration: 125120 loss: 0.0020 lr: 0.02\n",
            "iteration: 125130 loss: 0.0017 lr: 0.02\n",
            "iteration: 125140 loss: 0.0017 lr: 0.02\n",
            "iteration: 125150 loss: 0.0018 lr: 0.02\n",
            "iteration: 125160 loss: 0.0018 lr: 0.02\n",
            "iteration: 125170 loss: 0.0016 lr: 0.02\n",
            "iteration: 125180 loss: 0.0016 lr: 0.02\n",
            "iteration: 125190 loss: 0.0016 lr: 0.02\n",
            "iteration: 125200 loss: 0.0015 lr: 0.02\n",
            "iteration: 125210 loss: 0.0017 lr: 0.02\n",
            "iteration: 125220 loss: 0.0018 lr: 0.02\n",
            "iteration: 125230 loss: 0.0018 lr: 0.02\n",
            "iteration: 125240 loss: 0.0016 lr: 0.02\n",
            "iteration: 125250 loss: 0.0015 lr: 0.02\n",
            "iteration: 125260 loss: 0.0020 lr: 0.02\n",
            "iteration: 125270 loss: 0.0014 lr: 0.02\n",
            "iteration: 125280 loss: 0.0013 lr: 0.02\n",
            "iteration: 125290 loss: 0.0018 lr: 0.02\n",
            "iteration: 125300 loss: 0.0014 lr: 0.02\n",
            "iteration: 125310 loss: 0.0020 lr: 0.02\n",
            "iteration: 125320 loss: 0.0016 lr: 0.02\n",
            "iteration: 125330 loss: 0.0016 lr: 0.02\n",
            "iteration: 125340 loss: 0.0017 lr: 0.02\n",
            "iteration: 125350 loss: 0.0014 lr: 0.02\n",
            "iteration: 125360 loss: 0.0020 lr: 0.02\n",
            "iteration: 125370 loss: 0.0015 lr: 0.02\n",
            "iteration: 125380 loss: 0.0016 lr: 0.02\n",
            "iteration: 125390 loss: 0.0016 lr: 0.02\n",
            "iteration: 125400 loss: 0.0015 lr: 0.02\n",
            "iteration: 125410 loss: 0.0016 lr: 0.02\n",
            "iteration: 125420 loss: 0.0016 lr: 0.02\n",
            "iteration: 125430 loss: 0.0015 lr: 0.02\n",
            "iteration: 125440 loss: 0.0016 lr: 0.02\n",
            "iteration: 125450 loss: 0.0017 lr: 0.02\n",
            "iteration: 125460 loss: 0.0016 lr: 0.02\n",
            "iteration: 125470 loss: 0.0017 lr: 0.02\n",
            "iteration: 125480 loss: 0.0015 lr: 0.02\n",
            "iteration: 125490 loss: 0.0020 lr: 0.02\n",
            "iteration: 125500 loss: 0.0017 lr: 0.02\n",
            "iteration: 125510 loss: 0.0014 lr: 0.02\n",
            "iteration: 125520 loss: 0.0015 lr: 0.02\n",
            "iteration: 125530 loss: 0.0018 lr: 0.02\n",
            "iteration: 125540 loss: 0.0011 lr: 0.02\n",
            "iteration: 125550 loss: 0.0017 lr: 0.02\n",
            "iteration: 125560 loss: 0.0016 lr: 0.02\n",
            "iteration: 125570 loss: 0.0013 lr: 0.02\n",
            "iteration: 125580 loss: 0.0017 lr: 0.02\n",
            "iteration: 125590 loss: 0.0016 lr: 0.02\n",
            "iteration: 125600 loss: 0.0015 lr: 0.02\n",
            "iteration: 125610 loss: 0.0017 lr: 0.02\n",
            "iteration: 125620 loss: 0.0014 lr: 0.02\n",
            "iteration: 125630 loss: 0.0018 lr: 0.02\n",
            "iteration: 125640 loss: 0.0015 lr: 0.02\n",
            "iteration: 125650 loss: 0.0015 lr: 0.02\n",
            "iteration: 125660 loss: 0.0016 lr: 0.02\n",
            "iteration: 125670 loss: 0.0019 lr: 0.02\n",
            "iteration: 125680 loss: 0.0016 lr: 0.02\n",
            "iteration: 125690 loss: 0.0013 lr: 0.02\n",
            "iteration: 125700 loss: 0.0023 lr: 0.02\n",
            "iteration: 125710 loss: 0.0016 lr: 0.02\n",
            "iteration: 125720 loss: 0.0018 lr: 0.02\n",
            "iteration: 125730 loss: 0.0014 lr: 0.02\n",
            "iteration: 125740 loss: 0.0016 lr: 0.02\n",
            "iteration: 125750 loss: 0.0020 lr: 0.02\n",
            "iteration: 125760 loss: 0.0015 lr: 0.02\n",
            "iteration: 125770 loss: 0.0015 lr: 0.02\n",
            "iteration: 125780 loss: 0.0015 lr: 0.02\n",
            "iteration: 125790 loss: 0.0013 lr: 0.02\n",
            "iteration: 125800 loss: 0.0021 lr: 0.02\n",
            "iteration: 125810 loss: 0.0015 lr: 0.02\n",
            "iteration: 125820 loss: 0.0016 lr: 0.02\n",
            "iteration: 125830 loss: 0.0012 lr: 0.02\n",
            "iteration: 125840 loss: 0.0014 lr: 0.02\n",
            "iteration: 125850 loss: 0.0023 lr: 0.02\n",
            "iteration: 125860 loss: 0.0013 lr: 0.02\n",
            "iteration: 125870 loss: 0.0015 lr: 0.02\n",
            "iteration: 125880 loss: 0.0011 lr: 0.02\n",
            "iteration: 125890 loss: 0.0013 lr: 0.02\n",
            "iteration: 125900 loss: 0.0022 lr: 0.02\n",
            "iteration: 125910 loss: 0.0013 lr: 0.02\n",
            "iteration: 125920 loss: 0.0014 lr: 0.02\n",
            "iteration: 125930 loss: 0.0012 lr: 0.02\n",
            "iteration: 125940 loss: 0.0016 lr: 0.02\n",
            "iteration: 125950 loss: 0.0013 lr: 0.02\n",
            "iteration: 125960 loss: 0.0018 lr: 0.02\n",
            "iteration: 125970 loss: 0.0015 lr: 0.02\n",
            "iteration: 125980 loss: 0.0017 lr: 0.02\n",
            "iteration: 125990 loss: 0.0019 lr: 0.02\n",
            "iteration: 126000 loss: 0.0016 lr: 0.02\n",
            "iteration: 126010 loss: 0.0022 lr: 0.02\n",
            "iteration: 126020 loss: 0.0020 lr: 0.02\n",
            "iteration: 126030 loss: 0.0017 lr: 0.02\n",
            "iteration: 126040 loss: 0.0019 lr: 0.02\n",
            "iteration: 126050 loss: 0.0014 lr: 0.02\n",
            "iteration: 126060 loss: 0.0014 lr: 0.02\n",
            "iteration: 126070 loss: 0.0020 lr: 0.02\n",
            "iteration: 126080 loss: 0.0016 lr: 0.02\n",
            "iteration: 126090 loss: 0.0014 lr: 0.02\n",
            "iteration: 126100 loss: 0.0015 lr: 0.02\n",
            "iteration: 126110 loss: 0.0014 lr: 0.02\n",
            "iteration: 126120 loss: 0.0013 lr: 0.02\n",
            "iteration: 126130 loss: 0.0016 lr: 0.02\n",
            "iteration: 126140 loss: 0.0016 lr: 0.02\n",
            "iteration: 126150 loss: 0.0013 lr: 0.02\n",
            "iteration: 126160 loss: 0.0022 lr: 0.02\n",
            "iteration: 126170 loss: 0.0015 lr: 0.02\n",
            "iteration: 126180 loss: 0.0017 lr: 0.02\n",
            "iteration: 126190 loss: 0.0013 lr: 0.02\n",
            "iteration: 126200 loss: 0.0017 lr: 0.02\n",
            "iteration: 126210 loss: 0.0016 lr: 0.02\n",
            "iteration: 126220 loss: 0.0018 lr: 0.02\n",
            "iteration: 126230 loss: 0.0013 lr: 0.02\n",
            "iteration: 126240 loss: 0.0016 lr: 0.02\n",
            "iteration: 126250 loss: 0.0017 lr: 0.02\n",
            "iteration: 126260 loss: 0.0019 lr: 0.02\n",
            "iteration: 126270 loss: 0.0014 lr: 0.02\n",
            "iteration: 126280 loss: 0.0015 lr: 0.02\n",
            "iteration: 126290 loss: 0.0014 lr: 0.02\n",
            "iteration: 126300 loss: 0.0014 lr: 0.02\n",
            "iteration: 126310 loss: 0.0014 lr: 0.02\n",
            "iteration: 126320 loss: 0.0014 lr: 0.02\n",
            "iteration: 126330 loss: 0.0013 lr: 0.02\n",
            "iteration: 126340 loss: 0.0017 lr: 0.02\n",
            "iteration: 126350 loss: 0.0023 lr: 0.02\n",
            "iteration: 126360 loss: 0.0019 lr: 0.02\n",
            "iteration: 126370 loss: 0.0013 lr: 0.02\n",
            "iteration: 126380 loss: 0.0014 lr: 0.02\n",
            "iteration: 126390 loss: 0.0019 lr: 0.02\n",
            "iteration: 126400 loss: 0.0021 lr: 0.02\n",
            "iteration: 126410 loss: 0.0013 lr: 0.02\n",
            "iteration: 126420 loss: 0.0014 lr: 0.02\n",
            "iteration: 126430 loss: 0.0018 lr: 0.02\n",
            "iteration: 126440 loss: 0.0011 lr: 0.02\n",
            "iteration: 126450 loss: 0.0019 lr: 0.02\n",
            "iteration: 126460 loss: 0.0015 lr: 0.02\n",
            "iteration: 126470 loss: 0.0014 lr: 0.02\n",
            "iteration: 126480 loss: 0.0018 lr: 0.02\n",
            "iteration: 126490 loss: 0.0015 lr: 0.02\n",
            "iteration: 126500 loss: 0.0017 lr: 0.02\n",
            "iteration: 126510 loss: 0.0013 lr: 0.02\n",
            "iteration: 126520 loss: 0.0017 lr: 0.02\n",
            "iteration: 126530 loss: 0.0016 lr: 0.02\n",
            "iteration: 126540 loss: 0.0014 lr: 0.02\n",
            "iteration: 126550 loss: 0.0012 lr: 0.02\n",
            "iteration: 126560 loss: 0.0014 lr: 0.02\n",
            "iteration: 126570 loss: 0.0017 lr: 0.02\n",
            "iteration: 126580 loss: 0.0016 lr: 0.02\n",
            "iteration: 126590 loss: 0.0017 lr: 0.02\n",
            "iteration: 126600 loss: 0.0015 lr: 0.02\n",
            "iteration: 126610 loss: 0.0017 lr: 0.02\n",
            "iteration: 126620 loss: 0.0013 lr: 0.02\n",
            "iteration: 126630 loss: 0.0018 lr: 0.02\n",
            "iteration: 126640 loss: 0.0016 lr: 0.02\n",
            "iteration: 126650 loss: 0.0014 lr: 0.02\n",
            "iteration: 126660 loss: 0.0016 lr: 0.02\n",
            "iteration: 126670 loss: 0.0017 lr: 0.02\n",
            "iteration: 126680 loss: 0.0016 lr: 0.02\n",
            "iteration: 126690 loss: 0.0014 lr: 0.02\n",
            "iteration: 126700 loss: 0.0017 lr: 0.02\n",
            "iteration: 126710 loss: 0.0019 lr: 0.02\n",
            "iteration: 126720 loss: 0.0015 lr: 0.02\n",
            "iteration: 126730 loss: 0.0018 lr: 0.02\n",
            "iteration: 126740 loss: 0.0013 lr: 0.02\n",
            "iteration: 126750 loss: 0.0013 lr: 0.02\n",
            "iteration: 126760 loss: 0.0015 lr: 0.02\n",
            "iteration: 126770 loss: 0.0013 lr: 0.02\n",
            "iteration: 126780 loss: 0.0014 lr: 0.02\n",
            "iteration: 126790 loss: 0.0013 lr: 0.02\n",
            "iteration: 126800 loss: 0.0021 lr: 0.02\n",
            "iteration: 126810 loss: 0.0017 lr: 0.02\n",
            "iteration: 126820 loss: 0.0012 lr: 0.02\n",
            "iteration: 126830 loss: 0.0021 lr: 0.02\n",
            "iteration: 126840 loss: 0.0017 lr: 0.02\n",
            "iteration: 126850 loss: 0.0019 lr: 0.02\n",
            "iteration: 126860 loss: 0.0013 lr: 0.02\n",
            "iteration: 126870 loss: 0.0018 lr: 0.02\n",
            "iteration: 126880 loss: 0.0014 lr: 0.02\n",
            "iteration: 126890 loss: 0.0014 lr: 0.02\n",
            "iteration: 126900 loss: 0.0015 lr: 0.02\n",
            "iteration: 126910 loss: 0.0013 lr: 0.02\n",
            "iteration: 126920 loss: 0.0016 lr: 0.02\n",
            "iteration: 126930 loss: 0.0015 lr: 0.02\n",
            "iteration: 126940 loss: 0.0018 lr: 0.02\n",
            "iteration: 126950 loss: 0.0016 lr: 0.02\n",
            "iteration: 126960 loss: 0.0017 lr: 0.02\n",
            "iteration: 126970 loss: 0.0020 lr: 0.02\n",
            "iteration: 126980 loss: 0.0011 lr: 0.02\n",
            "iteration: 126990 loss: 0.0017 lr: 0.02\n",
            "iteration: 127000 loss: 0.0017 lr: 0.02\n",
            "iteration: 127010 loss: 0.0015 lr: 0.02\n",
            "iteration: 127020 loss: 0.0020 lr: 0.02\n",
            "iteration: 127030 loss: 0.0018 lr: 0.02\n",
            "iteration: 127040 loss: 0.0017 lr: 0.02\n",
            "iteration: 127050 loss: 0.0020 lr: 0.02\n",
            "iteration: 127060 loss: 0.0016 lr: 0.02\n",
            "iteration: 127070 loss: 0.0015 lr: 0.02\n",
            "iteration: 127080 loss: 0.0017 lr: 0.02\n",
            "iteration: 127090 loss: 0.0015 lr: 0.02\n",
            "iteration: 127100 loss: 0.0011 lr: 0.02\n",
            "iteration: 127110 loss: 0.0015 lr: 0.02\n",
            "iteration: 127120 loss: 0.0019 lr: 0.02\n",
            "iteration: 127130 loss: 0.0017 lr: 0.02\n",
            "iteration: 127140 loss: 0.0016 lr: 0.02\n",
            "iteration: 127150 loss: 0.0019 lr: 0.02\n",
            "iteration: 127160 loss: 0.0016 lr: 0.02\n",
            "iteration: 127170 loss: 0.0020 lr: 0.02\n",
            "iteration: 127180 loss: 0.0013 lr: 0.02\n",
            "iteration: 127190 loss: 0.0019 lr: 0.02\n",
            "iteration: 127200 loss: 0.0022 lr: 0.02\n",
            "iteration: 127210 loss: 0.0018 lr: 0.02\n",
            "iteration: 127220 loss: 0.0017 lr: 0.02\n",
            "iteration: 127230 loss: 0.0016 lr: 0.02\n",
            "iteration: 127240 loss: 0.0015 lr: 0.02\n",
            "iteration: 127250 loss: 0.0016 lr: 0.02\n",
            "iteration: 127260 loss: 0.0019 lr: 0.02\n",
            "iteration: 127270 loss: 0.0016 lr: 0.02\n",
            "iteration: 127280 loss: 0.0014 lr: 0.02\n",
            "iteration: 127290 loss: 0.0018 lr: 0.02\n",
            "iteration: 127300 loss: 0.0017 lr: 0.02\n",
            "iteration: 127310 loss: 0.0017 lr: 0.02\n",
            "iteration: 127320 loss: 0.0016 lr: 0.02\n",
            "iteration: 127330 loss: 0.0013 lr: 0.02\n",
            "iteration: 127340 loss: 0.0021 lr: 0.02\n",
            "iteration: 127350 loss: 0.0020 lr: 0.02\n",
            "iteration: 127360 loss: 0.0014 lr: 0.02\n",
            "iteration: 127370 loss: 0.0018 lr: 0.02\n",
            "iteration: 127380 loss: 0.0012 lr: 0.02\n",
            "iteration: 127390 loss: 0.0017 lr: 0.02\n",
            "iteration: 127400 loss: 0.0019 lr: 0.02\n",
            "iteration: 127410 loss: 0.0014 lr: 0.02\n",
            "iteration: 127420 loss: 0.0017 lr: 0.02\n",
            "iteration: 127430 loss: 0.0013 lr: 0.02\n",
            "iteration: 127440 loss: 0.0017 lr: 0.02\n",
            "iteration: 127450 loss: 0.0015 lr: 0.02\n",
            "iteration: 127460 loss: 0.0017 lr: 0.02\n",
            "iteration: 127470 loss: 0.0016 lr: 0.02\n",
            "iteration: 127480 loss: 0.0015 lr: 0.02\n",
            "iteration: 127490 loss: 0.0016 lr: 0.02\n",
            "iteration: 127500 loss: 0.0018 lr: 0.02\n",
            "iteration: 127510 loss: 0.0016 lr: 0.02\n",
            "iteration: 127520 loss: 0.0014 lr: 0.02\n",
            "iteration: 127530 loss: 0.0013 lr: 0.02\n",
            "iteration: 127540 loss: 0.0017 lr: 0.02\n",
            "iteration: 127550 loss: 0.0016 lr: 0.02\n",
            "iteration: 127560 loss: 0.0018 lr: 0.02\n",
            "iteration: 127570 loss: 0.0015 lr: 0.02\n",
            "iteration: 127580 loss: 0.0017 lr: 0.02\n",
            "iteration: 127590 loss: 0.0013 lr: 0.02\n",
            "iteration: 127600 loss: 0.0015 lr: 0.02\n",
            "iteration: 127610 loss: 0.0016 lr: 0.02\n",
            "iteration: 127620 loss: 0.0014 lr: 0.02\n",
            "iteration: 127630 loss: 0.0016 lr: 0.02\n",
            "iteration: 127640 loss: 0.0013 lr: 0.02\n",
            "iteration: 127650 loss: 0.0017 lr: 0.02\n",
            "iteration: 127660 loss: 0.0016 lr: 0.02\n",
            "iteration: 127670 loss: 0.0017 lr: 0.02\n",
            "iteration: 127680 loss: 0.0015 lr: 0.02\n",
            "iteration: 127690 loss: 0.0019 lr: 0.02\n",
            "iteration: 127700 loss: 0.0014 lr: 0.02\n",
            "iteration: 127710 loss: 0.0017 lr: 0.02\n",
            "iteration: 127720 loss: 0.0022 lr: 0.02\n",
            "iteration: 127730 loss: 0.0019 lr: 0.02\n",
            "iteration: 127740 loss: 0.0021 lr: 0.02\n",
            "iteration: 127750 loss: 0.0019 lr: 0.02\n",
            "iteration: 127760 loss: 0.0017 lr: 0.02\n",
            "iteration: 127770 loss: 0.0014 lr: 0.02\n",
            "iteration: 127780 loss: 0.0018 lr: 0.02\n",
            "iteration: 127790 loss: 0.0012 lr: 0.02\n",
            "iteration: 127800 loss: 0.0015 lr: 0.02\n",
            "iteration: 127810 loss: 0.0018 lr: 0.02\n",
            "iteration: 127820 loss: 0.0016 lr: 0.02\n",
            "iteration: 127830 loss: 0.0014 lr: 0.02\n",
            "iteration: 127840 loss: 0.0014 lr: 0.02\n",
            "iteration: 127850 loss: 0.0014 lr: 0.02\n",
            "iteration: 127860 loss: 0.0015 lr: 0.02\n",
            "iteration: 127870 loss: 0.0012 lr: 0.02\n",
            "iteration: 127880 loss: 0.0018 lr: 0.02\n",
            "iteration: 127890 loss: 0.0015 lr: 0.02\n",
            "iteration: 127900 loss: 0.0019 lr: 0.02\n",
            "iteration: 127910 loss: 0.0017 lr: 0.02\n",
            "iteration: 127920 loss: 0.0013 lr: 0.02\n",
            "iteration: 127930 loss: 0.0018 lr: 0.02\n",
            "iteration: 127940 loss: 0.0017 lr: 0.02\n",
            "iteration: 127950 loss: 0.0022 lr: 0.02\n",
            "iteration: 127960 loss: 0.0021 lr: 0.02\n",
            "iteration: 127970 loss: 0.0014 lr: 0.02\n",
            "iteration: 127980 loss: 0.0017 lr: 0.02\n",
            "iteration: 127990 loss: 0.0015 lr: 0.02\n",
            "iteration: 128000 loss: 0.0017 lr: 0.02\n",
            "iteration: 128010 loss: 0.0014 lr: 0.02\n",
            "iteration: 128020 loss: 0.0014 lr: 0.02\n",
            "iteration: 128030 loss: 0.0012 lr: 0.02\n",
            "iteration: 128040 loss: 0.0021 lr: 0.02\n",
            "iteration: 128050 loss: 0.0019 lr: 0.02\n",
            "iteration: 128060 loss: 0.0016 lr: 0.02\n",
            "iteration: 128070 loss: 0.0016 lr: 0.02\n",
            "iteration: 128080 loss: 0.0019 lr: 0.02\n",
            "iteration: 128090 loss: 0.0018 lr: 0.02\n",
            "iteration: 128100 loss: 0.0019 lr: 0.02\n",
            "iteration: 128110 loss: 0.0018 lr: 0.02\n",
            "iteration: 128120 loss: 0.0023 lr: 0.02\n",
            "iteration: 128130 loss: 0.0013 lr: 0.02\n",
            "iteration: 128140 loss: 0.0017 lr: 0.02\n",
            "iteration: 128150 loss: 0.0014 lr: 0.02\n",
            "iteration: 128160 loss: 0.0012 lr: 0.02\n",
            "iteration: 128170 loss: 0.0014 lr: 0.02\n",
            "iteration: 128180 loss: 0.0016 lr: 0.02\n",
            "iteration: 128190 loss: 0.0017 lr: 0.02\n",
            "iteration: 128200 loss: 0.0019 lr: 0.02\n",
            "iteration: 128210 loss: 0.0016 lr: 0.02\n",
            "iteration: 128220 loss: 0.0016 lr: 0.02\n",
            "iteration: 128230 loss: 0.0014 lr: 0.02\n",
            "iteration: 128240 loss: 0.0030 lr: 0.02\n",
            "iteration: 128250 loss: 0.0021 lr: 0.02\n",
            "iteration: 128260 loss: 0.0016 lr: 0.02\n",
            "iteration: 128270 loss: 0.0021 lr: 0.02\n",
            "iteration: 128280 loss: 0.0014 lr: 0.02\n",
            "iteration: 128290 loss: 0.0016 lr: 0.02\n",
            "iteration: 128300 loss: 0.0022 lr: 0.02\n",
            "iteration: 128310 loss: 0.0015 lr: 0.02\n",
            "iteration: 128320 loss: 0.0019 lr: 0.02\n",
            "iteration: 128330 loss: 0.0015 lr: 0.02\n",
            "iteration: 128340 loss: 0.0017 lr: 0.02\n",
            "iteration: 128350 loss: 0.0015 lr: 0.02\n",
            "iteration: 128360 loss: 0.0021 lr: 0.02\n",
            "iteration: 128370 loss: 0.0016 lr: 0.02\n",
            "iteration: 128380 loss: 0.0015 lr: 0.02\n",
            "iteration: 128390 loss: 0.0018 lr: 0.02\n",
            "iteration: 128400 loss: 0.0014 lr: 0.02\n",
            "iteration: 128410 loss: 0.0016 lr: 0.02\n",
            "iteration: 128420 loss: 0.0017 lr: 0.02\n",
            "iteration: 128430 loss: 0.0025 lr: 0.02\n",
            "iteration: 128440 loss: 0.0014 lr: 0.02\n",
            "iteration: 128450 loss: 0.0019 lr: 0.02\n",
            "iteration: 128460 loss: 0.0017 lr: 0.02\n",
            "iteration: 128470 loss: 0.0017 lr: 0.02\n",
            "iteration: 128480 loss: 0.0014 lr: 0.02\n",
            "iteration: 128490 loss: 0.0013 lr: 0.02\n",
            "iteration: 128500 loss: 0.0018 lr: 0.02\n",
            "iteration: 128510 loss: 0.0016 lr: 0.02\n",
            "iteration: 128520 loss: 0.0017 lr: 0.02\n",
            "iteration: 128530 loss: 0.0017 lr: 0.02\n",
            "iteration: 128540 loss: 0.0014 lr: 0.02\n",
            "iteration: 128550 loss: 0.0016 lr: 0.02\n",
            "iteration: 128560 loss: 0.0016 lr: 0.02\n",
            "iteration: 128570 loss: 0.0016 lr: 0.02\n",
            "iteration: 128580 loss: 0.0018 lr: 0.02\n",
            "iteration: 128590 loss: 0.0017 lr: 0.02\n",
            "iteration: 128600 loss: 0.0016 lr: 0.02\n",
            "iteration: 128610 loss: 0.0012 lr: 0.02\n",
            "iteration: 128620 loss: 0.0020 lr: 0.02\n",
            "iteration: 128630 loss: 0.0019 lr: 0.02\n",
            "iteration: 128640 loss: 0.0015 lr: 0.02\n",
            "iteration: 128650 loss: 0.0018 lr: 0.02\n",
            "iteration: 128660 loss: 0.0018 lr: 0.02\n",
            "iteration: 128670 loss: 0.0018 lr: 0.02\n",
            "iteration: 128680 loss: 0.0017 lr: 0.02\n",
            "iteration: 128690 loss: 0.0017 lr: 0.02\n",
            "iteration: 128700 loss: 0.0014 lr: 0.02\n",
            "iteration: 128710 loss: 0.0017 lr: 0.02\n",
            "iteration: 128720 loss: 0.0015 lr: 0.02\n",
            "iteration: 128730 loss: 0.0015 lr: 0.02\n",
            "iteration: 128740 loss: 0.0019 lr: 0.02\n",
            "iteration: 128750 loss: 0.0015 lr: 0.02\n",
            "iteration: 128760 loss: 0.0020 lr: 0.02\n",
            "iteration: 128770 loss: 0.0016 lr: 0.02\n",
            "iteration: 128780 loss: 0.0014 lr: 0.02\n",
            "iteration: 128790 loss: 0.0015 lr: 0.02\n",
            "iteration: 128800 loss: 0.0020 lr: 0.02\n",
            "iteration: 128810 loss: 0.0012 lr: 0.02\n",
            "iteration: 128820 loss: 0.0016 lr: 0.02\n",
            "iteration: 128830 loss: 0.0017 lr: 0.02\n",
            "iteration: 128840 loss: 0.0013 lr: 0.02\n",
            "iteration: 128850 loss: 0.0013 lr: 0.02\n",
            "iteration: 128860 loss: 0.0013 lr: 0.02\n",
            "iteration: 128870 loss: 0.0013 lr: 0.02\n",
            "iteration: 128880 loss: 0.0011 lr: 0.02\n",
            "iteration: 128890 loss: 0.0017 lr: 0.02\n",
            "iteration: 128900 loss: 0.0016 lr: 0.02\n",
            "iteration: 128910 loss: 0.0017 lr: 0.02\n",
            "iteration: 128920 loss: 0.0017 lr: 0.02\n",
            "iteration: 128930 loss: 0.0017 lr: 0.02\n",
            "iteration: 128940 loss: 0.0014 lr: 0.02\n",
            "iteration: 128950 loss: 0.0015 lr: 0.02\n",
            "iteration: 128960 loss: 0.0015 lr: 0.02\n",
            "iteration: 128970 loss: 0.0018 lr: 0.02\n",
            "iteration: 128980 loss: 0.0016 lr: 0.02\n",
            "iteration: 128990 loss: 0.0015 lr: 0.02\n",
            "iteration: 129000 loss: 0.0019 lr: 0.02\n",
            "iteration: 129010 loss: 0.0015 lr: 0.02\n",
            "iteration: 129020 loss: 0.0014 lr: 0.02\n",
            "iteration: 129030 loss: 0.0015 lr: 0.02\n",
            "iteration: 129040 loss: 0.0017 lr: 0.02\n",
            "iteration: 129050 loss: 0.0017 lr: 0.02\n",
            "iteration: 129060 loss: 0.0014 lr: 0.02\n",
            "iteration: 129070 loss: 0.0015 lr: 0.02\n",
            "iteration: 129080 loss: 0.0015 lr: 0.02\n",
            "iteration: 129090 loss: 0.0022 lr: 0.02\n",
            "iteration: 129100 loss: 0.0016 lr: 0.02\n",
            "iteration: 129110 loss: 0.0014 lr: 0.02\n",
            "iteration: 129120 loss: 0.0010 lr: 0.02\n",
            "iteration: 129130 loss: 0.0017 lr: 0.02\n",
            "iteration: 129140 loss: 0.0017 lr: 0.02\n",
            "iteration: 129150 loss: 0.0018 lr: 0.02\n",
            "iteration: 129160 loss: 0.0015 lr: 0.02\n",
            "iteration: 129170 loss: 0.0017 lr: 0.02\n",
            "iteration: 129180 loss: 0.0019 lr: 0.02\n",
            "iteration: 129190 loss: 0.0019 lr: 0.02\n",
            "iteration: 129200 loss: 0.0018 lr: 0.02\n",
            "iteration: 129210 loss: 0.0021 lr: 0.02\n",
            "iteration: 129220 loss: 0.0013 lr: 0.02\n",
            "iteration: 129230 loss: 0.0016 lr: 0.02\n",
            "iteration: 129240 loss: 0.0019 lr: 0.02\n",
            "iteration: 129250 loss: 0.0015 lr: 0.02\n",
            "iteration: 129260 loss: 0.0015 lr: 0.02\n",
            "iteration: 129270 loss: 0.0017 lr: 0.02\n",
            "iteration: 129280 loss: 0.0014 lr: 0.02\n",
            "iteration: 129290 loss: 0.0016 lr: 0.02\n",
            "iteration: 129300 loss: 0.0016 lr: 0.02\n",
            "iteration: 129310 loss: 0.0010 lr: 0.02\n",
            "iteration: 129320 loss: 0.0017 lr: 0.02\n",
            "iteration: 129330 loss: 0.0016 lr: 0.02\n",
            "iteration: 129340 loss: 0.0013 lr: 0.02\n",
            "iteration: 129350 loss: 0.0015 lr: 0.02\n",
            "iteration: 129360 loss: 0.0020 lr: 0.02\n",
            "iteration: 129370 loss: 0.0017 lr: 0.02\n",
            "iteration: 129380 loss: 0.0017 lr: 0.02\n",
            "iteration: 129390 loss: 0.0014 lr: 0.02\n",
            "iteration: 129400 loss: 0.0020 lr: 0.02\n",
            "iteration: 129410 loss: 0.0016 lr: 0.02\n",
            "iteration: 129420 loss: 0.0014 lr: 0.02\n",
            "iteration: 129430 loss: 0.0017 lr: 0.02\n",
            "iteration: 129440 loss: 0.0019 lr: 0.02\n",
            "iteration: 129450 loss: 0.0014 lr: 0.02\n",
            "iteration: 129460 loss: 0.0015 lr: 0.02\n",
            "iteration: 129470 loss: 0.0016 lr: 0.02\n",
            "iteration: 129480 loss: 0.0014 lr: 0.02\n",
            "iteration: 129490 loss: 0.0013 lr: 0.02\n",
            "iteration: 129500 loss: 0.0014 lr: 0.02\n",
            "iteration: 129510 loss: 0.0013 lr: 0.02\n",
            "iteration: 129520 loss: 0.0013 lr: 0.02\n",
            "iteration: 129530 loss: 0.0019 lr: 0.02\n",
            "iteration: 129540 loss: 0.0014 lr: 0.02\n",
            "iteration: 129550 loss: 0.0019 lr: 0.02\n",
            "iteration: 129560 loss: 0.0016 lr: 0.02\n",
            "iteration: 129570 loss: 0.0015 lr: 0.02\n",
            "iteration: 129580 loss: 0.0015 lr: 0.02\n",
            "iteration: 129590 loss: 0.0015 lr: 0.02\n",
            "iteration: 129600 loss: 0.0013 lr: 0.02\n",
            "iteration: 129610 loss: 0.0013 lr: 0.02\n",
            "iteration: 129620 loss: 0.0015 lr: 0.02\n",
            "iteration: 129630 loss: 0.0022 lr: 0.02\n",
            "iteration: 129640 loss: 0.0016 lr: 0.02\n",
            "iteration: 129650 loss: 0.0019 lr: 0.02\n",
            "iteration: 129660 loss: 0.0017 lr: 0.02\n",
            "iteration: 129670 loss: 0.0014 lr: 0.02\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bfbcd4c1603f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#more info and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#don't need to do this again for pre-trained network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposeconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplayiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pass on path and file name for pose_cfg.yaml!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgputouse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposeconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplayiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pass on path and file name for pose_cfg.yaml!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n\u001b[0;32m--> 190\u001b[0;31m                                           feed_dict={learning_rate: current_lr})\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiDwIVf5-3H_",
        "colab_type": "text"
      },
      "source": [
        "**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZygsb2DoEJc",
        "colab_type": "text"
      },
      "source": [
        "## Start evaluating:\n",
        "This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
        "and stores the results as .csv file in a subdirectory under **evaluation-results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv4zlbrnoEJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b37e4904-5200-4760-c8d5-767918da4fe5"
      },
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)\n",
        "\n",
        "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, \n",
        "#so be sure your labels are good! (And you have trained enough ;)\n",
        "#don't need to do this  for already trained network"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/evaluate.py:242: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4]],\n",
            " 'all_joints_names': ['led', 'left', 'right', 'tail', 'snout'],\n",
            " 'batch_size': 1,\n",
            " 'bottomheight': 400,\n",
            " 'crop': True,\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_bodyoutlineMar4/bodyoutline_Jim95shuffle1.mat',\n",
            " 'dataset_type': 'default',\n",
            " 'deconvolutionstride': 2,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'leftwidth': 400,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_bodyoutlineMar4/Documentation_data-bodyoutline_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'minsize': 100,\n",
            " 'mirror': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 5,\n",
            " 'optimizer': 'sgd',\n",
            " 'output_stride': 16,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04',\n",
            " 'regularize': False,\n",
            " 'rightwidth': 400,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/DLC/bodyoutline-Jim-2020-03-04/dlc-models/iteration-0/bodyoutlineMar4-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'topheight': 400,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running  DLC_resnet50_bodyoutlineMar4shuffle1_31500  with # of trainingiterations: 31500\n",
            "Initializing ResNet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_Buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m_swig_setattr\u001b[0;34m(self, class_type, name, value)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swig_setattr_nondynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m_swig_setattr_nondynamic\u001b[0;34m(self, class_type, name, value, static)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"this\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SwigPyObject'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-55088855ed98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplotting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Here you want to see a low pixel error! Of course, it can only be as good as the labeler,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#so be sure your labels are good! (And you have trained enough ;)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/evaluate.py\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m(config, Shuffles, trainingsetindex, plotting, show_errors, comparisonbodyparts, gputouse, rescale)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnotanalyzed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;31m# Specifying state of model (snapshot / training state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                     \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_pose_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \u001b[0mNumimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0mPredicteData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNumimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlc_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_joints_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/nnet/predict.py\u001b[0m in \u001b[0;36msetup_pose_prediction\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m   \u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mnet_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnet_heads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'part_prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation_refinement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'part_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'part_prob'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'locref'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'locref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py\u001b[0m in \u001b[0;36mget_net\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_arg_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 net, end_points = net_fun(im_centered,\n\u001b[0;32m---> 52\u001b[0;31m                                           global_pool=False, output_stride=self.cfg.output_stride,is_training=False)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/slim/python/slim/nets/resnet_v1.py\u001b[0m in \u001b[0;36mresnet_v1_50\u001b[0;34m(inputs, num_classes, is_training, global_pool, output_stride, reuse, scope)\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0minclude_root_block\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m       scope=scope)\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/slim/python/slim/nets/resnet_v1.py\u001b[0m in \u001b[0;36mresnet_v1\u001b[0;34m(inputs, blocks, num_classes, is_training, global_pool, output_stride, include_root_block, reuse, scope)\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_blocks_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m           \u001b[0;31m# Global average pooling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/slim/python/slim/nets/resnet_utils.py\u001b[0m in \u001b[0;36mstack_blocks_dense\u001b[0;34m(net, blocks, output_stride, outputs_collections)\u001b[0m\n\u001b[1;32m    209\u001b[0m           \u001b[0;31m# current unit's stride for use in subsequent layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutput_stride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent_stride\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutput_stride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mrate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stride'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/slim/python/slim/nets/resnet_v1.py\u001b[0m in \u001b[0;36mbottleneck\u001b[0;34m(inputs, depth, depth_bottleneck, stride, rate, outputs_collections, scope)\u001b[0m\n\u001b[1;32m    119\u001b[0m         residual, depth_bottleneck, 3, stride, rate=rate, scope='conv2')\n\u001b[1;32m    120\u001b[0m     residual = layers.conv2d(\n\u001b[0;32m--> 121\u001b[0;31m         residual, depth, [1, 1], stride=1, activation_fn=None, scope='conv3')\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshortcut\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mconvolution2d\u001b[0;34m(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[1;32m   1157\u001b[0m       \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m       \u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m       conv_dims=2)\n\u001b[0m\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope, conv_dims)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalizer_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0mnormalizer_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer_params\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnormalizer_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mactivation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(inputs, decay, center, scale, epsilon, activation_fn, param_initializers, param_regularizers, updates_collections, is_training, reuse, variables_collections, outputs_collections, trainable, batch_weights, fused, data_format, zero_debias_moving_mean, scope, renorm, renorm_clipping, renorm_decay, adjustment)\u001b[0m\n\u001b[1;32m    648\u001b[0m           \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m           fused=fused)\n\u001b[0;32m--> 650\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m       \u001b[0;31m# Add variables to collections.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1698\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \"\"\"\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    377\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m           experimental_autocast=False)\n\u001b[0m\u001b[1;32m    380\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    548\u001b[0m           function_utils.has_kwargs(custom_getter)):\n\u001b[1;32m    549\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m       return _true_getter(\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mwrapped_custom_getter\u001b[0;34m(getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;31m# processing, and return the results to the current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m     \u001b[0;31m# getter, which will also perform additional processing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_getter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_custom_getter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mlayer_variable_getter\u001b[0;34m(getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_model_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36m_model_variable_getter\u001b[0;34m(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, use_resource, synchronization, aggregation, **_)\u001b[0m\n\u001b[1;32m   1750\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mmodel_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m    352\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mlayer_variable_getter\u001b[0;34m(getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_model_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36m_model_variable_getter\u001b[0;34m(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, use_resource, synchronization, aggregation, **_)\u001b[0m\n\u001b[1;32m   1750\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mmodel_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m    352\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_eager_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2517\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1686\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m           shape=shape)\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m             _try_guard_against_uninitialized_dependencies(\n\u001b[0;32m-> 1861\u001b[0;31m                 name, self._initial_value),\n\u001b[0m\u001b[1;32m   1862\u001b[0m             validate_shape=validate_shape).op\n\u001b[1;32m   1863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_try_guard_against_uninitialized_dependencies\u001b[0;34m(name, initial_value)\u001b[0m\n\u001b[1;32m   2709\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_has_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_safe_initial_value_from_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_safe_initial_value_from_tensor\u001b[0;34m(name, tensor, op_cache)\u001b[0m\n\u001b[1;32m   2749\u001b[0m   \u001b[0mnew_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnew_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2751\u001b[0;31m     \u001b[0mnew_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_initial_value_from_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2752\u001b[0m     \u001b[0mop_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2753\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnew_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_safe_initial_value_from_op\u001b[0;34m(name, op, op_cache)\u001b[0m\n\u001b[1;32m   2769\u001b[0m     \u001b[0mmodifications\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmade\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mop\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0munchanged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2770\u001b[0m   \"\"\"\n\u001b[0;32m-> 2771\u001b[0;31m   \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2772\u001b[0m   if op_type in (\"IsVariableInitialized\", \"VarIsInitializedOp\",\n\u001b[1;32m   2773\u001b[0m                  \"ReadVariableOp\", \"If\"):\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnode_def\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2272\u001b[0m     \"\"\"\n\u001b[1;32m   2273\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2275\u001b[0m       \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationToNodeDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/framework/c_api_util.py\u001b[0m in \u001b[0;36mtf_buffer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBufferFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: <built-in function TF_NewBuffer> returned a result with an error set"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLBl3TQtrfB",
        "colab_type": "text"
      },
      "source": [
        "## There is an optional refinement step you can do outside of Colab:\n",
        "- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n",
        "- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos... \n",
        "- pplease see the repo and protocol instructions on how to refine your data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVFLSKKfoEJk",
        "colab_type": "text"
      },
      "source": [
        "## Start Analyzing videos: \n",
        "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
        "\n",
        "The results are stored in hd5 file in the same directory where the video resides. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_LZiS_0oEJl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d195c7c7-a3f9-4207-ab20-c3d2f545eae4"
      },
      "source": [
        "#do this for new vids. To use pretrained network, essentially need to write path to the config file of the\n",
        "#dlc project you want to use, write path to vids you want to analyze, and then run this code.\n",
        "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using snapshot-129500 for model /content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/dlc-models/iteration-0/SnoutAndImplantTrackingMay20-trainset95shuffle1\n",
            "Initializing ResNet\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/dlc-models/iteration-0/SnoutAndImplantTrackingMay20-trainset95shuffle1/train/snapshot-129500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/DLC/SnoutAndImplantTracking-Jim-2020-05-20/dlc-models/iteration-0/SnoutAndImplantTrackingMay20-trainset95shuffle1/train/snapshot-129500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Analyzing all the videos in the directory\n",
            "Starting to analyze %  test copy 3.mp4\n",
            "Loading  test copy 3.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/20143 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  335.72 , recorded with  60.0 fps!\n",
            "Overall # of frames:  20143  found with (before cropping) frame dimensions:  640 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "20301it [04:52, 69.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Detected frames:  20143\n",
            "Saving results in ....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting to analyze %  test.mp4\n",
            "Loading  test.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/22617 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  376.95 , recorded with  60.0 fps!\n",
            "Overall # of frames:  22617  found with (before cropping) frame dimensions:  640 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "22826it [05:19, 71.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Detected frames:  22617\n",
            "Saving results in ....\n",
            "Starting to analyze %  NOT2.mp4\n",
            "Loading  NOT2.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/14706 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  245.1 , recorded with  60.0 fps!\n",
            "Overall # of frames:  14706  found with (before cropping) frame dimensions:  640 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "14847it [03:27, 71.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Detected frames:  14706\n",
            "Saving results in ....\n",
            "Starting to analyze %  test copy.mp4\n",
            "Loading  test copy.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/18332 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  305.53 , recorded with  60.0 fps!\n",
            "Overall # of frames:  18332  found with (before cropping) frame dimensions:  640 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "18483it [04:19, 71.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Detected frames:  18332\n",
            "Saving results in ....\n",
            "Starting to analyze %  NOT1.mp4\n",
            "Loading  NOT1.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/20203 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  336.72 , recorded with  60.0 fps!\n",
            "Overall # of frames:  20203  found with (before cropping) frame dimensions:  640 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "20402it [04:44, 71.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Detected frames:  20203\n",
            "Saving results in ....\n",
            "Starting to analyze %  test copy 2.mp4\n",
            "Loading  test copy 2.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/19643 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  327.38 , recorded with  60.0 fps!\n",
            "Overall # of frames:  19643  found with (before cropping) frame dimensions:  640 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "19796it [04:36, 71.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Detected frames:  19643\n",
            "Saving results in ....\n",
            "Starting to analyze %  fam2.mp4\n",
            "Loading  fam2.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/38169 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  636.15 , recorded with  60.0 fps!\n",
            "Overall # of frames:  38169  found with (before cropping) frame dimensions:  640 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "38481it [08:53, 72.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Detected frames:  38169\n",
            "Saving results in ....\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'.\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DLC_resnet50_SnoutAndImplantTrackingMay20shuffle1_129500'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrUvQIvoEKD",
        "colab_type": "text"
      },
      "source": [
        "## Create labeled video:\n",
        "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aDF7Q7KoEKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "564a4196-c717-4f94-89a5-f944a5ca1533"
      },
      "source": [
        "#this makes the video with tracking\n",
        "deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analyzing all the videos in the directory\n",
            "Starting %  . ['/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04/videos/']\n",
            "Loading  NOT2.mp4 and data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 26/14706 [00:00<00:56, 259.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14706\n",
            "Duration of video [s]:  245.1 , recorded with  60.0 fps!\n",
            "Overall # of frames:  14706 with cropped frame dimensions:  640 480\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14706/14706 [00:46<00:00, 314.46it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting %  . ['/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04/videos/']\n",
            "Loading  test copy 3.mp4 and data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 24/20143 [00:00<01:25, 236.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20143\n",
            "Duration of video [s]:  335.72 , recorded with  60.0 fps!\n",
            "Overall # of frames:  20143 with cropped frame dimensions:  640 480\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20143/20143 [01:07<00:00, 296.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting %  . ['/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04/videos/']\n",
            "Loading  test.mp4 and data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 17/22617 [00:00<02:14, 168.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22617\n",
            "Duration of video [s]:  376.95 , recorded with  60.0 fps!\n",
            "Overall # of frames:  22617 with cropped frame dimensions:  640 480\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 22617/22617 [01:17<00:00, 293.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting %  . ['/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04/videos/']\n",
            "Loading  test copy 2.mp4 and data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 17/19643 [00:00<01:57, 167.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "19643\n",
            "Duration of video [s]:  327.38 , recorded with  60.0 fps!\n",
            "Overall # of frames:  19643 with cropped frame dimensions:  640 480\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19643/19643 [01:06<00:00, 294.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting %  . ['/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04/videos/']\n",
            "Loading  NOT1.mp4 and data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 26/20203 [00:00<01:18, 257.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20203\n",
            "Duration of video [s]:  336.72 , recorded with  60.0 fps!\n",
            "Overall # of frames:  20203 with cropped frame dimensions:  640 480\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20203/20203 [01:06<00:00, 303.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting %  . ['/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04/videos/']\n",
            "Loading  test copy.mp4 and data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 27/18332 [00:00<01:09, 262.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18332\n",
            "Duration of video [s]:  305.53 , recorded with  60.0 fps!\n",
            "Overall # of frames:  18332 with cropped frame dimensions:  640 480\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18332/18332 [01:03<00:00, 288.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting %  . ['/content/drive/My Drive/DLC/bodyoutline-Jim-2020-03-04/videos/']\n",
            "Loading  fam2.mp4 and data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 28/38169 [00:00<02:17, 278.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "38169\n",
            "Duration of video [s]:  636.15 , recorded with  60.0 fps!\n",
            "Overall # of frames:  38169 with cropped frame dimensions:  640 480\n",
            "Generating frames and creating video.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 38169/38169 [02:05<00:00, 305.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTiuJESoEKH",
        "colab_type": "text"
      },
      "source": [
        "## Plot the trajectories of the analyzed videos:\n",
        "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX21zZbXoEKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ab8f51e8-34ec-4340-ebbb-fb2f6a3e2cc2"
      },
      "source": [
        "deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analyzing all the videos in the directory\n",
            "NOT1.mp4\n",
            "Starting %  . NOT1.mp4\n",
            "Loading  NOT1.mp4 and data.\n",
            ".  already exists!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "window.mpl = {};\n",
              "\n",
              "\n",
              "mpl.get_websocket_type = function() {\n",
              "    if (typeof(WebSocket) !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert('Your browser does not have WebSocket support.' +\n",
              "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "              'Firefox 4 and 5 are also supported but you ' +\n",
              "              'have to enable WebSockets in about:config.');\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = (this.ws.binaryType != undefined);\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById(\"mpl-warnings\");\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent = (\n",
              "                \"This browser does not support binary websocket messages. \" +\n",
              "                    \"Performance may be slow.\");\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = $('<div/>');\n",
              "    this._root_extra_style(this.root)\n",
              "    this.root.attr('style', 'display: inline-block');\n",
              "\n",
              "    $(parent_element).append(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen =  function () {\n",
              "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
              "            fig.send_message(\"send_image_mode\", {});\n",
              "            if (mpl.ratio != 1) {\n",
              "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
              "            }\n",
              "            fig.send_message(\"refresh\", {});\n",
              "        }\n",
              "\n",
              "    this.imageObj.onload = function() {\n",
              "            if (fig.image_mode == 'full') {\n",
              "                // Full images could contain transparency (where diff images\n",
              "                // almost always do), so we need to clear the canvas so that\n",
              "                // there is no ghosting.\n",
              "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "            }\n",
              "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "        };\n",
              "\n",
              "    this.imageObj.onunload = function() {\n",
              "        fig.ws.close();\n",
              "    }\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_header = function() {\n",
              "    var titlebar = $(\n",
              "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
              "        'ui-helper-clearfix\"/>');\n",
              "    var titletext = $(\n",
              "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
              "        'text-align: center; padding: 3px;\"/>');\n",
              "    titlebar.append(titletext)\n",
              "    this.root.append(titlebar);\n",
              "    this.header = titletext[0];\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = $('<div/>');\n",
              "\n",
              "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
              "\n",
              "    function canvas_keyboard_event(event) {\n",
              "        return fig.key_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
              "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
              "    this.canvas_div = canvas_div\n",
              "    this._canvas_extra_style(canvas_div)\n",
              "    this.root.append(canvas_div);\n",
              "\n",
              "    var canvas = $('<canvas/>');\n",
              "    canvas.addClass('mpl-canvas');\n",
              "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
              "\n",
              "    this.canvas = canvas[0];\n",
              "    this.context = canvas[0].getContext(\"2d\");\n",
              "\n",
              "    var backingStore = this.context.backingStorePixelRatio ||\n",
              "\tthis.context.webkitBackingStorePixelRatio ||\n",
              "\tthis.context.mozBackingStorePixelRatio ||\n",
              "\tthis.context.msBackingStorePixelRatio ||\n",
              "\tthis.context.oBackingStorePixelRatio ||\n",
              "\tthis.context.backingStorePixelRatio || 1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband = $('<canvas/>');\n",
              "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
              "\n",
              "    var pass_mouse_events = true;\n",
              "\n",
              "    canvas_div.resizable({\n",
              "        start: function(event, ui) {\n",
              "            pass_mouse_events = false;\n",
              "        },\n",
              "        resize: function(event, ui) {\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "        stop: function(event, ui) {\n",
              "            pass_mouse_events = true;\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "    });\n",
              "\n",
              "    function mouse_event_fn(event) {\n",
              "        if (pass_mouse_events)\n",
              "            return fig.mouse_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    rubberband.mousedown('button_press', mouse_event_fn);\n",
              "    rubberband.mouseup('button_release', mouse_event_fn);\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
              "\n",
              "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
              "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
              "\n",
              "    canvas_div.on(\"wheel\", function (event) {\n",
              "        event = event.originalEvent;\n",
              "        event['data'] = 'scroll'\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        mouse_event_fn(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.append(canvas);\n",
              "    canvas_div.append(rubberband);\n",
              "\n",
              "    this.rubberband = rubberband;\n",
              "    this.rubberband_canvas = rubberband[0];\n",
              "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
              "    this.rubberband_context.strokeStyle = \"#000000\";\n",
              "\n",
              "    this._resize_canvas = function(width, height) {\n",
              "        // Keep the size of the canvas, canvas container, and rubber band\n",
              "        // canvas in synch.\n",
              "        canvas_div.css('width', width)\n",
              "        canvas_div.css('height', height)\n",
              "\n",
              "        canvas.attr('width', width * mpl.ratio);\n",
              "        canvas.attr('height', height * mpl.ratio);\n",
              "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
              "\n",
              "        rubberband.attr('width', width);\n",
              "        rubberband.attr('height', height);\n",
              "    }\n",
              "\n",
              "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
              "    // upon first draw.\n",
              "    this._resize_canvas(600, 600);\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus () {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            // put a spacer in here.\n",
              "            continue;\n",
              "        }\n",
              "        var button = $('<button/>');\n",
              "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
              "                        'ui-button-icon-only');\n",
              "        button.attr('role', 'button');\n",
              "        button.attr('aria-disabled', 'false');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "\n",
              "        var icon_img = $('<span/>');\n",
              "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
              "        icon_img.addClass(image);\n",
              "        icon_img.addClass('ui-corner-all');\n",
              "\n",
              "        var tooltip_span = $('<span/>');\n",
              "        tooltip_span.addClass('ui-button-text');\n",
              "        tooltip_span.html(tooltip);\n",
              "\n",
              "        button.append(icon_img);\n",
              "        button.append(tooltip_span);\n",
              "\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    var fmt_picker_span = $('<span/>');\n",
              "\n",
              "    var fmt_picker = $('<select/>');\n",
              "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
              "    fmt_picker_span.append(fmt_picker);\n",
              "    nav_element.append(fmt_picker_span);\n",
              "    this.format_dropdown = fmt_picker[0];\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = $(\n",
              "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
              "        fmt_picker.append(option)\n",
              "    }\n",
              "\n",
              "    // Add hover states to the ui-buttons\n",
              "    $( \".ui-button\" ).hover(\n",
              "        function() { $(this).addClass(\"ui-state-hover\");},\n",
              "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
              "    );\n",
              "\n",
              "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_message = function(type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function() {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1]);\n",
              "        fig.send_message(\"refresh\", {});\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0, 0, fig.canvas.width, fig.canvas.height);\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch(cursor)\n",
              "    {\n",
              "    case 0:\n",
              "        cursor = 'pointer';\n",
              "        break;\n",
              "    case 1:\n",
              "        cursor = 'default';\n",
              "        break;\n",
              "    case 2:\n",
              "        cursor = 'crosshair';\n",
              "        break;\n",
              "    case 3:\n",
              "        cursor = 'move';\n",
              "        break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message(\"ack\", {});\n",
              "}\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = \"image/png\";\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src);\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data);\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig[\"handle_\" + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "}\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function(e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e)\n",
              "        e = window.event;\n",
              "    if (e.target)\n",
              "        targ = e.target;\n",
              "    else if (e.srcElement)\n",
              "        targ = e.srcElement;\n",
              "    if (targ.nodeType == 3) // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "\n",
              "    // jQuery normalizes the pageX and pageY\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    // offset() returns the position of the element relative to the document\n",
              "    var x = e.pageX - $(targ).offset().left;\n",
              "    var y = e.pageY - $(targ).offset().top;\n",
              "\n",
              "    return {\"x\": x, \"y\": y};\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys (original) {\n",
              "  return Object.keys(original).reduce(function (obj, key) {\n",
              "    if (typeof original[key] !== 'object')\n",
              "        obj[key] = original[key]\n",
              "    return obj;\n",
              "  }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function(event, name) {\n",
              "    var canvas_pos = mpl.findpos(event)\n",
              "\n",
              "    if (name === 'button_press')\n",
              "    {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {x: x, y: y, button: event.button,\n",
              "                             step: event.step,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.key_event = function(event, name) {\n",
              "\n",
              "    // Prevent repeat events\n",
              "    if (name == 'key_press')\n",
              "    {\n",
              "        if (event.which === this._key)\n",
              "            return;\n",
              "        else\n",
              "            this._key = event.which;\n",
              "    }\n",
              "    if (name == 'key_release')\n",
              "        this._key = null;\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which != 17)\n",
              "        value += \"ctrl+\";\n",
              "    if (event.altKey && event.which != 18)\n",
              "        value += \"alt+\";\n",
              "    if (event.shiftKey && event.which != 16)\n",
              "        value += \"shift+\";\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, {key: value,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
              "    if (name == 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message(\"toolbar_button\", {name: name});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function() {\n",
              "        comm.close()\n",
              "    };\n",
              "    ws.send = function(m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function(msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data'])\n",
              "    });\n",
              "    return ws;\n",
              "}\n",
              "\n",
              "mpl.mpl_figure_comm = function(comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = $(\"#\" + id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm)\n",
              "\n",
              "    function ondownload(figure, format) {\n",
              "        window.open(figure.imageObj.src);\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy,\n",
              "                           ondownload,\n",
              "                           element.get(0));\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element.get(0);\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error(\"Failed to find cell for figure\", id, fig);\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    var output_index = fig.cell_info[2]\n",
              "    var cell = fig.cell_info[0];\n",
              "\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
              "    var width = fig.canvas.width/mpl.ratio\n",
              "    fig.root.unbind('remove')\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable()\n",
              "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
              "    fig.close_ws(fig, msg);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.close_ws = function(fig, msg){\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width/mpl.ratio\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message(\"ack\", {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items){\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) { continue; };\n",
              "\n",
              "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
              "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
              "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
              "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
              "    buttongrp.append(button);\n",
              "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
              "    titlebar.prepend(buttongrp);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(el){\n",
              "    var fig = this\n",
              "    el.on(\"remove\", function(){\n",
              "\tfig.close_ws(fig, {});\n",
              "    });\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(el){\n",
              "    // this is important to make the div 'focusable\n",
              "    el.attr('tabindex', 0)\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    }\n",
              "    else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager)\n",
              "        manager = IPython.keyboard_manager;\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which == 13) {\n",
              "        this.canvas_div.blur();\n",
              "        event.shiftKey = false;\n",
              "        // Send a \"J\" for go to next cell\n",
              "        event.which = 74;\n",
              "        event.keyCode = 74;\n",
              "        manager.command_mode();\n",
              "        manager.handle_keydown(event);\n",
              "    }\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.find_output_cell = function(html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i=0; i<ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code'){\n",
              "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] == html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel != null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id='78660c4b-4534-4e1d-93ac-a4fa3f49657e'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "window.mpl = {};\n",
              "\n",
              "\n",
              "mpl.get_websocket_type = function() {\n",
              "    if (typeof(WebSocket) !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert('Your browser does not have WebSocket support.' +\n",
              "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "              'Firefox 4 and 5 are also supported but you ' +\n",
              "              'have to enable WebSockets in about:config.');\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = (this.ws.binaryType != undefined);\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById(\"mpl-warnings\");\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent = (\n",
              "                \"This browser does not support binary websocket messages. \" +\n",
              "                    \"Performance may be slow.\");\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = $('<div/>');\n",
              "    this._root_extra_style(this.root)\n",
              "    this.root.attr('style', 'display: inline-block');\n",
              "\n",
              "    $(parent_element).append(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen =  function () {\n",
              "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
              "            fig.send_message(\"send_image_mode\", {});\n",
              "            if (mpl.ratio != 1) {\n",
              "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
              "            }\n",
              "            fig.send_message(\"refresh\", {});\n",
              "        }\n",
              "\n",
              "    this.imageObj.onload = function() {\n",
              "            if (fig.image_mode == 'full') {\n",
              "                // Full images could contain transparency (where diff images\n",
              "                // almost always do), so we need to clear the canvas so that\n",
              "                // there is no ghosting.\n",
              "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "            }\n",
              "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "        };\n",
              "\n",
              "    this.imageObj.onunload = function() {\n",
              "        fig.ws.close();\n",
              "    }\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_header = function() {\n",
              "    var titlebar = $(\n",
              "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
              "        'ui-helper-clearfix\"/>');\n",
              "    var titletext = $(\n",
              "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
              "        'text-align: center; padding: 3px;\"/>');\n",
              "    titlebar.append(titletext)\n",
              "    this.root.append(titlebar);\n",
              "    this.header = titletext[0];\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = $('<div/>');\n",
              "\n",
              "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
              "\n",
              "    function canvas_keyboard_event(event) {\n",
              "        return fig.key_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
              "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
              "    this.canvas_div = canvas_div\n",
              "    this._canvas_extra_style(canvas_div)\n",
              "    this.root.append(canvas_div);\n",
              "\n",
              "    var canvas = $('<canvas/>');\n",
              "    canvas.addClass('mpl-canvas');\n",
              "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
              "\n",
              "    this.canvas = canvas[0];\n",
              "    this.context = canvas[0].getContext(\"2d\");\n",
              "\n",
              "    var backingStore = this.context.backingStorePixelRatio ||\n",
              "\tthis.context.webkitBackingStorePixelRatio ||\n",
              "\tthis.context.mozBackingStorePixelRatio ||\n",
              "\tthis.context.msBackingStorePixelRatio ||\n",
              "\tthis.context.oBackingStorePixelRatio ||\n",
              "\tthis.context.backingStorePixelRatio || 1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband = $('<canvas/>');\n",
              "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
              "\n",
              "    var pass_mouse_events = true;\n",
              "\n",
              "    canvas_div.resizable({\n",
              "        start: function(event, ui) {\n",
              "            pass_mouse_events = false;\n",
              "        },\n",
              "        resize: function(event, ui) {\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "        stop: function(event, ui) {\n",
              "            pass_mouse_events = true;\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "    });\n",
              "\n",
              "    function mouse_event_fn(event) {\n",
              "        if (pass_mouse_events)\n",
              "            return fig.mouse_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    rubberband.mousedown('button_press', mouse_event_fn);\n",
              "    rubberband.mouseup('button_release', mouse_event_fn);\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
              "\n",
              "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
              "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
              "\n",
              "    canvas_div.on(\"wheel\", function (event) {\n",
              "        event = event.originalEvent;\n",
              "        event['data'] = 'scroll'\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        mouse_event_fn(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.append(canvas);\n",
              "    canvas_div.append(rubberband);\n",
              "\n",
              "    this.rubberband = rubberband;\n",
              "    this.rubberband_canvas = rubberband[0];\n",
              "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
              "    this.rubberband_context.strokeStyle = \"#000000\";\n",
              "\n",
              "    this._resize_canvas = function(width, height) {\n",
              "        // Keep the size of the canvas, canvas container, and rubber band\n",
              "        // canvas in synch.\n",
              "        canvas_div.css('width', width)\n",
              "        canvas_div.css('height', height)\n",
              "\n",
              "        canvas.attr('width', width * mpl.ratio);\n",
              "        canvas.attr('height', height * mpl.ratio);\n",
              "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
              "\n",
              "        rubberband.attr('width', width);\n",
              "        rubberband.attr('height', height);\n",
              "    }\n",
              "\n",
              "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
              "    // upon first draw.\n",
              "    this._resize_canvas(600, 600);\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus () {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            // put a spacer in here.\n",
              "            continue;\n",
              "        }\n",
              "        var button = $('<button/>');\n",
              "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
              "                        'ui-button-icon-only');\n",
              "        button.attr('role', 'button');\n",
              "        button.attr('aria-disabled', 'false');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "\n",
              "        var icon_img = $('<span/>');\n",
              "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
              "        icon_img.addClass(image);\n",
              "        icon_img.addClass('ui-corner-all');\n",
              "\n",
              "        var tooltip_span = $('<span/>');\n",
              "        tooltip_span.addClass('ui-button-text');\n",
              "        tooltip_span.html(tooltip);\n",
              "\n",
              "        button.append(icon_img);\n",
              "        button.append(tooltip_span);\n",
              "\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    var fmt_picker_span = $('<span/>');\n",
              "\n",
              "    var fmt_picker = $('<select/>');\n",
              "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
              "    fmt_picker_span.append(fmt_picker);\n",
              "    nav_element.append(fmt_picker_span);\n",
              "    this.format_dropdown = fmt_picker[0];\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = $(\n",
              "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
              "        fmt_picker.append(option)\n",
              "    }\n",
              "\n",
              "    // Add hover states to the ui-buttons\n",
              "    $( \".ui-button\" ).hover(\n",
              "        function() { $(this).addClass(\"ui-state-hover\");},\n",
              "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
              "    );\n",
              "\n",
              "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_message = function(type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function() {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1]);\n",
              "        fig.send_message(\"refresh\", {});\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0, 0, fig.canvas.width, fig.canvas.height);\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch(cursor)\n",
              "    {\n",
              "    case 0:\n",
              "        cursor = 'pointer';\n",
              "        break;\n",
              "    case 1:\n",
              "        cursor = 'default';\n",
              "        break;\n",
              "    case 2:\n",
              "        cursor = 'crosshair';\n",
              "        break;\n",
              "    case 3:\n",
              "        cursor = 'move';\n",
              "        break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message(\"ack\", {});\n",
              "}\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = \"image/png\";\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src);\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data);\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig[\"handle_\" + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "}\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function(e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e)\n",
              "        e = window.event;\n",
              "    if (e.target)\n",
              "        targ = e.target;\n",
              "    else if (e.srcElement)\n",
              "        targ = e.srcElement;\n",
              "    if (targ.nodeType == 3) // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "\n",
              "    // jQuery normalizes the pageX and pageY\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    // offset() returns the position of the element relative to the document\n",
              "    var x = e.pageX - $(targ).offset().left;\n",
              "    var y = e.pageY - $(targ).offset().top;\n",
              "\n",
              "    return {\"x\": x, \"y\": y};\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys (original) {\n",
              "  return Object.keys(original).reduce(function (obj, key) {\n",
              "    if (typeof original[key] !== 'object')\n",
              "        obj[key] = original[key]\n",
              "    return obj;\n",
              "  }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function(event, name) {\n",
              "    var canvas_pos = mpl.findpos(event)\n",
              "\n",
              "    if (name === 'button_press')\n",
              "    {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {x: x, y: y, button: event.button,\n",
              "                             step: event.step,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.key_event = function(event, name) {\n",
              "\n",
              "    // Prevent repeat events\n",
              "    if (name == 'key_press')\n",
              "    {\n",
              "        if (event.which === this._key)\n",
              "            return;\n",
              "        else\n",
              "            this._key = event.which;\n",
              "    }\n",
              "    if (name == 'key_release')\n",
              "        this._key = null;\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which != 17)\n",
              "        value += \"ctrl+\";\n",
              "    if (event.altKey && event.which != 18)\n",
              "        value += \"alt+\";\n",
              "    if (event.shiftKey && event.which != 16)\n",
              "        value += \"shift+\";\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, {key: value,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
              "    if (name == 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message(\"toolbar_button\", {name: name});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function() {\n",
              "        comm.close()\n",
              "    };\n",
              "    ws.send = function(m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function(msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data'])\n",
              "    });\n",
              "    return ws;\n",
              "}\n",
              "\n",
              "mpl.mpl_figure_comm = function(comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = $(\"#\" + id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm)\n",
              "\n",
              "    function ondownload(figure, format) {\n",
              "        window.open(figure.imageObj.src);\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy,\n",
              "                           ondownload,\n",
              "                           element.get(0));\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element.get(0);\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error(\"Failed to find cell for figure\", id, fig);\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    var output_index = fig.cell_info[2]\n",
              "    var cell = fig.cell_info[0];\n",
              "\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
              "    var width = fig.canvas.width/mpl.ratio\n",
              "    fig.root.unbind('remove')\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable()\n",
              "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
              "    fig.close_ws(fig, msg);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.close_ws = function(fig, msg){\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width/mpl.ratio\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message(\"ack\", {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items){\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) { continue; };\n",
              "\n",
              "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
              "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
              "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
              "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
              "    buttongrp.append(button);\n",
              "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
              "    titlebar.prepend(buttongrp);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(el){\n",
              "    var fig = this\n",
              "    el.on(\"remove\", function(){\n",
              "\tfig.close_ws(fig, {});\n",
              "    });\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(el){\n",
              "    // this is important to make the div 'focusable\n",
              "    el.attr('tabindex', 0)\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    }\n",
              "    else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager)\n",
              "        manager = IPython.keyboard_manager;\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which == 13) {\n",
              "        this.canvas_div.blur();\n",
              "        event.shiftKey = false;\n",
              "        // Send a \"J\" for go to next cell\n",
              "        event.which = 74;\n",
              "        event.keyCode = 74;\n",
              "        manager.command_mode();\n",
              "        manager.handle_keydown(event);\n",
              "    }\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.find_output_cell = function(html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i=0; i<ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code'){\n",
              "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] == html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel != null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id='6afca6db-037e-48cd-9d48-216473e9d076'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "window.mpl = {};\n",
              "\n",
              "\n",
              "mpl.get_websocket_type = function() {\n",
              "    if (typeof(WebSocket) !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert('Your browser does not have WebSocket support.' +\n",
              "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "              'Firefox 4 and 5 are also supported but you ' +\n",
              "              'have to enable WebSockets in about:config.');\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = (this.ws.binaryType != undefined);\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById(\"mpl-warnings\");\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent = (\n",
              "                \"This browser does not support binary websocket messages. \" +\n",
              "                    \"Performance may be slow.\");\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = $('<div/>');\n",
              "    this._root_extra_style(this.root)\n",
              "    this.root.attr('style', 'display: inline-block');\n",
              "\n",
              "    $(parent_element).append(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen =  function () {\n",
              "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
              "            fig.send_message(\"send_image_mode\", {});\n",
              "            if (mpl.ratio != 1) {\n",
              "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
              "            }\n",
              "            fig.send_message(\"refresh\", {});\n",
              "        }\n",
              "\n",
              "    this.imageObj.onload = function() {\n",
              "            if (fig.image_mode == 'full') {\n",
              "                // Full images could contain transparency (where diff images\n",
              "                // almost always do), so we need to clear the canvas so that\n",
              "                // there is no ghosting.\n",
              "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "            }\n",
              "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "        };\n",
              "\n",
              "    this.imageObj.onunload = function() {\n",
              "        fig.ws.close();\n",
              "    }\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_header = function() {\n",
              "    var titlebar = $(\n",
              "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
              "        'ui-helper-clearfix\"/>');\n",
              "    var titletext = $(\n",
              "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
              "        'text-align: center; padding: 3px;\"/>');\n",
              "    titlebar.append(titletext)\n",
              "    this.root.append(titlebar);\n",
              "    this.header = titletext[0];\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = $('<div/>');\n",
              "\n",
              "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
              "\n",
              "    function canvas_keyboard_event(event) {\n",
              "        return fig.key_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
              "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
              "    this.canvas_div = canvas_div\n",
              "    this._canvas_extra_style(canvas_div)\n",
              "    this.root.append(canvas_div);\n",
              "\n",
              "    var canvas = $('<canvas/>');\n",
              "    canvas.addClass('mpl-canvas');\n",
              "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
              "\n",
              "    this.canvas = canvas[0];\n",
              "    this.context = canvas[0].getContext(\"2d\");\n",
              "\n",
              "    var backingStore = this.context.backingStorePixelRatio ||\n",
              "\tthis.context.webkitBackingStorePixelRatio ||\n",
              "\tthis.context.mozBackingStorePixelRatio ||\n",
              "\tthis.context.msBackingStorePixelRatio ||\n",
              "\tthis.context.oBackingStorePixelRatio ||\n",
              "\tthis.context.backingStorePixelRatio || 1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband = $('<canvas/>');\n",
              "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
              "\n",
              "    var pass_mouse_events = true;\n",
              "\n",
              "    canvas_div.resizable({\n",
              "        start: function(event, ui) {\n",
              "            pass_mouse_events = false;\n",
              "        },\n",
              "        resize: function(event, ui) {\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "        stop: function(event, ui) {\n",
              "            pass_mouse_events = true;\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "    });\n",
              "\n",
              "    function mouse_event_fn(event) {\n",
              "        if (pass_mouse_events)\n",
              "            return fig.mouse_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    rubberband.mousedown('button_press', mouse_event_fn);\n",
              "    rubberband.mouseup('button_release', mouse_event_fn);\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
              "\n",
              "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
              "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
              "\n",
              "    canvas_div.on(\"wheel\", function (event) {\n",
              "        event = event.originalEvent;\n",
              "        event['data'] = 'scroll'\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        mouse_event_fn(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.append(canvas);\n",
              "    canvas_div.append(rubberband);\n",
              "\n",
              "    this.rubberband = rubberband;\n",
              "    this.rubberband_canvas = rubberband[0];\n",
              "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
              "    this.rubberband_context.strokeStyle = \"#000000\";\n",
              "\n",
              "    this._resize_canvas = function(width, height) {\n",
              "        // Keep the size of the canvas, canvas container, and rubber band\n",
              "        // canvas in synch.\n",
              "        canvas_div.css('width', width)\n",
              "        canvas_div.css('height', height)\n",
              "\n",
              "        canvas.attr('width', width * mpl.ratio);\n",
              "        canvas.attr('height', height * mpl.ratio);\n",
              "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
              "\n",
              "        rubberband.attr('width', width);\n",
              "        rubberband.attr('height', height);\n",
              "    }\n",
              "\n",
              "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
              "    // upon first draw.\n",
              "    this._resize_canvas(600, 600);\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus () {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            // put a spacer in here.\n",
              "            continue;\n",
              "        }\n",
              "        var button = $('<button/>');\n",
              "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
              "                        'ui-button-icon-only');\n",
              "        button.attr('role', 'button');\n",
              "        button.attr('aria-disabled', 'false');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "\n",
              "        var icon_img = $('<span/>');\n",
              "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
              "        icon_img.addClass(image);\n",
              "        icon_img.addClass('ui-corner-all');\n",
              "\n",
              "        var tooltip_span = $('<span/>');\n",
              "        tooltip_span.addClass('ui-button-text');\n",
              "        tooltip_span.html(tooltip);\n",
              "\n",
              "        button.append(icon_img);\n",
              "        button.append(tooltip_span);\n",
              "\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    var fmt_picker_span = $('<span/>');\n",
              "\n",
              "    var fmt_picker = $('<select/>');\n",
              "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
              "    fmt_picker_span.append(fmt_picker);\n",
              "    nav_element.append(fmt_picker_span);\n",
              "    this.format_dropdown = fmt_picker[0];\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = $(\n",
              "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
              "        fmt_picker.append(option)\n",
              "    }\n",
              "\n",
              "    // Add hover states to the ui-buttons\n",
              "    $( \".ui-button\" ).hover(\n",
              "        function() { $(this).addClass(\"ui-state-hover\");},\n",
              "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
              "    );\n",
              "\n",
              "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_message = function(type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function() {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1]);\n",
              "        fig.send_message(\"refresh\", {});\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0, 0, fig.canvas.width, fig.canvas.height);\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch(cursor)\n",
              "    {\n",
              "    case 0:\n",
              "        cursor = 'pointer';\n",
              "        break;\n",
              "    case 1:\n",
              "        cursor = 'default';\n",
              "        break;\n",
              "    case 2:\n",
              "        cursor = 'crosshair';\n",
              "        break;\n",
              "    case 3:\n",
              "        cursor = 'move';\n",
              "        break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message(\"ack\", {});\n",
              "}\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = \"image/png\";\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src);\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data);\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig[\"handle_\" + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "}\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function(e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e)\n",
              "        e = window.event;\n",
              "    if (e.target)\n",
              "        targ = e.target;\n",
              "    else if (e.srcElement)\n",
              "        targ = e.srcElement;\n",
              "    if (targ.nodeType == 3) // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "\n",
              "    // jQuery normalizes the pageX and pageY\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    // offset() returns the position of the element relative to the document\n",
              "    var x = e.pageX - $(targ).offset().left;\n",
              "    var y = e.pageY - $(targ).offset().top;\n",
              "\n",
              "    return {\"x\": x, \"y\": y};\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys (original) {\n",
              "  return Object.keys(original).reduce(function (obj, key) {\n",
              "    if (typeof original[key] !== 'object')\n",
              "        obj[key] = original[key]\n",
              "    return obj;\n",
              "  }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function(event, name) {\n",
              "    var canvas_pos = mpl.findpos(event)\n",
              "\n",
              "    if (name === 'button_press')\n",
              "    {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {x: x, y: y, button: event.button,\n",
              "                             step: event.step,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.key_event = function(event, name) {\n",
              "\n",
              "    // Prevent repeat events\n",
              "    if (name == 'key_press')\n",
              "    {\n",
              "        if (event.which === this._key)\n",
              "            return;\n",
              "        else\n",
              "            this._key = event.which;\n",
              "    }\n",
              "    if (name == 'key_release')\n",
              "        this._key = null;\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which != 17)\n",
              "        value += \"ctrl+\";\n",
              "    if (event.altKey && event.which != 18)\n",
              "        value += \"alt+\";\n",
              "    if (event.shiftKey && event.which != 16)\n",
              "        value += \"shift+\";\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, {key: value,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
              "    if (name == 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message(\"toolbar_button\", {name: name});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function() {\n",
              "        comm.close()\n",
              "    };\n",
              "    ws.send = function(m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function(msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data'])\n",
              "    });\n",
              "    return ws;\n",
              "}\n",
              "\n",
              "mpl.mpl_figure_comm = function(comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = $(\"#\" + id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm)\n",
              "\n",
              "    function ondownload(figure, format) {\n",
              "        window.open(figure.imageObj.src);\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy,\n",
              "                           ondownload,\n",
              "                           element.get(0));\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element.get(0);\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error(\"Failed to find cell for figure\", id, fig);\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    var output_index = fig.cell_info[2]\n",
              "    var cell = fig.cell_info[0];\n",
              "\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
              "    var width = fig.canvas.width/mpl.ratio\n",
              "    fig.root.unbind('remove')\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable()\n",
              "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
              "    fig.close_ws(fig, msg);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.close_ws = function(fig, msg){\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width/mpl.ratio\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message(\"ack\", {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items){\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) { continue; };\n",
              "\n",
              "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
              "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
              "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
              "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
              "    buttongrp.append(button);\n",
              "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
              "    titlebar.prepend(buttongrp);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(el){\n",
              "    var fig = this\n",
              "    el.on(\"remove\", function(){\n",
              "\tfig.close_ws(fig, {});\n",
              "    });\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(el){\n",
              "    // this is important to make the div 'focusable\n",
              "    el.attr('tabindex', 0)\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    }\n",
              "    else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager)\n",
              "        manager = IPython.keyboard_manager;\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which == 13) {\n",
              "        this.canvas_div.blur();\n",
              "        event.shiftKey = false;\n",
              "        // Send a \"J\" for go to next cell\n",
              "        event.which = 74;\n",
              "        event.keyCode = 74;\n",
              "        manager.command_mode();\n",
              "        manager.handle_keydown(event);\n",
              "    }\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.find_output_cell = function(html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i=0; i<ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code'){\n",
              "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] == html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel != null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id='1732fd28-6c54-42f6-9da3-6ec9d01f474c'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "window.mpl = {};\n",
              "\n",
              "\n",
              "mpl.get_websocket_type = function() {\n",
              "    if (typeof(WebSocket) !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert('Your browser does not have WebSocket support.' +\n",
              "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "              'Firefox 4 and 5 are also supported but you ' +\n",
              "              'have to enable WebSockets in about:config.');\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = (this.ws.binaryType != undefined);\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById(\"mpl-warnings\");\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent = (\n",
              "                \"This browser does not support binary websocket messages. \" +\n",
              "                    \"Performance may be slow.\");\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = $('<div/>');\n",
              "    this._root_extra_style(this.root)\n",
              "    this.root.attr('style', 'display: inline-block');\n",
              "\n",
              "    $(parent_element).append(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen =  function () {\n",
              "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
              "            fig.send_message(\"send_image_mode\", {});\n",
              "            if (mpl.ratio != 1) {\n",
              "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
              "            }\n",
              "            fig.send_message(\"refresh\", {});\n",
              "        }\n",
              "\n",
              "    this.imageObj.onload = function() {\n",
              "            if (fig.image_mode == 'full') {\n",
              "                // Full images could contain transparency (where diff images\n",
              "                // almost always do), so we need to clear the canvas so that\n",
              "                // there is no ghosting.\n",
              "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "            }\n",
              "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "        };\n",
              "\n",
              "    this.imageObj.onunload = function() {\n",
              "        fig.ws.close();\n",
              "    }\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_header = function() {\n",
              "    var titlebar = $(\n",
              "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
              "        'ui-helper-clearfix\"/>');\n",
              "    var titletext = $(\n",
              "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
              "        'text-align: center; padding: 3px;\"/>');\n",
              "    titlebar.append(titletext)\n",
              "    this.root.append(titlebar);\n",
              "    this.header = titletext[0];\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = $('<div/>');\n",
              "\n",
              "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
              "\n",
              "    function canvas_keyboard_event(event) {\n",
              "        return fig.key_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
              "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
              "    this.canvas_div = canvas_div\n",
              "    this._canvas_extra_style(canvas_div)\n",
              "    this.root.append(canvas_div);\n",
              "\n",
              "    var canvas = $('<canvas/>');\n",
              "    canvas.addClass('mpl-canvas');\n",
              "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
              "\n",
              "    this.canvas = canvas[0];\n",
              "    this.context = canvas[0].getContext(\"2d\");\n",
              "\n",
              "    var backingStore = this.context.backingStorePixelRatio ||\n",
              "\tthis.context.webkitBackingStorePixelRatio ||\n",
              "\tthis.context.mozBackingStorePixelRatio ||\n",
              "\tthis.context.msBackingStorePixelRatio ||\n",
              "\tthis.context.oBackingStorePixelRatio ||\n",
              "\tthis.context.backingStorePixelRatio || 1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband = $('<canvas/>');\n",
              "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
              "\n",
              "    var pass_mouse_events = true;\n",
              "\n",
              "    canvas_div.resizable({\n",
              "        start: function(event, ui) {\n",
              "            pass_mouse_events = false;\n",
              "        },\n",
              "        resize: function(event, ui) {\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "        stop: function(event, ui) {\n",
              "            pass_mouse_events = true;\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "    });\n",
              "\n",
              "    function mouse_event_fn(event) {\n",
              "        if (pass_mouse_events)\n",
              "            return fig.mouse_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    rubberband.mousedown('button_press', mouse_event_fn);\n",
              "    rubberband.mouseup('button_release', mouse_event_fn);\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
              "\n",
              "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
              "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
              "\n",
              "    canvas_div.on(\"wheel\", function (event) {\n",
              "        event = event.originalEvent;\n",
              "        event['data'] = 'scroll'\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        mouse_event_fn(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.append(canvas);\n",
              "    canvas_div.append(rubberband);\n",
              "\n",
              "    this.rubberband = rubberband;\n",
              "    this.rubberband_canvas = rubberband[0];\n",
              "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
              "    this.rubberband_context.strokeStyle = \"#000000\";\n",
              "\n",
              "    this._resize_canvas = function(width, height) {\n",
              "        // Keep the size of the canvas, canvas container, and rubber band\n",
              "        // canvas in synch.\n",
              "        canvas_div.css('width', width)\n",
              "        canvas_div.css('height', height)\n",
              "\n",
              "        canvas.attr('width', width * mpl.ratio);\n",
              "        canvas.attr('height', height * mpl.ratio);\n",
              "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
              "\n",
              "        rubberband.attr('width', width);\n",
              "        rubberband.attr('height', height);\n",
              "    }\n",
              "\n",
              "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
              "    // upon first draw.\n",
              "    this._resize_canvas(600, 600);\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus () {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            // put a spacer in here.\n",
              "            continue;\n",
              "        }\n",
              "        var button = $('<button/>');\n",
              "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
              "                        'ui-button-icon-only');\n",
              "        button.attr('role', 'button');\n",
              "        button.attr('aria-disabled', 'false');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "\n",
              "        var icon_img = $('<span/>');\n",
              "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
              "        icon_img.addClass(image);\n",
              "        icon_img.addClass('ui-corner-all');\n",
              "\n",
              "        var tooltip_span = $('<span/>');\n",
              "        tooltip_span.addClass('ui-button-text');\n",
              "        tooltip_span.html(tooltip);\n",
              "\n",
              "        button.append(icon_img);\n",
              "        button.append(tooltip_span);\n",
              "\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    var fmt_picker_span = $('<span/>');\n",
              "\n",
              "    var fmt_picker = $('<select/>');\n",
              "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
              "    fmt_picker_span.append(fmt_picker);\n",
              "    nav_element.append(fmt_picker_span);\n",
              "    this.format_dropdown = fmt_picker[0];\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = $(\n",
              "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
              "        fmt_picker.append(option)\n",
              "    }\n",
              "\n",
              "    // Add hover states to the ui-buttons\n",
              "    $( \".ui-button\" ).hover(\n",
              "        function() { $(this).addClass(\"ui-state-hover\");},\n",
              "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
              "    );\n",
              "\n",
              "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_message = function(type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function() {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1]);\n",
              "        fig.send_message(\"refresh\", {});\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0, 0, fig.canvas.width, fig.canvas.height);\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch(cursor)\n",
              "    {\n",
              "    case 0:\n",
              "        cursor = 'pointer';\n",
              "        break;\n",
              "    case 1:\n",
              "        cursor = 'default';\n",
              "        break;\n",
              "    case 2:\n",
              "        cursor = 'crosshair';\n",
              "        break;\n",
              "    case 3:\n",
              "        cursor = 'move';\n",
              "        break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message(\"ack\", {});\n",
              "}\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = \"image/png\";\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src);\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data);\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig[\"handle_\" + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "}\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function(e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e)\n",
              "        e = window.event;\n",
              "    if (e.target)\n",
              "        targ = e.target;\n",
              "    else if (e.srcElement)\n",
              "        targ = e.srcElement;\n",
              "    if (targ.nodeType == 3) // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "\n",
              "    // jQuery normalizes the pageX and pageY\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    // offset() returns the position of the element relative to the document\n",
              "    var x = e.pageX - $(targ).offset().left;\n",
              "    var y = e.pageY - $(targ).offset().top;\n",
              "\n",
              "    return {\"x\": x, \"y\": y};\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys (original) {\n",
              "  return Object.keys(original).reduce(function (obj, key) {\n",
              "    if (typeof original[key] !== 'object')\n",
              "        obj[key] = original[key]\n",
              "    return obj;\n",
              "  }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function(event, name) {\n",
              "    var canvas_pos = mpl.findpos(event)\n",
              "\n",
              "    if (name === 'button_press')\n",
              "    {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {x: x, y: y, button: event.button,\n",
              "                             step: event.step,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.key_event = function(event, name) {\n",
              "\n",
              "    // Prevent repeat events\n",
              "    if (name == 'key_press')\n",
              "    {\n",
              "        if (event.which === this._key)\n",
              "            return;\n",
              "        else\n",
              "            this._key = event.which;\n",
              "    }\n",
              "    if (name == 'key_release')\n",
              "        this._key = null;\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which != 17)\n",
              "        value += \"ctrl+\";\n",
              "    if (event.altKey && event.which != 18)\n",
              "        value += \"alt+\";\n",
              "    if (event.shiftKey && event.which != 16)\n",
              "        value += \"shift+\";\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, {key: value,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
              "    if (name == 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message(\"toolbar_button\", {name: name});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function() {\n",
              "        comm.close()\n",
              "    };\n",
              "    ws.send = function(m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function(msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data'])\n",
              "    });\n",
              "    return ws;\n",
              "}\n",
              "\n",
              "mpl.mpl_figure_comm = function(comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = $(\"#\" + id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm)\n",
              "\n",
              "    function ondownload(figure, format) {\n",
              "        window.open(figure.imageObj.src);\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy,\n",
              "                           ondownload,\n",
              "                           element.get(0));\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element.get(0);\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error(\"Failed to find cell for figure\", id, fig);\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    var output_index = fig.cell_info[2]\n",
              "    var cell = fig.cell_info[0];\n",
              "\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
              "    var width = fig.canvas.width/mpl.ratio\n",
              "    fig.root.unbind('remove')\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable()\n",
              "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
              "    fig.close_ws(fig, msg);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.close_ws = function(fig, msg){\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width/mpl.ratio\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message(\"ack\", {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>')\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items){\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) { continue; };\n",
              "\n",
              "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
              "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
              "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
              "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
              "    buttongrp.append(button);\n",
              "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
              "    titlebar.prepend(buttongrp);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(el){\n",
              "    var fig = this\n",
              "    el.on(\"remove\", function(){\n",
              "\tfig.close_ws(fig, {});\n",
              "    });\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(el){\n",
              "    // this is important to make the div 'focusable\n",
              "    el.attr('tabindex', 0)\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    }\n",
              "    else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager)\n",
              "        manager = IPython.keyboard_manager;\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which == 13) {\n",
              "        this.canvas_div.blur();\n",
              "        event.shiftKey = false;\n",
              "        // Send a \"J\" for go to next cell\n",
              "        event.which = 74;\n",
              "        event.keyCode = 74;\n",
              "        manager.command_mode();\n",
              "        manager.handle_keydown(event);\n",
              "    }\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.find_output_cell = function(html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i=0; i<ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code'){\n",
              "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] == html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel != null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id='5413b90f-8b05-410c-ac1c-5cca7b999079'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}